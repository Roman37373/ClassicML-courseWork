{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.937578Z",
     "start_time": "2025-05-28T16:44:43.537577Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shap\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from scipy import stats\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, \n",
    "                           r2_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, classification_report,\n",
    "                           roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Other ML libraries\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267e6757ca7a5130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.960272Z",
     "start_time": "2025-05-28T16:44:45.957343Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5b2279b2b7ab49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:11.783119Z",
     "start_time": "2025-05-28T16:57:11.742132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "df = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d4d43a0b8f2e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:26.787706Z",
     "start_time": "2025-05-28T17:02:26.674195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>IC50, mM</th>\n",
       "      <th>CC50, mM</th>\n",
       "      <th>SI</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>NumValenceElectrons</th>\n",
       "      <th>NumRadicalElectrons</th>\n",
       "      <th>MaxPartialCharge</th>\n",
       "      <th>MinPartialCharge</th>\n",
       "      <th>MaxAbsPartialCharge</th>\n",
       "      <th>MinAbsPartialCharge</th>\n",
       "      <th>FpDensityMorgan1</th>\n",
       "      <th>FpDensityMorgan2</th>\n",
       "      <th>FpDensityMorgan3</th>\n",
       "      <th>BCUT2D_MWHI</th>\n",
       "      <th>BCUT2D_MWLOW</th>\n",
       "      <th>BCUT2D_CHGHI</th>\n",
       "      <th>BCUT2D_CHGLO</th>\n",
       "      <th>BCUT2D_LOGPHI</th>\n",
       "      <th>BCUT2D_LOGPLOW</th>\n",
       "      <th>BCUT2D_MRHI</th>\n",
       "      <th>BCUT2D_MRLOW</th>\n",
       "      <th>AvgIpc</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi0n</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi1n</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>Chi2n</th>\n",
       "      <th>Chi2v</th>\n",
       "      <th>Chi3n</th>\n",
       "      <th>Chi3v</th>\n",
       "      <th>Chi4n</th>\n",
       "      <th>Chi4v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Ipc</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>LabuteASA</th>\n",
       "      <th>PEOE_VSA1</th>\n",
       "      <th>PEOE_VSA10</th>\n",
       "      <th>PEOE_VSA11</th>\n",
       "      <th>PEOE_VSA12</th>\n",
       "      <th>PEOE_VSA13</th>\n",
       "      <th>PEOE_VSA14</th>\n",
       "      <th>PEOE_VSA2</th>\n",
       "      <th>PEOE_VSA3</th>\n",
       "      <th>PEOE_VSA4</th>\n",
       "      <th>PEOE_VSA5</th>\n",
       "      <th>PEOE_VSA6</th>\n",
       "      <th>PEOE_VSA7</th>\n",
       "      <th>PEOE_VSA8</th>\n",
       "      <th>PEOE_VSA9</th>\n",
       "      <th>SMR_VSA1</th>\n",
       "      <th>SMR_VSA10</th>\n",
       "      <th>SMR_VSA2</th>\n",
       "      <th>SMR_VSA3</th>\n",
       "      <th>SMR_VSA4</th>\n",
       "      <th>SMR_VSA5</th>\n",
       "      <th>SMR_VSA6</th>\n",
       "      <th>SMR_VSA7</th>\n",
       "      <th>SMR_VSA8</th>\n",
       "      <th>SMR_VSA9</th>\n",
       "      <th>SlogP_VSA1</th>\n",
       "      <th>SlogP_VSA10</th>\n",
       "      <th>SlogP_VSA11</th>\n",
       "      <th>SlogP_VSA12</th>\n",
       "      <th>SlogP_VSA2</th>\n",
       "      <th>SlogP_VSA3</th>\n",
       "      <th>SlogP_VSA4</th>\n",
       "      <th>SlogP_VSA5</th>\n",
       "      <th>SlogP_VSA6</th>\n",
       "      <th>SlogP_VSA7</th>\n",
       "      <th>SlogP_VSA8</th>\n",
       "      <th>SlogP_VSA9</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>EState_VSA1</th>\n",
       "      <th>EState_VSA10</th>\n",
       "      <th>EState_VSA11</th>\n",
       "      <th>EState_VSA2</th>\n",
       "      <th>EState_VSA3</th>\n",
       "      <th>EState_VSA4</th>\n",
       "      <th>EState_VSA5</th>\n",
       "      <th>EState_VSA6</th>\n",
       "      <th>EState_VSA7</th>\n",
       "      <th>EState_VSA8</th>\n",
       "      <th>EState_VSA9</th>\n",
       "      <th>VSA_EState1</th>\n",
       "      <th>VSA_EState10</th>\n",
       "      <th>VSA_EState2</th>\n",
       "      <th>VSA_EState3</th>\n",
       "      <th>VSA_EState4</th>\n",
       "      <th>VSA_EState5</th>\n",
       "      <th>VSA_EState6</th>\n",
       "      <th>VSA_EState7</th>\n",
       "      <th>VSA_EState8</th>\n",
       "      <th>VSA_EState9</th>\n",
       "      <th>FractionCSP3</th>\n",
       "      <th>HeavyAtomCount</th>\n",
       "      <th>NHOHCount</th>\n",
       "      <th>NOCount</th>\n",
       "      <th>NumAliphaticCarbocycles</th>\n",
       "      <th>NumAliphaticHeterocycles</th>\n",
       "      <th>NumAliphaticRings</th>\n",
       "      <th>NumAromaticCarbocycles</th>\n",
       "      <th>NumAromaticHeterocycles</th>\n",
       "      <th>NumAromaticRings</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHeteroatoms</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>NumSaturatedCarbocycles</th>\n",
       "      <th>NumSaturatedHeterocycles</th>\n",
       "      <th>NumSaturatedRings</th>\n",
       "      <th>RingCount</th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>MolMR</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>fr_Ar_OH</th>\n",
       "      <th>fr_COO</th>\n",
       "      <th>fr_COO2</th>\n",
       "      <th>fr_C_O</th>\n",
       "      <th>fr_C_O_noCOO</th>\n",
       "      <th>fr_C_S</th>\n",
       "      <th>fr_HOCCN</th>\n",
       "      <th>fr_Imine</th>\n",
       "      <th>fr_NH0</th>\n",
       "      <th>fr_NH1</th>\n",
       "      <th>fr_NH2</th>\n",
       "      <th>fr_N_O</th>\n",
       "      <th>fr_Ndealkylation1</th>\n",
       "      <th>fr_Ndealkylation2</th>\n",
       "      <th>fr_Nhpyrrole</th>\n",
       "      <th>fr_SH</th>\n",
       "      <th>fr_aldehyde</th>\n",
       "      <th>fr_alkyl_carbamate</th>\n",
       "      <th>fr_alkyl_halide</th>\n",
       "      <th>fr_allylic_oxid</th>\n",
       "      <th>fr_amide</th>\n",
       "      <th>fr_amidine</th>\n",
       "      <th>fr_aniline</th>\n",
       "      <th>fr_aryl_methyl</th>\n",
       "      <th>fr_azide</th>\n",
       "      <th>fr_azo</th>\n",
       "      <th>fr_barbitur</th>\n",
       "      <th>fr_benzene</th>\n",
       "      <th>fr_benzodiazepine</th>\n",
       "      <th>fr_bicyclic</th>\n",
       "      <th>fr_diazo</th>\n",
       "      <th>fr_dihydropyridine</th>\n",
       "      <th>fr_epoxide</th>\n",
       "      <th>fr_ester</th>\n",
       "      <th>fr_ether</th>\n",
       "      <th>fr_furan</th>\n",
       "      <th>fr_guanido</th>\n",
       "      <th>fr_halogen</th>\n",
       "      <th>fr_hdrzine</th>\n",
       "      <th>fr_hdrzone</th>\n",
       "      <th>fr_imidazole</th>\n",
       "      <th>fr_imide</th>\n",
       "      <th>fr_isocyan</th>\n",
       "      <th>fr_isothiocyan</th>\n",
       "      <th>fr_ketone</th>\n",
       "      <th>fr_ketone_Topliss</th>\n",
       "      <th>fr_lactam</th>\n",
       "      <th>fr_lactone</th>\n",
       "      <th>fr_methoxy</th>\n",
       "      <th>fr_morpholine</th>\n",
       "      <th>fr_nitrile</th>\n",
       "      <th>fr_nitro</th>\n",
       "      <th>fr_nitro_arom</th>\n",
       "      <th>fr_nitro_arom_nonortho</th>\n",
       "      <th>fr_nitroso</th>\n",
       "      <th>fr_oxazole</th>\n",
       "      <th>fr_oxime</th>\n",
       "      <th>fr_para_hydroxylation</th>\n",
       "      <th>fr_phenol</th>\n",
       "      <th>fr_phenol_noOrthoHbond</th>\n",
       "      <th>fr_phos_acid</th>\n",
       "      <th>fr_phos_ester</th>\n",
       "      <th>fr_piperdine</th>\n",
       "      <th>fr_piperzine</th>\n",
       "      <th>fr_priamide</th>\n",
       "      <th>fr_prisulfonamd</th>\n",
       "      <th>fr_pyridine</th>\n",
       "      <th>fr_quatN</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.239374</td>\n",
       "      <td>175.482382</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.417362</td>\n",
       "      <td>42.928571</td>\n",
       "      <td>384.652</td>\n",
       "      <td>340.300</td>\n",
       "      <td>384.350449</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>1.321429</td>\n",
       "      <td>14.822266</td>\n",
       "      <td>9.700470</td>\n",
       "      <td>2.600532</td>\n",
       "      <td>-2.343082</td>\n",
       "      <td>2.644698</td>\n",
       "      <td>-2.322229</td>\n",
       "      <td>5.944519</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.164038</td>\n",
       "      <td>611.920301</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>20.606247</td>\n",
       "      <td>6.947534</td>\n",
       "      <td>2.868737</td>\n",
       "      <td>173.630124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>64.208216</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.188192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.807589</td>\n",
       "      <td>1.764908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.258223</td>\n",
       "      <td>16.981087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1212</td>\n",
       "      <td>121.5300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.771831</td>\n",
       "      <td>5.402819</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.462473</td>\n",
       "      <td>45.214286</td>\n",
       "      <td>388.684</td>\n",
       "      <td>340.300</td>\n",
       "      <td>388.381750</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>-0.313407</td>\n",
       "      <td>0.313407</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>14.975110</td>\n",
       "      <td>9.689226</td>\n",
       "      <td>2.614066</td>\n",
       "      <td>-2.394690</td>\n",
       "      <td>2.658342</td>\n",
       "      <td>-2.444817</td>\n",
       "      <td>5.134527</td>\n",
       "      <td>0.120322</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.080362</td>\n",
       "      <td>516.780124</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>21.163454</td>\n",
       "      <td>7.257648</td>\n",
       "      <td>3.027177</td>\n",
       "      <td>174.939204</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>97.951860</td>\n",
       "      <td>12.083682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.173194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>77.297729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.922833</td>\n",
       "      <td>2.153503</td>\n",
       "      <td>1.914377</td>\n",
       "      <td>1.536674</td>\n",
       "      <td>14.135381</td>\n",
       "      <td>17.670565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1556</td>\n",
       "      <td>120.5074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>223.808778</td>\n",
       "      <td>161.142320</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.260923</td>\n",
       "      <td>42.187500</td>\n",
       "      <td>446.808</td>\n",
       "      <td>388.344</td>\n",
       "      <td>446.458903</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>-0.325573</td>\n",
       "      <td>0.325573</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.156250</td>\n",
       "      <td>15.353938</td>\n",
       "      <td>9.681293</td>\n",
       "      <td>2.665274</td>\n",
       "      <td>-2.477203</td>\n",
       "      <td>2.679014</td>\n",
       "      <td>-2.565224</td>\n",
       "      <td>5.117187</td>\n",
       "      <td>-0.922902</td>\n",
       "      <td>3.214947</td>\n",
       "      <td>1.219066</td>\n",
       "      <td>643.620154</td>\n",
       "      <td>23.794682</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>14.595754</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>8.610751e+06</td>\n",
       "      <td>25.026112</td>\n",
       "      <td>7.709373</td>\n",
       "      <td>3.470070</td>\n",
       "      <td>201.238858</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>23.671624</td>\n",
       "      <td>53.363882</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>41.280201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.329944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>86.263791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.733111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.517630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.184127</td>\n",
       "      <td>1.930720</td>\n",
       "      <td>1.738402</td>\n",
       "      <td>14.491619</td>\n",
       "      <td>18.287216</td>\n",
       "      <td>10.183618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1292</td>\n",
       "      <td>138.4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.705624</td>\n",
       "      <td>107.855654</td>\n",
       "      <td>63.235294</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.377846</td>\n",
       "      <td>41.862069</td>\n",
       "      <td>398.679</td>\n",
       "      <td>352.311</td>\n",
       "      <td>398.366099</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>14.821216</td>\n",
       "      <td>9.700497</td>\n",
       "      <td>2.600529</td>\n",
       "      <td>-2.342885</td>\n",
       "      <td>2.644709</td>\n",
       "      <td>-2.322030</td>\n",
       "      <td>5.944502</td>\n",
       "      <td>0.193510</td>\n",
       "      <td>3.179270</td>\n",
       "      <td>1.120513</td>\n",
       "      <td>626.651366</td>\n",
       "      <td>20.916003</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>13.627794</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>3.572142e+06</td>\n",
       "      <td>21.567454</td>\n",
       "      <td>7.485204</td>\n",
       "      <td>3.263848</td>\n",
       "      <td>179.995066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.804888</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>70.629038</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.194720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.827852</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.695439</td>\n",
       "      <td>17.012013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.5113</td>\n",
       "      <td>126.1470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>107.131532</td>\n",
       "      <td>139.270991</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.429038</td>\n",
       "      <td>36.514286</td>\n",
       "      <td>466.713</td>\n",
       "      <td>424.377</td>\n",
       "      <td>466.334799</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>-0.257239</td>\n",
       "      <td>0.257239</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.257143</td>\n",
       "      <td>14.831112</td>\n",
       "      <td>9.700386</td>\n",
       "      <td>2.602486</td>\n",
       "      <td>-2.342009</td>\n",
       "      <td>2.648473</td>\n",
       "      <td>-2.318893</td>\n",
       "      <td>5.963448</td>\n",
       "      <td>0.193687</td>\n",
       "      <td>3.337074</td>\n",
       "      <td>1.136678</td>\n",
       "      <td>1101.164252</td>\n",
       "      <td>24.639617</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>16.526773</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>1.053758e+08</td>\n",
       "      <td>23.194917</td>\n",
       "      <td>7.639211</td>\n",
       "      <td>3.345855</td>\n",
       "      <td>211.919602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.807891</td>\n",
       "      <td>103.003916</td>\n",
       "      <td>22.253351</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.798143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>86.488175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.657840</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>91.194256</td>\n",
       "      <td>58.515746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>29.631406</td>\n",
       "      <td>61.075203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.073360</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.301020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.071783</td>\n",
       "      <td>1.605178</td>\n",
       "      <td>17.869058</td>\n",
       "      <td>8.627311</td>\n",
       "      <td>14.692318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9.1148</td>\n",
       "      <td>148.3380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>996</td>\n",
       "      <td>31.000104</td>\n",
       "      <td>34.999650</td>\n",
       "      <td>1.129017</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>-0.476142</td>\n",
       "      <td>0.382752</td>\n",
       "      <td>49.133333</td>\n",
       "      <td>414.542</td>\n",
       "      <td>380.270</td>\n",
       "      <td>414.240624</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>1.866667</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>16.586886</td>\n",
       "      <td>9.344314</td>\n",
       "      <td>2.726237</td>\n",
       "      <td>-2.677345</td>\n",
       "      <td>2.739076</td>\n",
       "      <td>-2.646743</td>\n",
       "      <td>5.980114</td>\n",
       "      <td>-0.196385</td>\n",
       "      <td>3.023764</td>\n",
       "      <td>1.646946</td>\n",
       "      <td>857.600295</td>\n",
       "      <td>21.637464</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>14.097861</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>6.242348e+06</td>\n",
       "      <td>20.263719</td>\n",
       "      <td>6.198453</td>\n",
       "      <td>2.219273</td>\n",
       "      <td>178.490760</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>24.360600</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.017713</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.67</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.409521</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.503509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.515343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498752</td>\n",
       "      <td>-0.405436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.985276</td>\n",
       "      <td>8.824371</td>\n",
       "      <td>1.494852</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.3002</td>\n",
       "      <td>109.8350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>997</td>\n",
       "      <td>31.999934</td>\n",
       "      <td>33.999415</td>\n",
       "      <td>1.062484</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>0.369425</td>\n",
       "      <td>44.542857</td>\n",
       "      <td>485.621</td>\n",
       "      <td>446.309</td>\n",
       "      <td>485.277738</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>-0.467493</td>\n",
       "      <td>0.467493</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.457143</td>\n",
       "      <td>16.586914</td>\n",
       "      <td>9.343622</td>\n",
       "      <td>2.725543</td>\n",
       "      <td>-2.679467</td>\n",
       "      <td>2.738755</td>\n",
       "      <td>-2.655659</td>\n",
       "      <td>5.980828</td>\n",
       "      <td>-0.187625</td>\n",
       "      <td>3.130958</td>\n",
       "      <td>1.535171</td>\n",
       "      <td>1016.917688</td>\n",
       "      <td>25.499271</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>16.402391</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>5.897229e+07</td>\n",
       "      <td>24.511583</td>\n",
       "      <td>7.908743</td>\n",
       "      <td>3.147136</td>\n",
       "      <td>207.296970</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>23.815096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.185457</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.966734</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>73.143616</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>23.344043</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.347395</td>\n",
       "      <td>5.917906</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.086396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.516353</td>\n",
       "      <td>2.923049</td>\n",
       "      <td>0.162524</td>\n",
       "      <td>-1.300354</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>7.521836</td>\n",
       "      <td>10.378794</td>\n",
       "      <td>1.327425</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3.8049</td>\n",
       "      <td>127.4397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>998</td>\n",
       "      <td>30.999883</td>\n",
       "      <td>33.999458</td>\n",
       "      <td>1.096761</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>0.026535</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>41.973684</td>\n",
       "      <td>545.742</td>\n",
       "      <td>502.398</td>\n",
       "      <td>545.281109</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>-0.467485</td>\n",
       "      <td>0.467485</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>1.894737</td>\n",
       "      <td>2.552632</td>\n",
       "      <td>32.166365</td>\n",
       "      <td>9.343613</td>\n",
       "      <td>2.725818</td>\n",
       "      <td>-2.679527</td>\n",
       "      <td>2.738943</td>\n",
       "      <td>-2.656447</td>\n",
       "      <td>7.980998</td>\n",
       "      <td>-0.187687</td>\n",
       "      <td>3.204255</td>\n",
       "      <td>1.493776</td>\n",
       "      <td>1070.961298</td>\n",
       "      <td>27.620591</td>\n",
       "      <td>23.633394</td>\n",
       "      <td>24.449891</td>\n",
       "      <td>17.940396</td>\n",
       "      <td>14.301838</td>\n",
       "      <td>15.695685</td>\n",
       "      <td>13.248561</td>\n",
       "      <td>14.234160</td>\n",
       "      <td>11.326709</td>\n",
       "      <td>11.970659</td>\n",
       "      <td>9.725583</td>\n",
       "      <td>10.196987</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>2.627956e+08</td>\n",
       "      <td>27.726151</td>\n",
       "      <td>9.668673</td>\n",
       "      <td>3.822745</td>\n",
       "      <td>230.149965</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>79.620167</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>35.576981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>78.682541</td>\n",
       "      <td>19.118420</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>48.975357</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>72.640700</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>28.759033</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.932405</td>\n",
       "      <td>12.338728</td>\n",
       "      <td>44.277783</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>6.255769</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.305031</td>\n",
       "      <td>1.639399</td>\n",
       "      <td>52.527620</td>\n",
       "      <td>3.081660</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>-0.487671</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>10.055493</td>\n",
       "      <td>8.774745</td>\n",
       "      <td>1.364715</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.5381</td>\n",
       "      <td>144.7647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>999</td>\n",
       "      <td>31.998959</td>\n",
       "      <td>32.999644</td>\n",
       "      <td>1.031272</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>0.146522</td>\n",
       "      <td>-1.408652</td>\n",
       "      <td>0.381559</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>522.635</td>\n",
       "      <td>480.299</td>\n",
       "      <td>522.282883</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>-0.468755</td>\n",
       "      <td>0.468755</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.351351</td>\n",
       "      <td>1.864865</td>\n",
       "      <td>16.540061</td>\n",
       "      <td>9.364015</td>\n",
       "      <td>2.730109</td>\n",
       "      <td>-2.652209</td>\n",
       "      <td>2.704027</td>\n",
       "      <td>-2.678553</td>\n",
       "      <td>5.950258</td>\n",
       "      <td>-0.225309</td>\n",
       "      <td>2.887043</td>\n",
       "      <td>2.325807</td>\n",
       "      <td>957.299494</td>\n",
       "      <td>27.921921</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>17.309003</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>7.702780e+07</td>\n",
       "      <td>29.111081</td>\n",
       "      <td>10.369092</td>\n",
       "      <td>4.164473</td>\n",
       "      <td>218.836986</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>5.783245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.877221</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.192033</td>\n",
       "      <td>56.278648</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>51.104983</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>29.660466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.099656</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.27</td>\n",
       "      <td>63.742418</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.512100</td>\n",
       "      <td>19.262465</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>13.847474</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>20.885832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.303382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.785593</td>\n",
       "      <td>-6.848660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.955837</td>\n",
       "      <td>7.488627</td>\n",
       "      <td>5.083909</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.3649</td>\n",
       "      <td>131.7080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1000</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>-0.447978</td>\n",
       "      <td>0.452565</td>\n",
       "      <td>48.580645</td>\n",
       "      <td>426.597</td>\n",
       "      <td>388.293</td>\n",
       "      <td>426.277010</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>1.064516</td>\n",
       "      <td>1.774194</td>\n",
       "      <td>2.451613</td>\n",
       "      <td>16.525216</td>\n",
       "      <td>9.330327</td>\n",
       "      <td>2.704274</td>\n",
       "      <td>-2.696212</td>\n",
       "      <td>2.737481</td>\n",
       "      <td>-2.668850</td>\n",
       "      <td>5.978085</td>\n",
       "      <td>-0.201381</td>\n",
       "      <td>2.740713</td>\n",
       "      <td>1.651446</td>\n",
       "      <td>870.462214</td>\n",
       "      <td>22.344571</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>14.597861</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>9.086032e+06</td>\n",
       "      <td>21.398566</td>\n",
       "      <td>6.776167</td>\n",
       "      <td>2.566599</td>\n",
       "      <td>186.107099</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>11.566490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.969305</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>30.092446</td>\n",
       "      <td>12.524788</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>17.535795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.645593</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.44</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.470910</td>\n",
       "      <td>36.243945</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>5.298868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.472742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776999</td>\n",
       "      <td>1.605640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.247616</td>\n",
       "      <td>8.999949</td>\n",
       "      <td>1.514853</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.1488</td>\n",
       "      <td>117.9840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    IC50, mM    CC50, mM         SI  MaxAbsEStateIndex  \\\n",
       "0             0    6.239374  175.482382  28.125000           5.094096   \n",
       "1             1    0.771831    5.402819   7.000000           3.961417   \n",
       "2             2  223.808778  161.142320   0.720000           2.627117   \n",
       "3             3    1.705624  107.855654  63.235294           5.097360   \n",
       "4             4  107.131532  139.270991   1.300000           5.150510   \n",
       "..          ...         ...         ...        ...                ...   \n",
       "993         996   31.000104   34.999650   1.129017          12.934891   \n",
       "994         997   31.999934   33.999415   1.062484          13.635345   \n",
       "995         998   30.999883   33.999458   1.096761          13.991690   \n",
       "996         999   31.998959   32.999644   1.031272          13.830180   \n",
       "997        1000   99.999531   99.999531   1.000000          13.380863   \n",
       "\n",
       "     MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex       qed        SPS  \\\n",
       "0          5.094096           0.387225        0.387225  0.417362  42.928571   \n",
       "1          3.961417           0.533868        0.533868  0.462473  45.214286   \n",
       "2          2.627117           0.543231        0.543231  0.260923  42.187500   \n",
       "3          5.097360           0.390603        0.390603  0.377846  41.862069   \n",
       "4          5.150510           0.270476        0.270476  0.429038  36.514286   \n",
       "..              ...                ...             ...       ...        ...   \n",
       "993       12.934891           0.048029       -0.476142  0.382752  49.133333   \n",
       "994       13.635345           0.030329       -0.699355  0.369425  44.542857   \n",
       "995       13.991690           0.026535       -0.650790  0.284923  41.973684   \n",
       "996       13.830180           0.146522       -1.408652  0.381559  39.000000   \n",
       "997       13.380863           0.002425       -0.447978  0.452565  48.580645   \n",
       "\n",
       "       MolWt  HeavyAtomMolWt  ExactMolWt  NumValenceElectrons  \\\n",
       "0    384.652         340.300  384.350449                  158   \n",
       "1    388.684         340.300  388.381750                  162   \n",
       "2    446.808         388.344  446.458903                  186   \n",
       "3    398.679         352.311  398.366099                  164   \n",
       "4    466.713         424.377  466.334799                  184   \n",
       "..       ...             ...         ...                  ...   \n",
       "993  414.542         380.270  414.240624                  164   \n",
       "994  485.621         446.309  485.277738                  192   \n",
       "995  545.742         502.398  545.281109                  210   \n",
       "996  522.635         480.299  522.282883                  208   \n",
       "997  426.597         388.293  426.277010                  170   \n",
       "\n",
       "     NumRadicalElectrons  MaxPartialCharge  MinPartialCharge  \\\n",
       "0                      0          0.038844         -0.293526   \n",
       "1                      0          0.012887         -0.313407   \n",
       "2                      0          0.094802         -0.325573   \n",
       "3                      0          0.038844         -0.293526   \n",
       "4                      0          0.062897         -0.257239   \n",
       "..                   ...               ...               ...   \n",
       "993                    0          0.317890         -0.468587   \n",
       "994                    0          0.327562         -0.467493   \n",
       "995                    0          0.327887         -0.467485   \n",
       "996                    0          0.312509         -0.468755   \n",
       "997                    0          0.311311         -0.468587   \n",
       "\n",
       "     MaxAbsPartialCharge  MinAbsPartialCharge  FpDensityMorgan1  \\\n",
       "0               0.293526             0.038844          0.642857   \n",
       "1               0.313407             0.012887          0.607143   \n",
       "2               0.325573             0.094802          0.562500   \n",
       "3               0.293526             0.038844          0.620690   \n",
       "4               0.257239             0.062897          0.600000   \n",
       "..                   ...                  ...               ...   \n",
       "993             0.468587             0.317890          1.133333   \n",
       "994             0.467493             0.327562          1.085714   \n",
       "995             0.467485             0.327887          1.157895   \n",
       "996             0.468755             0.312509          0.756757   \n",
       "997             0.468587             0.311311          1.064516   \n",
       "\n",
       "     FpDensityMorgan2  FpDensityMorgan3  BCUT2D_MWHI  BCUT2D_MWLOW  \\\n",
       "0            1.035714          1.321429    14.822266      9.700470   \n",
       "1            1.000000          1.285714    14.975110      9.689226   \n",
       "2            0.906250          1.156250    15.353938      9.681293   \n",
       "3            1.000000          1.310345    14.821216      9.700497   \n",
       "4            0.971429          1.257143    14.831112      9.700386   \n",
       "..                ...               ...          ...           ...   \n",
       "993          1.866667          2.533333    16.586886      9.344314   \n",
       "994          1.800000          2.457143    16.586914      9.343622   \n",
       "995          1.894737          2.552632    32.166365      9.343613   \n",
       "996          1.351351          1.864865    16.540061      9.364015   \n",
       "997          1.774194          2.451613    16.525216      9.330327   \n",
       "\n",
       "     BCUT2D_CHGHI  BCUT2D_CHGLO  BCUT2D_LOGPHI  BCUT2D_LOGPLOW  BCUT2D_MRHI  \\\n",
       "0        2.600532     -2.343082       2.644698       -2.322229     5.944519   \n",
       "1        2.614066     -2.394690       2.658342       -2.444817     5.134527   \n",
       "2        2.665274     -2.477203       2.679014       -2.565224     5.117187   \n",
       "3        2.600529     -2.342885       2.644709       -2.322030     5.944502   \n",
       "4        2.602486     -2.342009       2.648473       -2.318893     5.963448   \n",
       "..            ...           ...            ...             ...          ...   \n",
       "993      2.726237     -2.677345       2.739076       -2.646743     5.980114   \n",
       "994      2.725543     -2.679467       2.738755       -2.655659     5.980828   \n",
       "995      2.725818     -2.679527       2.738943       -2.656447     7.980998   \n",
       "996      2.730109     -2.652209       2.704027       -2.678553     5.950258   \n",
       "997      2.704274     -2.696212       2.737481       -2.668850     5.978085   \n",
       "\n",
       "     BCUT2D_MRLOW    AvgIpc  BalabanJ      BertzCT       Chi0      Chi0n  \\\n",
       "0        0.193481  3.150503  1.164038   611.920301  20.208896  19.534409   \n",
       "1        0.120322  3.150503  1.080362   516.780124  20.208896  19.794682   \n",
       "2       -0.922902  3.214947  1.219066   643.620154  23.794682  23.689110   \n",
       "3        0.193510  3.179270  1.120513   626.651366  20.916003  20.241516   \n",
       "4        0.193687  3.337074  1.136678  1101.164252  24.639617  22.617677   \n",
       "..            ...       ...       ...          ...        ...        ...   \n",
       "993     -0.196385  3.023764  1.646946   857.600295  21.637464  18.825334   \n",
       "994     -0.187625  3.130958  1.535171  1016.917688  25.499271  21.810933   \n",
       "995     -0.187687  3.204255  1.493776  1070.961298  27.620591  23.633394   \n",
       "996     -0.225309  2.887043  2.325807   957.299494  27.921921  23.380977   \n",
       "997     -0.201381  2.740713  1.651446   870.462214  22.344571  19.831299   \n",
       "\n",
       "         Chi0v       Chi1      Chi1n      Chi1v      Chi2n      Chi2v  \\\n",
       "0    19.534409  13.127794  12.204226  12.204226  12.058078  12.058078   \n",
       "1    19.794682  13.127794  12.595754  12.595754  12.648545  12.648545   \n",
       "2    23.689110  14.595754  14.249005  14.249005  15.671216  15.671216   \n",
       "3    20.241516  13.627794  12.704226  12.704226  12.411631  12.411631   \n",
       "4    22.617677  16.526773  13.868825  13.868825  13.613700  13.613700   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "993  18.825334  14.097861  11.665192  11.665192  11.409461  11.409461   \n",
       "994  21.810933  16.402391  13.274017  13.274017  12.636189  12.636189   \n",
       "995  24.449891  17.940396  14.301838  15.695685  13.248561  14.234160   \n",
       "996  23.380977  17.309003  13.174959  13.174959  11.893254  11.893254   \n",
       "997  19.831299  14.597861  12.464050  12.464050  12.101953  12.101953   \n",
       "\n",
       "         Chi3n      Chi3v      Chi4n      Chi4v  HallKierAlpha           Ipc  \\\n",
       "0    10.695991  10.695991   7.340247   7.340247          -0.66  2.187750e+06   \n",
       "1    11.473090  11.473090   8.180905   8.180905          -0.08  2.187750e+06   \n",
       "2    13.402236  13.402236  10.140303  10.140303          -0.08  8.610751e+06   \n",
       "3    10.945991  10.945991   7.517023   7.517023          -0.66  3.572142e+06   \n",
       "4    11.833480  11.833480   8.119076   8.119076          -2.22  1.053758e+08   \n",
       "..         ...        ...        ...        ...            ...           ...   \n",
       "993  10.058026  10.058026   8.981266   8.981266          -1.65  6.242348e+06   \n",
       "994  10.827369  10.827369   9.372775   9.372775          -2.18  5.897229e+07   \n",
       "995  11.326709  11.970659   9.725583  10.196987          -1.83  2.627956e+08   \n",
       "996  10.158570  10.158570   8.609327   8.609327          -2.45  7.702780e+07   \n",
       "997  10.667206  10.667206   9.563076   9.563076          -1.45  9.086032e+06   \n",
       "\n",
       "        Kappa1     Kappa2    Kappa3   LabuteASA  PEOE_VSA1  PEOE_VSA10  \\\n",
       "0    20.606247   6.947534  2.868737  173.630124   0.000000    0.000000   \n",
       "1    21.163454   7.257648  3.027177  174.939204  10.633577    0.000000   \n",
       "2    25.026112   7.709373  3.470070  201.238858   8.966062    0.000000   \n",
       "3    21.567454   7.485204  3.263848  179.995066   0.000000    0.000000   \n",
       "4    23.194917   7.639211  3.345855  211.919602   0.000000    0.000000   \n",
       "..         ...        ...       ...         ...        ...         ...   \n",
       "993  20.263719   6.198453  2.219273  178.490760   9.473726    0.000000   \n",
       "994  24.511583   7.908743  3.147136  207.296970  14.790515    6.041841   \n",
       "995  27.726151   9.668673  3.822745  230.149965  14.790515    6.041841   \n",
       "996  29.111081  10.369092  4.164473  218.836986  18.947452    5.783245   \n",
       "997  21.398566   6.776167  2.566599  186.107099   4.736863   11.566490   \n",
       "\n",
       "     PEOE_VSA11  PEOE_VSA12  PEOE_VSA13  PEOE_VSA14  PEOE_VSA2  PEOE_VSA3  \\\n",
       "0           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "1           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "2           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "3           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "4           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "..          ...         ...         ...         ...        ...        ...   \n",
       "993         0.0     0.00000         0.0   17.907916  14.383612   0.000000   \n",
       "994         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "995         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "996         0.0     0.00000         0.0   23.877221  23.972686   0.000000   \n",
       "997         0.0     0.00000         0.0    5.969305  14.383612   0.000000   \n",
       "\n",
       "     PEOE_VSA4  PEOE_VSA5  PEOE_VSA6   PEOE_VSA7  PEOE_VSA8  PEOE_VSA9  \\\n",
       "0     0.000000        0.0  54.384066   74.032366  35.342864   0.000000   \n",
       "1     0.000000        0.0  54.384066   97.951860  12.083682   0.000000   \n",
       "2     0.000000        0.0  41.542423   74.032366  23.671624  53.363882   \n",
       "3     0.000000        0.0  60.804888   74.032366  35.342864   0.000000   \n",
       "4     0.000000        0.0  65.807891  103.003916  22.253351  11.374773   \n",
       "..         ...        ...        ...         ...        ...        ...   \n",
       "993   0.000000        0.0  38.841158   68.114460   5.414990  24.360600   \n",
       "994   0.000000        0.0  45.764895   68.114460  10.829981  18.945610   \n",
       "995  11.761885        0.0  45.764895   79.620167  10.829981  18.945610   \n",
       "996   0.000000        0.0  27.192033   56.278648  11.835812  51.104983   \n",
       "997   0.000000        0.0  38.841158   68.114460  30.092446  12.524788   \n",
       "\n",
       "      SMR_VSA1  SMR_VSA10  SMR_VSA2   SMR_VSA3   SMR_VSA4    SMR_VSA5  \\\n",
       "0     0.000000  11.423370       0.0   0.000000  43.480583  105.750639   \n",
       "1     0.000000   0.000000       0.0  10.633577  33.495774  117.834321   \n",
       "2     8.966062   0.000000       0.0   0.000000  33.495774  117.834321   \n",
       "3     0.000000  11.423370       0.0   0.000000  43.480583  112.171461   \n",
       "4     0.000000  22.798143       0.0   0.000000  43.480583   86.488175   \n",
       "..         ...        ...       ...        ...        ...         ...   \n",
       "993  23.857337  17.907916       0.0   0.000000  51.752408   66.219879   \n",
       "994  28.651875  23.815096       0.0   5.316789  51.752408   79.185457   \n",
       "995  28.651875  35.576981       0.0   5.316789  51.752408   78.682541   \n",
       "996  42.920138  29.660466       0.0   0.000000  51.752408   66.219879   \n",
       "997  19.120475  17.535795       0.0   0.000000  51.752408   79.061522   \n",
       "\n",
       "      SMR_VSA6   SMR_VSA7  SMR_VSA8  SMR_VSA9  SlogP_VSA1  SlogP_VSA10  \\\n",
       "0    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "1    13.089513   0.000000         0       0.0   10.633577     0.000000   \n",
       "2    41.280201   0.000000         0       0.0    0.000000     0.000000   \n",
       "3    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "4     0.000000  59.657840         0       0.0    0.000000    11.374773   \n",
       "..         ...        ...       ...       ...         ...          ...   \n",
       "993   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "994   7.109798  11.649125         0       0.0    5.316789     0.000000   \n",
       "995  19.118420  11.649125         0       0.0    5.316789     0.000000   \n",
       "996  28.439190   0.000000         0       0.0    0.000000     0.000000   \n",
       "997   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "\n",
       "     SlogP_VSA11  SlogP_VSA12  SlogP_VSA2  SlogP_VSA3  SlogP_VSA4  SlogP_VSA5  \\\n",
       "0            0.0     0.000000   24.512883    0.000000   33.495774  105.750639   \n",
       "1            0.0     0.000000   25.173194    0.000000   33.495774  105.750639   \n",
       "2            0.0     0.000000   62.329944    0.000000   33.495774  105.750639   \n",
       "3            0.0     0.000000   24.512883    0.000000   33.495774  112.171461   \n",
       "4            0.0     0.000000   11.423370    6.420822   33.495774   91.194256   \n",
       "..           ...          ...         ...         ...         ...         ...   \n",
       "993          0.0     0.000000   25.017713   23.857337   51.752408   66.219879   \n",
       "994          0.0     0.000000   36.966734   28.651875   51.752408   73.143616   \n",
       "995          0.0    11.761885   48.975357   28.651875   51.752408   72.640700   \n",
       "996          0.0     0.000000   58.099656   42.920138   51.752408   66.219879   \n",
       "997          0.0     0.000000   24.645593   19.120475   51.752408   79.061522   \n",
       "\n",
       "     SlogP_VSA6  SlogP_VSA7  SlogP_VSA8  SlogP_VSA9    TPSA  EState_VSA1  \\\n",
       "0      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "1      0.000000         0.0         0.0           0   24.06     0.000000   \n",
       "2      0.000000         0.0         0.0           0    0.00     0.000000   \n",
       "3      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "4     58.515746         0.0         0.0           0   24.72     0.000000   \n",
       "..          ...         ...         ...         ...     ...          ...   \n",
       "993   11.649125         0.0         0.0           0   69.67     5.414990   \n",
       "994   11.649125         0.0         0.0           0   98.77    23.344043   \n",
       "995   11.649125         0.0         0.0           0   98.77    28.759033   \n",
       "996    0.000000         0.0         0.0           0  122.27    63.742418   \n",
       "997   11.649125         0.0         0.0           0   60.44     5.414990   \n",
       "\n",
       "     EState_VSA10  EState_VSA11  EState_VSA2  EState_VSA3  EState_VSA4  \\\n",
       "0        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "1        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "2        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "3        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "4        0.000000           0.0    10.829981    10.829981    29.631406   \n",
       "..            ...           ...          ...          ...          ...   \n",
       "993     14.383612           0.0    52.409521    11.835812    38.524930   \n",
       "994     19.178149           0.0    52.347395     5.917906    38.524930   \n",
       "995     19.178149           0.0    46.932405    12.338728    44.277783   \n",
       "996     23.972686           0.0    30.512100    19.262465     6.420822   \n",
       "997     14.383612           0.0    40.470910    36.243945    38.524930   \n",
       "\n",
       "     EState_VSA5  EState_VSA6  EState_VSA7  EState_VSA8  EState_VSA9  \\\n",
       "0      64.208216    11.423370     0.000000    41.542423     9.984809   \n",
       "1      77.297729     0.000000     0.000000    52.176000     0.000000   \n",
       "2      86.263791     0.000000     0.000000    69.733111     0.000000   \n",
       "3      70.629038    11.423370     0.000000    41.542423     9.984809   \n",
       "4      61.075203     0.000000     0.000000    90.073360     9.984809   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    12.682902     0.000000     0.000000    33.770969     9.473726   \n",
       "994    12.682902     6.923737     0.000000    39.087758     9.473726   \n",
       "995    12.682902    11.761885     6.255769    39.087758     9.473726   \n",
       "996    28.439190    13.847474     6.923737     6.923737    18.947452   \n",
       "997    12.682902     0.000000     0.000000    33.770969     4.736863   \n",
       "\n",
       "     VSA_EState1  VSA_EState10  VSA_EState2  VSA_EState3  VSA_EState4  \\\n",
       "0       0.000000      0.000000    10.188192     0.000000     4.807589   \n",
       "1       0.000000      0.000000     0.000000     7.922833     2.153503   \n",
       "2       2.517630      0.000000     0.000000     0.000000     2.184127   \n",
       "3       0.000000      0.000000    10.194720     0.000000     4.827852   \n",
       "4       0.000000      0.000000    10.301020     0.000000     9.071783   \n",
       "..           ...           ...          ...          ...          ...   \n",
       "993    10.503509      0.000000    38.515343     0.000000     0.498752   \n",
       "994    10.086396      0.000000    51.516353     2.923049     0.162524   \n",
       "995    10.305031      1.639399    52.527620     3.081660     0.139799   \n",
       "996    20.885832      0.000000    67.303382     0.000000    -2.785593   \n",
       "997     5.298868      0.000000    39.472742     0.000000     0.776999   \n",
       "\n",
       "     VSA_EState5  VSA_EState6  VSA_EState7  VSA_EState8  VSA_EState9  \\\n",
       "0       1.764908     0.000000    13.258223    16.981087     0.000000   \n",
       "1       1.914377     1.536674    14.135381    17.670565     0.000000   \n",
       "2       1.930720     1.738402    14.491619    18.287216    10.183618   \n",
       "3       1.769975     0.000000    14.695439    17.012013     0.000000   \n",
       "4       1.605178    17.869058     8.627311    14.692318     0.000000   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    -0.405436     0.000000     7.985276     8.824371     1.494852   \n",
       "994    -1.300354    -0.699355     7.521836    10.378794     1.327425   \n",
       "995    -0.487671    -0.650790    10.055493     8.774745     1.364715   \n",
       "996    -6.848660     0.000000     2.955837     7.488627     5.083909   \n",
       "997     1.605640     0.000000     9.247616     8.999949     1.514853   \n",
       "\n",
       "     FractionCSP3  HeavyAtomCount  NHOHCount  NOCount  \\\n",
       "0        0.923077              28          0        2   \n",
       "1        1.000000              28          2        2   \n",
       "2        1.000000              32          0        2   \n",
       "3        0.925926              29          0        2   \n",
       "4        0.575758              35          0        2   \n",
       "..            ...             ...        ...      ...   \n",
       "993      0.800000              30          0        5   \n",
       "994      0.785714              35          1        7   \n",
       "995      0.800000              38          1        7   \n",
       "996      0.821429              37          0        9   \n",
       "997      0.814815              31          0        4   \n",
       "\n",
       "     NumAliphaticCarbocycles  NumAliphaticHeterocycles  NumAliphaticRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        5                         1                  6   \n",
       "994                        5                         1                  6   \n",
       "995                        5                         1                  6   \n",
       "996                        3                         0                  3   \n",
       "997                        6                         0                  6   \n",
       "\n",
       "     NumAromaticCarbocycles  NumAromaticHeterocycles  NumAromaticRings  \\\n",
       "0                         0                        0                 0   \n",
       "1                         0                        0                 0   \n",
       "2                         0                        0                 0   \n",
       "3                         0                        0                 0   \n",
       "4                         2                        0                 2   \n",
       "..                      ...                      ...               ...   \n",
       "993                       0                        0                 0   \n",
       "994                       0                        0                 0   \n",
       "995                       0                        0                 0   \n",
       "996                       0                        0                 0   \n",
       "997                       0                        0                 0   \n",
       "\n",
       "     NumHAcceptors  NumHDonors  NumHeteroatoms  NumRotatableBonds  \\\n",
       "0                2           0               2                  7   \n",
       "1                2           2               2                  9   \n",
       "2                0           0               2                  9   \n",
       "3                2           0               2                  8   \n",
       "4                2           0               2                  4   \n",
       "..             ...         ...             ...                ...   \n",
       "993              5           0               5                  2   \n",
       "994              6           1               7                  4   \n",
       "995              7           1               8                  7   \n",
       "996              9           0               9                  6   \n",
       "997              4           0               4                  2   \n",
       "\n",
       "     NumSaturatedCarbocycles  NumSaturatedHeterocycles  NumSaturatedRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        3                         1                  4   \n",
       "994                        3                         1                  4   \n",
       "995                        3                         1                  4   \n",
       "996                        3                         0                  3   \n",
       "997                        4                         0                  4   \n",
       "\n",
       "     RingCount  MolLogP     MolMR  fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  \\\n",
       "0            4   7.1212  121.5300          0         0                0   \n",
       "1            4   6.1556  120.5074          0         0                0   \n",
       "2            4   7.1292  138.4528          0         0                0   \n",
       "3            4   7.5113  126.1470          0         0                0   \n",
       "4            6   9.1148  148.3380          0         0                0   \n",
       "..         ...      ...       ...        ...       ...              ...   \n",
       "993          6   4.3002  109.8350          0         0                0   \n",
       "994          6   3.8049  127.4397          0         0                0   \n",
       "995          6   4.5381  144.7647          0         0                0   \n",
       "996          3   3.3649  131.7080          0         0                0   \n",
       "997          6   5.1488  117.9840          0         0                0   \n",
       "\n",
       "     fr_ArN  fr_Ar_COO  fr_Ar_N  fr_Ar_NH  fr_Ar_OH  fr_COO  fr_COO2  fr_C_O  \\\n",
       "0         0          0        0         0         0       0        0       0   \n",
       "1         0          0        0         0         0       0        0       0   \n",
       "2         0          0        0         0         0       0        0       0   \n",
       "3         0          0        0         0         0       0        0       0   \n",
       "4         0          0        0         0         0       0        0       0   \n",
       "..      ...        ...      ...       ...       ...     ...      ...     ...   \n",
       "993       0          0        0         0         0       0        0       3   \n",
       "994       0          0        0         0         0       0        0       4   \n",
       "995       0          0        0         0         0       0        0       4   \n",
       "996       0          0        0         0         0       0        0       5   \n",
       "997       0          0        0         0         0       0        0       3   \n",
       "\n",
       "     fr_C_O_noCOO  fr_C_S  fr_HOCCN  fr_Imine  fr_NH0  fr_NH1  fr_NH2  fr_N_O  \\\n",
       "0               0       0         0         2       2       0       0       0   \n",
       "1               0       0         0         0       0       2       0       0   \n",
       "2               0       0         0         0       2       0       0       0   \n",
       "3               0       0         0         2       2       0       0       0   \n",
       "4               0       0         0         2       2       0       0       0   \n",
       "..            ...     ...       ...       ...     ...     ...     ...     ...   \n",
       "993             3       0         0         0       0       0       0       0   \n",
       "994             4       0         0         0       0       1       0       0   \n",
       "995             4       0         0         0       0       1       0       0   \n",
       "996             5       0         0         0       0       0       0       0   \n",
       "997             3       0         0         0       0       0       0       0   \n",
       "\n",
       "     fr_Ndealkylation1  fr_Ndealkylation2  fr_Nhpyrrole  fr_SH  fr_aldehyde  \\\n",
       "0                    0                  0             0      0            0   \n",
       "1                    0                  0             0      0            0   \n",
       "2                    0                  0             0      0            0   \n",
       "3                    0                  0             0      0            0   \n",
       "4                    0                  0             0      0            0   \n",
       "..                 ...                ...           ...    ...          ...   \n",
       "993                  0                  0             0      0            0   \n",
       "994                  0                  0             0      0            0   \n",
       "995                  0                  0             0      0            0   \n",
       "996                  0                  0             0      0            0   \n",
       "997                  0                  0             0      0            0   \n",
       "\n",
       "     fr_alkyl_carbamate  fr_alkyl_halide  fr_allylic_oxid  fr_amide  \\\n",
       "0                     0                0                0         0   \n",
       "1                     0                0                0         0   \n",
       "2                     0                0                0         0   \n",
       "3                     0                0                0         0   \n",
       "4                     0                0                0         0   \n",
       "..                  ...              ...              ...       ...   \n",
       "993                   0                0                2         0   \n",
       "994                   0                0                2         1   \n",
       "995                   0                0                2         1   \n",
       "996                   0                0                0         0   \n",
       "997                   0                0                2         0   \n",
       "\n",
       "     fr_amidine  fr_aniline  fr_aryl_methyl  fr_azide  fr_azo  fr_barbitur  \\\n",
       "0             0           0               0         0       0            0   \n",
       "1             0           0               0         0       0            0   \n",
       "2             0           0               0         0       0            0   \n",
       "3             0           0               0         0       0            0   \n",
       "4             0           0               0         0       0            0   \n",
       "..          ...         ...             ...       ...     ...          ...   \n",
       "993           0           0               0         0       0            0   \n",
       "994           0           0               0         0       0            0   \n",
       "995           0           0               0         0       0            0   \n",
       "996           0           0               0         0       0            0   \n",
       "997           0           0               0         0       0            0   \n",
       "\n",
       "     fr_benzene  fr_benzodiazepine  fr_bicyclic  fr_diazo  fr_dihydropyridine  \\\n",
       "0             0                  0            4         0                   0   \n",
       "1             0                  0            4         0                   0   \n",
       "2             0                  0            4         0                   0   \n",
       "3             0                  0            4         0                   0   \n",
       "4             2                  0            4         0                   0   \n",
       "..          ...                ...          ...       ...                 ...   \n",
       "993           0                  0            1         0                   0   \n",
       "994           0                  0            1         0                   0   \n",
       "995           0                  0            1         0                   0   \n",
       "996           0                  0            3         0                   0   \n",
       "997           0                  0            1         0                   0   \n",
       "\n",
       "     fr_epoxide  fr_ester  fr_ether  fr_furan  fr_guanido  fr_halogen  \\\n",
       "0             0         0         0         0           0           0   \n",
       "1             0         0         0         0           0           0   \n",
       "2             0         0         0         0           0           0   \n",
       "3             0         0         0         0           0           0   \n",
       "4             0         0         0         0           0           0   \n",
       "..          ...       ...       ...       ...         ...         ...   \n",
       "993           0         3         2         0           0           0   \n",
       "994           0         3         2         0           0           0   \n",
       "995           0         3         2         0           0           0   \n",
       "996           0         4         4         0           0           0   \n",
       "997           0         1         1         0           0           0   \n",
       "\n",
       "     fr_hdrzine  fr_hdrzone  fr_imidazole  fr_imide  fr_isocyan  \\\n",
       "0             0           0             0         0           0   \n",
       "1             0           0             0         0           0   \n",
       "2             0           0             0         0           0   \n",
       "3             0           0             0         0           0   \n",
       "4             0           0             0         0           0   \n",
       "..          ...         ...           ...       ...         ...   \n",
       "993           0           0             0         0           0   \n",
       "994           0           0             0         0           0   \n",
       "995           0           0             0         0           0   \n",
       "996           0           0             0         0           0   \n",
       "997           0           0             0         0           0   \n",
       "\n",
       "     fr_isothiocyan  fr_ketone  fr_ketone_Topliss  fr_lactam  fr_lactone  \\\n",
       "0                 0          0                  0          0           0   \n",
       "1                 0          0                  0          0           0   \n",
       "2                 0          0                  0          0           0   \n",
       "3                 0          0                  0          0           0   \n",
       "4                 0          0                  0          0           0   \n",
       "..              ...        ...                ...        ...         ...   \n",
       "993               0          0                  0          0           2   \n",
       "994               0          0                  0          0           2   \n",
       "995               0          0                  0          0           2   \n",
       "996               0          1                  1          0           0   \n",
       "997               0          2                  2          0           0   \n",
       "\n",
       "     fr_methoxy  fr_morpholine  fr_nitrile  fr_nitro  fr_nitro_arom  \\\n",
       "0             0              0           0         0              0   \n",
       "1             0              0           0         0              0   \n",
       "2             0              0           0         0              0   \n",
       "3             0              0           0         0              0   \n",
       "4             0              0           0         0              0   \n",
       "..          ...            ...         ...       ...            ...   \n",
       "993           1              0           0         0              0   \n",
       "994           1              0           0         0              0   \n",
       "995           1              0           0         0              0   \n",
       "996           4              0           0         0              0   \n",
       "997           1              0           0         0              0   \n",
       "\n",
       "     fr_nitro_arom_nonortho  fr_nitroso  fr_oxazole  fr_oxime  \\\n",
       "0                         0           0           0         0   \n",
       "1                         0           0           0         0   \n",
       "2                         0           0           0         0   \n",
       "3                         0           0           0         0   \n",
       "4                         0           0           0         0   \n",
       "..                      ...         ...         ...       ...   \n",
       "993                       0           0           0         0   \n",
       "994                       0           0           0         0   \n",
       "995                       0           0           0         0   \n",
       "996                       0           0           0         0   \n",
       "997                       0           0           0         0   \n",
       "\n",
       "     fr_para_hydroxylation  fr_phenol  fr_phenol_noOrthoHbond  fr_phos_acid  \\\n",
       "0                        0          0                       0             0   \n",
       "1                        0          0                       0             0   \n",
       "2                        0          0                       0             0   \n",
       "3                        0          0                       0             0   \n",
       "4                        0          0                       0             0   \n",
       "..                     ...        ...                     ...           ...   \n",
       "993                      0          0                       0             0   \n",
       "994                      0          0                       0             0   \n",
       "995                      0          0                       0             0   \n",
       "996                      0          0                       0             0   \n",
       "997                      0          0                       0             0   \n",
       "\n",
       "     fr_phos_ester  fr_piperdine  fr_piperzine  fr_priamide  fr_prisulfonamd  \\\n",
       "0                0             0             0            0                0   \n",
       "1                0             0             0            0                0   \n",
       "2                0             0             0            0                0   \n",
       "3                0             0             0            0                0   \n",
       "4                0             0             0            0                0   \n",
       "..             ...           ...           ...          ...              ...   \n",
       "993              0             0             0            0                0   \n",
       "994              0             0             0            0                0   \n",
       "995              0             0             0            0                0   \n",
       "996              0             0             0            0                0   \n",
       "997              0             0             0            0                0   \n",
       "\n",
       "     fr_pyridine  fr_quatN  fr_sulfide  fr_sulfonamd  fr_sulfone  \\\n",
       "0              0         0           0             0           0   \n",
       "1              0         0           0             0           0   \n",
       "2              0         2           0             0           0   \n",
       "3              0         0           0             0           0   \n",
       "4              0         0           0             0           0   \n",
       "..           ...       ...         ...           ...         ...   \n",
       "993            0         0           0             0           0   \n",
       "994            0         0           0             0           0   \n",
       "995            0         0           1             0           0   \n",
       "996            0         0           0             0           0   \n",
       "997            0         0           0             0           0   \n",
       "\n",
       "     fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  \\\n",
       "0                    0             0            0            0             0   \n",
       "1                    0             0            0            0             0   \n",
       "2                    0             0            0            0             0   \n",
       "3                    0             0            0            0             0   \n",
       "4                    0             0            0            0             0   \n",
       "..                 ...           ...          ...          ...           ...   \n",
       "993                  0             0            0            0             0   \n",
       "994                  0             0            0            0             0   \n",
       "995                  0             0            0            0             0   \n",
       "996                  0             0            0            0             0   \n",
       "997                  0             0            0            0             0   \n",
       "\n",
       "     fr_unbrch_alkane  fr_urea  \n",
       "0                   3        0  \n",
       "1                   3        0  \n",
       "2                   3        0  \n",
       "3                   4        0  \n",
       "4                   0        0  \n",
       "..                ...      ...  \n",
       "993                 0        0  \n",
       "994                 0        0  \n",
       "995                 0        0  \n",
       "996                 0        0  \n",
       "997                 0        0  \n",
       "\n",
       "[998 rows x 214 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a946941faca937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:13.720652Z",
     "start_time": "2025-05-28T16:57:13.716638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем немнформативный признак\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22040ef150ffc8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.281372Z",
     "start_time": "2025-05-28T16:57:14.276214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем пропуски\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0bb2972cbaa906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.915613Z",
     "start_time": "2025-05-28T16:57:14.885015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем дубликаты\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7089b7fa5f494e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:16.413719Z",
     "start_time": "2025-05-28T16:57:16.409041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(966, 213)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4529ddf3083cdcc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:00:51.840374Z",
     "start_time": "2025-05-28T17:00:51.836087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вычисляем медиану столбца\n",
    "median_value_ic50 = df['IC50, mM'].median()\n",
    "median_value_cc50 = df['CC50, mM'].median()\n",
    "median_value_si = df['SI'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "400a82221f828fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:21.052804Z",
     "start_time": "2025-05-28T17:02:21.047421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем целевые признаки\n",
    "df['IC50, mM'] = (df['IC50, mM'] > median_value_ic50).astype(int)\n",
    "df['CC50, mM'] = (df['CC50, mM'] > median_value_ic50).astype(int)\n",
    "df['SI'] = (df['SI'] > median_value_ic50).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "502ea21feffa4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:05:44.334975Z",
     "start_time": "2025-05-28T17:05:44.331153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем данные и целевую переменную\n",
    "X = df.drop(columns = ['IC50, mM', 'CC50, mM', 'SI'])\n",
    "y = df['IC50, mM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae82771-5320-4ecd-a016-d937810c06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество выбросов по признакам:\n",
      "IC50, mM              0\n",
      "CC50, mM              0\n",
      "SI                    0\n",
      "MaxAbsEStateIndex    60\n",
      "MaxEStateIndex       60\n",
      "                     ..\n",
      "fr_thiazole          52\n",
      "fr_thiocyan           0\n",
      "fr_thiophene         68\n",
      "fr_unbrch_alkane     49\n",
      "fr_urea               7\n",
      "Length: 213, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers(df, alpha=0.05, method='iqr', normality_test='shapiro', add_sum_column=False):\n",
    "    \"\"\"\n",
    "    Обнаружение выбросов в DataFrame с использованием различных статистических методов.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Входной DataFrame с данными для анализа\n",
    "    alpha : float, по умолчанию 0.05\n",
    "        Уровень значимости для тестов на нормальность\n",
    "    method : str, по умолчанию 'iqr'\n",
    "        Метод обнаружения выбросов для ненормальных данных:\n",
    "        - 'iqr' - метод межквартильного размаха\n",
    "        - 'zscore' - модифицированный Z-score\n",
    "    normality_test : str, по умолчанию 'shapiro'\n",
    "        Тест на нормальность распределения:\n",
    "        - 'shapiro' - тест Шапиро-Уилка\n",
    "        - 'normaltest' - тест на нормальность D'Agostino-Pearson\n",
    "        - 'anderson' - тест Андерсона-Дарлинга\n",
    "    add_sum_column : bool, по умолчанию False\n",
    "        Если True, добавляет столбец с общим количеством выбросов для каждой строки\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        DataFrame с булевыми значениями, где True указывает на выброс\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем DataFrame для хранения результатов (по умолчанию все значения False)\n",
    "    outliers = pd.DataFrame(False, index=df.index, columns=df.columns)\n",
    "    \n",
    "    # Анализируем каждый столбец отдельно\n",
    "    for col in df.columns:\n",
    "        # Удаляем пропущенные значения для текущего столбца\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Если в столбце меньше 3 значений, пропускаем его\n",
    "        if len(data) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Проверяем нормальность распределения\n",
    "        normal = False  # Флаг нормальности распределения\n",
    "        \n",
    "        try:\n",
    "            # Выбираем тест на нормальность в зависимости от параметра normality_test\n",
    "            if normality_test == 'shapiro':\n",
    "                # Тест Шапиро-Уилка (подходит для небольших выборок < 5000)\n",
    "                _, p = stats.shapiro(data)\n",
    "                normal = p > alpha  # Если p-value > alpha, распределение считается нормальным\n",
    "                \n",
    "            elif normality_test == 'normaltest':\n",
    "                # Тест D'Agostino-Pearson (работает для выборок > 20)\n",
    "                _, p = stats.normaltest(data)\n",
    "                normal = p > alpha\n",
    "                \n",
    "            elif normality_test == 'anderson':\n",
    "                # Тест Андерсона-Дарлинга (более строгий)\n",
    "                result = stats.anderson(data)\n",
    "                # Сравниваем статистику с критическим значением для выбранного alpha\n",
    "                normal = result.statistic < result.critical_values[np.where(result.significance_level == int(alpha*100))[0][0]]\n",
    "        except:\n",
    "            # В случае ошибки в тесте считаем распределение ненормальным\n",
    "            pass\n",
    "        \n",
    "        # Если распределение нормальное, используем стандартный Z-score\n",
    "        if normal:\n",
    "            z = np.abs(stats.zscore(data))  # Вычисляем Z-оценки\n",
    "            outliers.loc[data.index, col] = z > 3  # Выбросы > 3 стандартных отклонений\n",
    "            \n",
    "        # Для ненормальных распределений используем выбранный метод\n",
    "        else:\n",
    "            if method == 'iqr':\n",
    "                # Метод межквартильного размаха (IQR)\n",
    "                q1 = data.quantile(0.25)  # Первый квартиль (25-й перцентиль)\n",
    "                q3 = data.quantile(0.75)  # Третий квартиль (75-й перцентиль)\n",
    "                iqr = q3 - q1  # Межквартильный размах\n",
    "                \n",
    "                # Границы для выбросов\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                # Отмечаем выбросы\n",
    "                outliers.loc[data.index, col] = (data < lower_bound) | (data > upper_bound)\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                # Модифицированный Z-score (более устойчивый к выбросам)\n",
    "                median = data.median()  # Медиана вместо среднего\n",
    "                mad = stats.median_abs_deviation(data, scale='normal')  # Медианное абсолютное отклонение\n",
    "                modified_z = np.abs(0.6745 * (data - median) / mad)  # Модифицированный Z-score\n",
    "                \n",
    "                # Выбросы при modified_z > 3.5\n",
    "                outliers.loc[data.index, col] = modified_z > 3.5\n",
    "    \n",
    "    # Добавляем столбец с суммой выбросов по строкам, если нужно\n",
    "    if add_sum_column:\n",
    "        outliers['outliers_sum'] = outliers.sum(axis=1)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Находим выбросы\n",
    "outliers = detect_outliers(df)\n",
    "\n",
    "# Выводим количество выбросов по каждому признаку\n",
    "print(\"\\nКоличество выбросов по признакам:\")\n",
    "print(outliers.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a0ea30522688a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:06:25.366761Z",
     "start_time": "2025-05-28T17:06:15.792777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Обучение Logistic Regression...\n",
      " Logistic Regression\n",
      "Accuracy: 0.7268 | Precision: 0.7270 | Recall: 0.7268 | F1: 0.7267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72        97\n",
      "           1       0.72      0.74      0.73        97\n",
      "\n",
      "    accuracy                           0.73       194\n",
      "   macro avg       0.73      0.73      0.73       194\n",
      "weighted avg       0.73      0.73      0.73       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение SVM...\n",
      " SVM\n",
      "Accuracy: 0.7268 | Precision: 0.7268 | Recall: 0.7268 | F1: 0.7268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73        97\n",
      "           1       0.72      0.73      0.73        97\n",
      "\n",
      "    accuracy                           0.73       194\n",
      "   macro avg       0.73      0.73      0.73       194\n",
      "weighted avg       0.73      0.73      0.73       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение KNN...\n",
      " KNN\n",
      "Accuracy: 0.7526 | Precision: 0.7526 | Recall: 0.7526 | F1: 0.7526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        97\n",
      "           1       0.75      0.75      0.75        97\n",
      "\n",
      "    accuracy                           0.75       194\n",
      "   macro avg       0.75      0.75      0.75       194\n",
      "weighted avg       0.75      0.75      0.75       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение Random Forest...\n",
      " Random Forest\n",
      "Accuracy: 0.7320 | Precision: 0.7321 | Recall: 0.7320 | F1: 0.7319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73        97\n",
      "           1       0.74      0.72      0.73        97\n",
      "\n",
      "    accuracy                           0.73       194\n",
      "   macro avg       0.73      0.73      0.73       194\n",
      "weighted avg       0.73      0.73      0.73       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение XGBoost...\n",
      " XGBoost\n",
      "Accuracy: 0.7113 | Precision: 0.7117 | Recall: 0.7113 | F1: 0.7112\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72        97\n",
      "           1       0.72      0.69      0.71        97\n",
      "\n",
      "    accuracy                           0.71       194\n",
      "   macro avg       0.71      0.71      0.71       194\n",
      "weighted avg       0.71      0.71      0.71       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение CatBoost...\n",
      " CatBoost\n",
      "Accuracy: 0.7216 | Precision: 0.7220 | Recall: 0.7216 | F1: 0.7215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73        97\n",
      "           1       0.73      0.70      0.72        97\n",
      "\n",
      "    accuracy                           0.72       194\n",
      "   macro avg       0.72      0.72      0.72       194\n",
      "weighted avg       0.72      0.72      0.72       194\n",
      "\n",
      "──────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def test_classification_models_with_scaling(X, y, test_size=0.2, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Тестирует классификационные модели со стандартизацией данных и возвращает результаты.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame или np.array\n",
    "        Матрица признаков\n",
    "    y : pd.Series или np.array\n",
    "        Целевая переменная\n",
    "    test_size : float, optional\n",
    "        Размер тестовой выборки (по умолчанию 0.2)\n",
    "    random_state : int, optional\n",
    "        Seed для воспроизводимости (по умолчанию 42)\n",
    "    verbose : bool, optional\n",
    "        Выводить ли прогресс (по умолчанию True)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Таблица с результатами метрик и временем обучения для каждой модели\n",
    "    \"\"\"\n",
    "\n",
    "    # Разделение данных\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Инициализация моделей с пайплайнами (StandardScaler + модель)\n",
    "    models = {\n",
    "        'Logistic Regression': make_pipeline(StandardScaler(),\n",
    "                                           LogisticRegression(random_state=random_state, max_iter=1000)),\n",
    "        'SVM': make_pipeline(StandardScaler(),\n",
    "                            SVC(random_state=random_state)),\n",
    "        'KNN': make_pipeline(StandardScaler(),\n",
    "                            KNeighborsClassifier()),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(random_state=random_state, n_jobs=-1, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(random_state=random_state, verbose=False)\n",
    "    }\n",
    "\n",
    "    # Словарь для хранения результатов\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Train Time (s)': [],\n",
    "        'Scaler Used': []\n",
    "    }\n",
    "\n",
    "    # Обучение и оценка моделей\n",
    "    for name, model in models.items():\n",
    "        if verbose:\n",
    "            print(f\"🔍 Обучение {name}...\")\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Обучение модели\n",
    "            model.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            # Предсказание\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Расчет метрик\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            # Определение использования StandardScaler\n",
    "            scaler_used = 'StandardScaler' if 'standardscaler' in str(model).lower() else 'No'\n",
    "\n",
    "            # Сохранение результатов\n",
    "            results['Model'].append(name)\n",
    "            results['Accuracy'].append(accuracy)\n",
    "            results['Precision'].append(precision)\n",
    "            results['Recall'].append(recall)\n",
    "            results['F1 Score'].append(f1)\n",
    "            results['Train Time (s)'].append(train_time)\n",
    "            results['Scaler Used'].append(scaler_used)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\" {name}\\nAccuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                print(\"─\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Ошибка в {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создание DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results).sort_values('F1 Score', ascending=False)\n",
    "\n",
    "    # Визуализация результатов\n",
    "    if verbose:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "        x = np.arange(len(results_df['Model']))\n",
    "        width = 0.2\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "        plt.title('Сравнение моделей классификации', pad=20)\n",
    "        plt.xlabel('Модели')\n",
    "        plt.ylabel('Оценка')\n",
    "        plt.xticks(x + width*1.5, results_df['Model'], rotation=45, ha='right')\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Запуск расчета\n",
    "results = test_classification_models_with_scaling(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294a5ea4c764740",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-28T17:09:08.860255Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Initial F1: 0.7267, Accuracy: 0.7268 with 210 features\n",
      "Best F1: 0.7679, Accuracy: 0.7680 with 204 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== SVM ===\n",
      "Initial F1: 0.7268, Accuracy: 0.7268 with 210 features\n",
      "Best F1: 0.7731, Accuracy: 0.7732 with 203 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_azide', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_urea']\n",
      "\n",
      "=== KNN ===\n",
      "Initial F1: 0.7526, Accuracy: 0.7526 with 210 features\n",
      "Best F1: 0.8247, Accuracy: 0.8247 with 198 features\n",
      "Optimal features: ['MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_azide', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== RandomForest ===\n",
      "Initial F1: 0.7319, Accuracy: 0.7320 with 210 features\n",
      "Best F1: 0.7473, Accuracy: 0.7474 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== XGBoost ===\n",
      "Initial F1: 0.7112, Accuracy: 0.7113 with 210 features\n",
      "Best F1: 0.7525, Accuracy: 0.7526 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== CatBoost ===\n",
      "Initial F1: 0.7215, Accuracy: 0.7216 with 210 features\n",
      "Best F1: 0.7422, Accuracy: 0.7423 with 209 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации\n",
    "def evaluate_model(X, y, model, test_size=0.2, random_state=42):\n",
    "    \"\"\"Функция оценки модели классификации с добавлением StandardScaler\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Для моделей, чувствительных к масштабу, используем Pipeline со StandardScaler\n",
    "    if isinstance(model, (LogisticRegression, SVC, KNeighborsClassifier)):\n",
    "        model = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "def find_best_feature_subset(X, y, model, model_name):\n",
    "    \"\"\"Модифицированная версия для классификации с указанием имени модели\"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    current_features = X.columns.tolist()\n",
    "    initial_scores = evaluate_model(X[current_features], y, model)\n",
    "    best_f1 = initial_scores['f1']\n",
    "    best_accuracy = initial_scores['accuracy']\n",
    "    best_features = current_features.copy()\n",
    "    history = []\n",
    "\n",
    "    history.append({\n",
    "        'features': current_features.copy(),\n",
    "        'f1': best_f1,\n",
    "        'accuracy': best_accuracy,\n",
    "        'action': 'initial'\n",
    "    })\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Initial F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(current_features)} features\")\n",
    "\n",
    "    improved = True\n",
    "    while improved and len(current_features) > 1:\n",
    "        improved = False\n",
    "        worst_feature = None\n",
    "\n",
    "        for feature in current_features:\n",
    "            trial_features = [f for f in current_features if f != feature]\n",
    "            current_scores = evaluate_model(X[trial_features], y, model)\n",
    "\n",
    "            history.append({\n",
    "                'features': trial_features.copy(),\n",
    "                'f1': current_scores['f1'],\n",
    "                'accuracy': current_scores['accuracy'],\n",
    "                'action': f'removed {feature}'\n",
    "            })\n",
    "\n",
    "            if current_scores['f1'] > best_f1:\n",
    "                best_f1 = current_scores['f1']\n",
    "                best_accuracy = current_scores['accuracy']\n",
    "                best_features = trial_features.copy()\n",
    "                worst_feature = feature\n",
    "                improved = True\n",
    "\n",
    "        if improved:\n",
    "            current_features.remove(worst_feature)\n",
    "\n",
    "    print(f\"Best F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(best_features)} features\")\n",
    "    print(\"Optimal features:\", best_features)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_f1,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей по очереди\"\"\"\n",
    "    # Создаем модели с дефолтными параметрами\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = find_best_feature_subset(X, y, model, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создаем DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "53e9f1a111a073db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: {'accuracy': 0.7628865979381443, 'f1': 0.7627857522594366, 'roc_auc': 0.8139547242002337}\n",
      "Selected features: 131 (Removed 79 outliers)\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: {'accuracy': 0.7422680412371134, 'f1': 0.7422406462585034, 'roc_auc': 0.7990753533850569}\n",
      "Selected features: 198 (Removed 12 outliers)\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: {'accuracy': 0.788659793814433, 'f1': 0.788654178291484, 'roc_auc': 0.8278244234243809}\n",
      "Selected features: 133 (Removed 77 outliers)\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: {'accuracy': 0.7525773195876289, 'f1': 0.7525510204081632, 'roc_auc': 0.8218726750983102}\n",
      "Selected features: 56 (Removed 154 outliers)\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: {'accuracy': 0.7474226804123711, 'f1': 0.7474159691776273, 'roc_auc': 0.8008821341268998}\n",
      "Selected features: 47 (Removed 163 outliers)\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: {'accuracy': 0.7680412371134021, 'f1': 0.767985754910038, 'roc_auc': 0.8271335954936763}\n",
      "Selected features: 63 (Removed 147 outliers)\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации, через последовательное удаление признаков, упорядоченных по количеству возможных выбросов\n",
    "\n",
    "outliers_count = pd.DataFrame({\n",
    "        'feature': df.columns,\n",
    "        'outliers': outliers.sum(axis=0)\n",
    "    }).sort_values('outliers', ascending=False)\n",
    "all_features_outliers = outliers_count['feature'][outliers_count['outliers']>0].tolist()\n",
    "\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = get_best_features(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_best_features(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с удалением выбросов для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    removed = []\n",
    "    # Перебираем количество удаляемых признаков с выбросами\n",
    "    for n in range(1, len(all_features_outliers)+1):\n",
    "        \n",
    "        current_features = X.drop(columns = all_features_outliers[:n]).columns.tolist()\n",
    "\n",
    "        metrics = evaluate_metrics(X[current_features], y, model_name)\n",
    "\n",
    "        if metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = metrics\n",
    "            best_features = current_features.copy()\n",
    "            removed = all_features_outliers[:n]\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: {best_metrics}\")\n",
    "    print(f\"Selected features: {len(best_features)} (Removed {len(removed)} outliers)\")\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'outliers'\n",
    "    }\n",
    "   \n",
    "\n",
    "def evaluate_metrics(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        metrics['roc_auc'] = None\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_2 = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8277aa7f99158c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7423, F1=0.7422, ROC-AUC=0.7974279944733765\n",
      "Optimal features (17): ['VSA_EState8', 'BCUT2D_MRLOW', 'NumHDonors', 'VSA_EState4', 'MolLogP']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7268, F1=0.7268, ROC-AUC=0.7730364544584971\n",
      "Optimal features (18): ['VSA_EState8', 'BCUT2D_MRLOW', 'NumHDonors', 'EState_VSA4', 'FpDensityMorgan3']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7165, F1=0.7164, ROC-AUC=0.7684663619938357\n",
      "Optimal features (10): ['BCUT2D_MRLOW', 'PEOE_VSA6', 'EState_VSA4', 'FpDensityMorgan3', 'NumSaturatedCarbocycles']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью SHAP\n",
    "\n",
    "def test_tree_models_sh(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с SHAP-анализом\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_shap_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_shap_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с помощью SHAP для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostRegressor(\n",
    "        iterations=100,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "   \n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': importance['feature'],\n",
    "        'importance': importance['mean_abs_shap']\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'][feat_importance['importance']>0].tolist()\n",
    "    \n",
    "    \n",
    "    # Находим оптимальное количество признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "\n",
    "    for n in range(1, min(20, len(all_features)+1)):  # Ограничиваем до 20 признаков для скорости\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'shap'\n",
    "    }\n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_significant_shap_features(shap_values, feature_names, threshold=0):\n",
    "    \"\"\"\n",
    "    Возвращает отсортированный по убыванию список признаков с SHAP-значимостью\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    shap_values : shap.Explanation или np.ndarray\n",
    "        SHAP значения для всех наблюдений\n",
    "    feature_names : list или pd.Index\n",
    "        Список названий признаков\n",
    "    threshold : float, optional\n",
    "        Порог значимости (по умолчанию 0)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame: DataFrame с колонками 'feature' и 'mean_abs_shap',\n",
    "                  отсортированный по убыванию важности\n",
    "    \"\"\"\n",
    "    if isinstance(shap_values, shap.Explanation):\n",
    "        shap_array = shap_values.values\n",
    "    else:\n",
    "        shap_array = shap_values\n",
    "\n",
    "    # Для многоклассовой классификации берем среднее по всем классам\n",
    "    if len(shap_array.shape) == 3:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=(0, 1))\n",
    "    else:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=0)\n",
    "\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': mean_abs_shap\n",
    "    })\n",
    "\n",
    "    significant_features = shap_importance[shap_importance['mean_abs_shap'] > threshold] \\\n",
    "        .sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    return significant_features.reset_index(drop=True)\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_3 = test_tree_models_sh(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13843390031221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7474, F1=0.7474, ROC-AUC=0.7960994792220216\n",
      "Optimal features (17): ['VSA_EState8', 'BCUT2D_MRLOW', 'MolLogP', 'SlogP_VSA5', 'BCUT2D_LOGPHI']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7165, F1=0.7165, ROC-AUC=0.7770751408226166\n",
      "Optimal features (17): ['NumHDonors', 'VSA_EState8', 'fr_ketone_Topliss', 'NumHeteroatoms', 'fr_ketone']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7320, F1=0.7319, ROC-AUC=0.8182059730045701\n",
      "Optimal features (20): ['BCUT2D_MRLOW', 'VSA_EState8', 'NHOHCount', 'NumSaturatedHeterocycles', 'VSA_EState1']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью features importance\n",
    "\n",
    "def test_tree_models_fs(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с отбором признаков\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_feature_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_feature_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков для классификации на основе важности признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Инициализация и обучение модели\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostClassifier(iterations=100, random_seed=42, verbose=False)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Получение важности признаков\n",
    "    if name == 'XGBoost':\n",
    "        importance = model.feature_importances_\n",
    "    else:\n",
    "        importance = model.feature_importances_\n",
    "    \n",
    "    # Сортировка признаков по важности\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'].tolist()\n",
    "    \n",
    "    # Поиск оптимального набора признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Ограничиваем количество проверяемых комбинаций для скорости\n",
    "    max_features_to_test = min(20, len(all_features))\n",
    "    \n",
    "    for n in range(1, max_features_to_test + 1):\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'feature_importance'\n",
    "    }\n",
    "    \n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_4  = test_tree_models_fs(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5860f10-8bb9-4002-8bf4-4c0889b7f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем все лучшие результаты\n",
    "results_df = pd.concat([results_col_combination,results_col_combination_2, results_col_combination_3, results_col_combination_4], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943b2ea9-ff60-444e-be72-a4bd2ff042ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.767887</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>204</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.773099</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>203</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.824724</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>[MaxEStateIndex, MinAbsEStateIndex, MinEStateI...</td>\n",
       "      <td>198</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.747255</td>\n",
       "      <td>0.747423</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.752472</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.742241</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>209</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.731930</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>196</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.742241</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>198</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.778351</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>195</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.737106</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>201</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.731702</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>207</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.737051</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>192</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.742241</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[VSA_EState8, BCUT2D_MRLOW, NumHDonors, VSA_ES...</td>\n",
       "      <td>17</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.726797</td>\n",
       "      <td>0.726804</td>\n",
       "      <td>[VSA_EState8, BCUT2D_MRLOW, NumHDonors, EState...</td>\n",
       "      <td>18</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.716427</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[BCUT2D_MRLOW, PEOE_VSA6, EState_VSA4, FpDensi...</td>\n",
       "      <td>10</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.747416</td>\n",
       "      <td>0.747423</td>\n",
       "      <td>[VSA_EState8, BCUT2D_MRLOW, MolLogP, SlogP_VSA...</td>\n",
       "      <td>17</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.716487</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[NumHDonors, VSA_EState8, fr_ketone_Topliss, N...</td>\n",
       "      <td>17</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.731930</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[BCUT2D_MRLOW, VSA_EState8, NHOHCount, NumSatu...</td>\n",
       "      <td>20</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model   best_f1  best_accuracy  \\\n",
       "0   LogisticRegression  0.767887       0.768041   \n",
       "1                  SVM  0.773099       0.773196   \n",
       "2                  KNN  0.824724       0.824742   \n",
       "3         RandomForest  0.747255       0.747423   \n",
       "4              XGBoost  0.752472       0.752577   \n",
       "5             CatBoost  0.742241       0.742268   \n",
       "6   LogisticRegression  0.731930       0.731959   \n",
       "7                  SVM  0.742241       0.742268   \n",
       "8                  KNN  0.778203       0.778351   \n",
       "9         RandomForest  0.737106       0.737113   \n",
       "10             XGBoost  0.731702       0.731959   \n",
       "11            CatBoost  0.737051       0.737113   \n",
       "12        RandomForest  0.742241       0.742268   \n",
       "13             XGBoost  0.726797       0.726804   \n",
       "14            CatBoost  0.716427       0.716495   \n",
       "15        RandomForest  0.747416       0.747423   \n",
       "16             XGBoost  0.716487       0.716495   \n",
       "17            CatBoost  0.731930       0.731959   \n",
       "\n",
       "                                        best_features  num_features  \\\n",
       "0   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           204   \n",
       "1   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           203   \n",
       "2   [MaxEStateIndex, MinAbsEStateIndex, MinEStateI...           198   \n",
       "3   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "4   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "5   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           209   \n",
       "6   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           196   \n",
       "7   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           198   \n",
       "8   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           195   \n",
       "9   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           201   \n",
       "10  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           207   \n",
       "11  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           192   \n",
       "12  [VSA_EState8, BCUT2D_MRLOW, NumHDonors, VSA_ES...            17   \n",
       "13  [VSA_EState8, BCUT2D_MRLOW, NumHDonors, EState...            18   \n",
       "14  [BCUT2D_MRLOW, PEOE_VSA6, EState_VSA4, FpDensi...            10   \n",
       "15  [VSA_EState8, BCUT2D_MRLOW, MolLogP, SlogP_VSA...            17   \n",
       "16  [NumHDonors, VSA_EState8, fr_ketone_Topliss, N...            17   \n",
       "17  [BCUT2D_MRLOW, VSA_EState8, NHOHCount, NumSatu...            20   \n",
       "\n",
       "              selector  \n",
       "0    without selection  \n",
       "1    without selection  \n",
       "2    without selection  \n",
       "3    without selection  \n",
       "4    without selection  \n",
       "5    without selection  \n",
       "6             outliers  \n",
       "7             outliers  \n",
       "8             outliers  \n",
       "9             outliers  \n",
       "10            outliers  \n",
       "11            outliers  \n",
       "12                shap  \n",
       "13                shap  \n",
       "14                shap  \n",
       "15  feature_importance  \n",
       "16  feature_importance  \n",
       "17  feature_importance  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e05de657-40c5-4224-8a4d-a4d6ee638132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.7320, F1=0.7319, ROC-AUC=0.8023700712084173\n",
      "Optimal features (198): ['SlogP_VSA7', 'VSA_EState7', 'fr_ketone', 'NumRotatableBonds', 'fr_alkyl_halide']...\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7732, F1=0.7731, ROC-AUC=0.8065150387926453\n",
      "Optimal features (203): ['SlogP_VSA7', 'VSA_EState7', 'fr_ketone', 'NumRotatableBonds', 'fr_alkyl_halide']...\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.8247, F1=0.8247, ROC-AUC=0.847433308534382\n",
      "Optimal features (198): ['VSA_EState7', 'fr_ketone', 'NumRotatableBonds', 'fr_alkyl_halide', 'VSA_EState8']...\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7320, F1=0.7319, ROC-AUC=0.7928579020087151\n",
      "Optimal features (204): ['SlogP_VSA7', 'VSA_EState7', 'fr_ketone', 'NumRotatableBonds', 'fr_alkyl_halide']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7320, F1=0.7319, ROC-AUC=0.7904665745562759\n",
      "Optimal features (20): ['BCUT2D_MRLOW', 'HallKierAlpha', 'SlogP_VSA5', 'SMR_VSA1', 'VSA_EState8']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7577, F1=0.7577, ROC-AUC=0.8117228185779573\n",
      "Optimal features (17): ['BCUT2D_MRLOW', 'Chi0n', 'NumHeteroatoms', 'SlogP_VSA11', 'NumValenceElectrons']...\n"
     ]
    }
   ],
   "source": [
    "# Определяем налучший результат работы моделей\n",
    "def test_all_classifiers(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = evaluate_classifier(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_classifier(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка классификатора с оптимальным набором признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    # Находим оптимальные метрики\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Получаем все наборы признаков из предыдущих результатов\n",
    "    feature_sets = [set(features) for features in results_df['best_features']]\n",
    "    \n",
    "    # Добавляем все признаки для сравнения\n",
    "    feature_sets.append(set(X.columns))\n",
    "    \n",
    "    # Удаляем дубликаты\n",
    "    unique_feature_sets = []\n",
    "    seen = set()\n",
    "    for fs in feature_sets:\n",
    "        frozen = frozenset(fs)\n",
    "        if frozen not in seen:\n",
    "            seen.add(frozen)\n",
    "            unique_feature_sets.append(list(fs))\n",
    "    \n",
    "    for current_features in unique_feature_sets:\n",
    "        current_metrics = get_classification_metrics(X[current_features], y, model_name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features)\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "def get_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_features_selection  = test_all_classifiers(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e76424a1-6c08-4999-b289-7a6dd18ad9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.731930</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[SlogP_VSA7, VSA_EState7, fr_ketone, NumRotata...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.773099</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>[SlogP_VSA7, VSA_EState7, fr_ketone, NumRotata...</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.824724</td>\n",
       "      <td>0.824742</td>\n",
       "      <td>[VSA_EState7, fr_ketone, NumRotatableBonds, fr...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.731930</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[SlogP_VSA7, VSA_EState7, fr_ketone, NumRotata...</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.731930</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[BCUT2D_MRLOW, HallKierAlpha, SlogP_VSA5, SMR_...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.757726</td>\n",
       "      <td>0.757732</td>\n",
       "      <td>[BCUT2D_MRLOW, Chi0n, NumHeteroatoms, SlogP_VS...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.731930       0.731959   \n",
       "1                 SVM  0.773099       0.773196   \n",
       "2                 KNN  0.824724       0.824742   \n",
       "3        RandomForest  0.731930       0.731959   \n",
       "4             XGBoost  0.731930       0.731959   \n",
       "5            CatBoost  0.757726       0.757732   \n",
       "\n",
       "                                       best_features  num_features  \n",
       "0  [SlogP_VSA7, VSA_EState7, fr_ketone, NumRotata...           198  \n",
       "1  [SlogP_VSA7, VSA_EState7, fr_ketone, NumRotata...           203  \n",
       "2  [VSA_EState7, fr_ketone, NumRotatableBonds, fr...           198  \n",
       "3  [SlogP_VSA7, VSA_EState7, fr_ketone, NumRotata...           204  \n",
       "4  [BCUT2D_MRLOW, HallKierAlpha, SlogP_VSA5, SMR_...            20  \n",
       "5  [BCUT2D_MRLOW, Chi0n, NumHeteroatoms, SlogP_VS...            17  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_features_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58a92eb3-7eb3-464f-b484-e89cf9b9702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing LogisticRegression: 100%|██████████| 772/772 [00:18<00:00, 41.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.7732, F1=0.7731, ROC-AUC=0.8012009777872251\n",
      "Удалено строк: 9 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing SVM: 100%|██████████| 772/772 [02:15<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7835, F1=0.7835, ROC-AUC=0.8075778509937294\n",
      "Удалено строк: 3 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing KNN: 100%|██████████| 772/772 [00:19<00:00, 40.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.8557, F1=0.8556, ROC-AUC=0.8585396960357106\n",
      "Удалено строк: 7 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest: 100%|██████████| 772/772 [03:04<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7629, F1=0.7629, ROC-AUC=0.815070677011372\n",
      "Удалено строк: 10 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost: 100%|██████████| 772/772 [00:49<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7732, F1=0.7732, ROC-AUC=0.8021575087682007\n",
      "Удалено строк: 7 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost: 100%|██████████| 772/772 [15:18<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7887, F1=0.7887, ROC-AUC=0.816080348602402\n",
      "Удалено строк: 7 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Определяем лучшую комбинацию строк для повышения качества работы моделей на всех данных\n",
    "\n",
    "def test_all_classifiers_optimized(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов с оптимизацией набора данных\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = optimize_classifier_data(X, y, name, model)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def optimize_classifier_data(X, y, name, model, verbose=True):\n",
    "    \"\"\"\n",
    "    Оптимизирует набор данных для классификатора путем последовательного удаления строк\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Признаки\n",
    "    y : pd.Series\n",
    "        Целевая переменная\n",
    "    name : str\n",
    "        Имя модели\n",
    "    model : sklearn classifier\n",
    "        Модель классификатора\n",
    "    verbose : bool\n",
    "        Выводить ли прогресс\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации\n",
    "    \"\"\"\n",
    "    # Используем оптимальные признаки для модели\n",
    "    models = {\n",
    "        'LogisticRegression': make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    if name == 'LogisticRegression':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][0]]\n",
    "        \n",
    "    elif name == 'SVM':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][1]]\n",
    "        \n",
    "    elif name == 'KNN':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][2]]\n",
    "    elif name == 'RandomForest':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][3]]\n",
    "    elif name == 'XGBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][4]]\n",
    "    elif name == 'CatBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][5]]\n",
    "    \n",
    "    \n",
    "    # Разделение данных со стратификацией\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Инициализация\n",
    "    X_opt = X_train.copy()\n",
    "    y_opt = y_train.copy()\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    removed_indices = []\n",
    "    \n",
    "    # Прогресс-бар для наглядности\n",
    "    iterator = tqdm(X_train.index, desc=f\"Optimizing {name}\") if verbose else X_train.index\n",
    "    \n",
    "    for idx in iterator:\n",
    "        # Временно удаляем строку\n",
    "        X_temp = X_opt.drop(index=idx)\n",
    "        y_temp = y_opt.drop(index=idx)\n",
    "        \n",
    "        # Обучаем и оцениваем\n",
    "        model = models[name]\n",
    "        model.fit(X_temp, y_temp)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Рассчитываем метрики\n",
    "        current_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y)) == 2:\n",
    "            current_metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Решение о сохранении/удалении строки\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            X_opt = X_temp\n",
    "            y_opt = y_temp\n",
    "            removed_indices.append(idx)\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Удалено строк: {len(removed_indices)} из {len(X_train)}\")\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'removed_indices': removed_indices,\n",
    "        'num_removed': len(removed_indices),\n",
    "        'indices': X_opt.index.tolist(),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "    \n",
    "# Запуск расчета\n",
    "results_indices_selection = test_all_classifiers_optimized(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d9c0bc-20b0-429e-93ef-d9e9d1fe0044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>removed_indices</th>\n",
       "      <th>num_removed</th>\n",
       "      <th>indices</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.762861</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>[322, 657, 617, 456, 176, 543]</td>\n",
       "      <td>6</td>\n",
       "      <td>[338, 31, 656, 216, 93, 579, 519, 713, 300, 83...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.783482</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>[322, 288, 662]</td>\n",
       "      <td>3</td>\n",
       "      <td>[338, 31, 656, 216, 93, 579, 519, 713, 657, 30...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.855609</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>[322, 31, 758, 997, 296, 153, 711]</td>\n",
       "      <td>7</td>\n",
       "      <td>[338, 656, 216, 93, 579, 519, 713, 657, 300, 8...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.762861</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>[322, 483, 20, 974, 641, 254, 376, 918, 512, 358]</td>\n",
       "      <td>10</td>\n",
       "      <td>[338, 31, 656, 216, 93, 579, 519, 713, 657, 30...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.773172</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>[322, 93, 955, 692, 88, 595, 131]</td>\n",
       "      <td>7</td>\n",
       "      <td>[338, 31, 656, 216, 579, 519, 713, 657, 300, 8...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.788654</td>\n",
       "      <td>0.788660</td>\n",
       "      <td>[322, 836, 56, 224, 891, 3, 307]</td>\n",
       "      <td>7</td>\n",
       "      <td>[338, 31, 656, 216, 93, 579, 519, 713, 657, 30...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.762861       0.762887   \n",
       "1                 SVM  0.783482       0.783505   \n",
       "2                 KNN  0.855609       0.855670   \n",
       "3        RandomForest  0.762861       0.762887   \n",
       "4             XGBoost  0.773172       0.773196   \n",
       "5            CatBoost  0.788654       0.788660   \n",
       "\n",
       "                                     removed_indices  num_removed  \\\n",
       "0                     [322, 657, 617, 456, 176, 543]            6   \n",
       "1                                    [322, 288, 662]            3   \n",
       "2                 [322, 31, 758, 997, 296, 153, 711]            7   \n",
       "3  [322, 483, 20, 974, 641, 254, 376, 918, 512, 358]           10   \n",
       "4                  [322, 93, 955, 692, 88, 595, 131]            7   \n",
       "5                   [322, 836, 56, 224, 891, 3, 307]            7   \n",
       "\n",
       "                                             indices           selector  \n",
       "0  [338, 31, 656, 216, 93, 579, 519, 713, 300, 83...  without selection  \n",
       "1  [338, 31, 656, 216, 93, 579, 519, 713, 657, 30...  without selection  \n",
       "2  [338, 656, 216, 93, 579, 519, 713, 657, 300, 8...  without selection  \n",
       "3  [338, 31, 656, 216, 93, 579, 519, 713, 657, 30...  without selection  \n",
       "4  [338, 31, 656, 216, 579, 519, 713, 657, 300, 8...  without selection  \n",
       "5  [338, 31, 656, 216, 93, 579, 519, 713, 657, 30...  without selection  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_indices_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "10259f61-a633-4394-88f7-24551659a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем налучшую комбинацию строк для повышения качества работы моделей\n",
    "def optimize_classifier(X_train, X_test, y_train, y_test, model_type='logistic', n_trials=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Оптимизирует гиперпараметры для классификаторов с улучшенной обработкой ошибок SVM\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Тип модели: 'logistic', 'svm' или 'knn'\n",
    "    n_trials : int\n",
    "        Количество испытаний для Optuna\n",
    "    random_state : int\n",
    "        Seed для воспроизводимости\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации (лучшие параметры, метрики, study)\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "    # Проверяем, является ли это первым trial (нулевым по индексу)\n",
    "        \n",
    "            # Обычная логика подбора параметров\n",
    "        try:\n",
    "            if model_type == 'logistic':\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['l2', 'l1']),\n",
    "                    'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),LogisticRegression(**params, random_state=random_state))\n",
    "                \n",
    "            elif model_type == 'svm':\n",
    "                kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'kernel': kernel,\n",
    "                    'gamma': trial.suggest_float('gamma', 1e-5, 10, log=True),\n",
    "                    'tol': trial.suggest_float('tol', 1e-5, 1e-1, log=True),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                if kernel == 'poly':\n",
    "                    params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "                \n",
    "                model = make_pipeline(StandardScaler(),SVC(**params, random_state=random_state, probability=True))\n",
    "                \n",
    "            elif model_type == 'knn':\n",
    "                params = {\n",
    "                    'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
    "                    'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "                    'p': trial.suggest_int('p', 1, 3)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),KNeighborsClassifier(**params))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if model_type == 'svm':\n",
    "                error_info = f\"SVM failed with params: {params}. Error: {str(e)}\"\n",
    "                trial.set_user_attr(\"svm_error\", error_info)\n",
    "            return float('-inf')\n",
    "        \n",
    "        # Общая часть для всех моделей (дефолтных и оптимизированных)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y_train)) == 2:\n",
    "            trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return f1\n",
    "        \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    # Анализ результатов\n",
    "    best_params = study.best_params\n",
    "    best_metrics = {\n",
    "        'f1': study.best_value,\n",
    "        'accuracy': study.best_trial.user_attrs['accuracy'],\n",
    "    }\n",
    "    \n",
    "    if 'roc_auc' in study.best_trial.user_attrs:\n",
    "        best_metrics['roc_auc'] = study.best_trial.user_attrs['roc_auc']\n",
    "    \n",
    "    # Добавляем информацию о неудачных trials для SVM\n",
    "    if model_type == 'svm':\n",
    "        failed_trials = [t for t in study.trials if 'svm_error' in t.user_attrs]\n",
    "        if failed_trials:\n",
    "            best_metrics['failed_trials_count'] = len(failed_trials)\n",
    "            best_metrics['last_error'] = failed_trials[-1].user_attrs['svm_error']\n",
    "    \n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'best_params': best_params,\n",
    "        'best_metrics': best_metrics,\n",
    "        'study': study\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "44b5a03c-04b9-48de-81be-5b1fb9e7eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск подбора гиперпараметров\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train_logistic = X_train[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]].drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "y_train_logistic = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "X_test_logistic = X_test[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]]\n",
    "logistic_results = optimize_classifier(X_train_logistic, X_test_logistic, y_train_logistic, y_test, 'logistic', n_trials=200)\n",
    "\n",
    "X_train_svm = X_train[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]].drop(index=results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "y_train_svm = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "X_test_svm = X_test[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]]\n",
    "svm_results = optimize_classifier(X_train_svm, X_test_svm, y_train_svm, y_test, 'svm', n_trials=200)\n",
    "X_train_knn = X_train[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]].drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "y_train_knn = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "X_test_knn = X_test[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]]\n",
    "knn_results = optimize_classifier(X_train_knn, X_test_knn, y_train_knn, y_test, 'knn', n_trials=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a7e4f328-a7fb-40ed-b301-9daaee300865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best params: {'C': 87.73712049661573, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 475}\n",
      "LogisticRegression best f1: 0.7680\n",
      "LogisticRegression best accuracy: 0.7680\n",
      "SVM best params: {'kernel': 'rbf', 'C': 0.9365132873713495, 'gamma': 0.005688551861154316, 'tol': 0.002866449889135651, 'max_iter': 482}\n",
      "SVM best f1: 0.7835\n",
      "SVM best accuracy: 0.7835\n",
      "KNN best params: {'n_neighbors': 5, 'weights': 'uniform', 'p': 2}\n",
      "KNN best f1: 0.8556\n",
      "KNN best accuracy: 0.8557\n"
     ]
    }
   ],
   "source": [
    "# Вывод результатов подбора гиперпараметров для линейных моделей\n",
    "print(f\"LogisticRegression best params: {logistic_results['best_params']}\")\n",
    "print(f\"LogisticRegression best f1: {logistic_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"LogisticRegression best accuracy: {logistic_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"SVM best params: {svm_results['best_params']}\")\n",
    "print(f\"SVM best f1: {svm_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"SVM best accuracy: {svm_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"KNN best params: {knn_results['best_params']}\")\n",
    "print(f\"KNN best f1: {knn_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"KNN best accuracy: {knn_results['best_metrics']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f55b546d-50e7-4fd4-9f1a-daf3824d6ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost Classifier...\n",
      "\n",
      "Best trial:\n",
      "  F1-score: 0.814789\n",
      "  Accuracy: 0.814433\n",
      "  ROC-AUC: 0.852648\n",
      "\n",
      "Best params:\n",
      "  iterations: 705\n",
      "  depth: 9\n",
      "  learning_rate: 0.10770010910094868\n",
      "  l2_leaf_reg: 3.595910511863626\n",
      "  random_strength: 9.832420369289425\n",
      "  bagging_temperature: 0.6728343909051019\n",
      "  border_count: 33\n",
      "  min_data_in_leaf: 25\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для CatBoost\n",
    "# Подготовка данных для CatBoost\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_cb = X_train[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]].drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "y_train_cb = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "X_test_cb = X_test[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]]\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров CatBoostClassifier\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42,\n",
    "        'thread_count': 4\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(\n",
    "        X_train_cb, \n",
    "        y_train_cb,\n",
    "        eval_set=(X_test_cb, y_test),\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Предсказание и метрики\n",
    "    y_pred = model.predict(X_test_cb)\n",
    "    y_proba = model.predict_proba(X_test_cb)[:, 1] if len(y_train_cb.unique()) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing CatBoost Classifier...\")\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study.trials) > 0:\n",
    "    best_trial_cb = study.best_trial\n",
    "    \n",
    "    print(\"\\nBest trial:\")\n",
    "    print(f\"  F1-score: {best_trial_cb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_cb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_cb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_cb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_cb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_cb.params\n",
    "    best_params.update({\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42\n",
    "    })\n",
    "    \n",
    "    best_model = CatBoostClassifier(**best_params)\n",
    "    best_model.fit(\n",
    "        pd.concat([X_train_cb, X_test_cb]),\n",
    "        pd.concat([y_train_cb, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6cc0194b-d65f-4c27-98bf-4258b6634371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost Classifier...\n",
      "\n",
      "Best XGBoost Classifier:\n",
      "  F1-score: 0.778203\n",
      "  Accuracy: 0.778351\n",
      "  ROC-AUC: 0.817143\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 405\n",
      "  max_depth: 8\n",
      "  learning_rate: 0.17804954060277914\n",
      "  subsample: 0.6462412740238879\n",
      "  colsample_bytree: 0.7579368653630655\n",
      "  gamma: 0.023045002178660144\n",
      "  reg_alpha: 7.036008267490183\n",
      "  reg_lambda: 9.130441034469362\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для XGBoost\n",
    "# Подготовка данных для XGBoost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]].drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]]\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров XGBoost (классификация)\"\"\"\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'eval_metric': 'logloss',\n",
    "        'early_stopping_rounds': 20  # Перенесено сюда\n",
    "    }\n",
    "    \n",
    "    # Автоматическое определение типа задачи\n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        xgb_params['objective'] = 'multi:softmax'\n",
    "        xgb_params['num_class'] = len(np.unique(y_train))\n",
    "    else:\n",
    "        xgb_params['objective'] = 'binary:logistic'\n",
    "\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)],  # Оставлено здесь\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Остальной код без изменений\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_xgb = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing XGBoost Classifier...\")\n",
    "    study_xgb.optimize(\n",
    "        objective_xgb,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_xgb.trials) > 0:\n",
    "    best_trial_xgb = study_xgb.best_trial\n",
    "    \n",
    "    print(\"\\nBest XGBoost Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_xgb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_xgb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_xgb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_xgb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_xgb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_xgb.params\n",
    "    best_params.update({\n",
    "        'objective': 'multi:softmax' if len(np.unique(y_train)) > 2 else 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        best_params['num_class'] = len(np.unique(y_train))\n",
    "    \n",
    "    best_xgb = XGBClassifier(**best_params)\n",
    "    best_xgb.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d4b9f77b-8173-459f-bd02-b586ef3f9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest Classifier...\n",
      "\n",
      "Best RandomForest Classifier:\n",
      "  F1-score: 0.767739\n",
      "  Accuracy: 0.768041\n",
      "  ROC-AUC: 0.809597\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 443\n",
      "  max_depth: 4\n",
      "  min_samples_split: 2\n",
      "  min_samples_leaf: 1\n",
      "  max_features: 0.15795879437070806\n",
      "  bootstrap: False\n",
      "  class_weight: None\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для RandomForest\n",
    "# Подготовка данных для RandomForest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]].drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]]\n",
    "\n",
    "def objective_rf(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров RandomForest (классификация)\"\"\"\n",
    "    \n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'criterion': 'gini'  # Можно добавить выбор между 'gini' и 'entropy'\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный для многоклассовой)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_rf = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing RandomForest Classifier...\")\n",
    "    study_rf.optimize(\n",
    "        objective_rf,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_rf.trials) > 0:\n",
    "    best_trial_rf = study_rf.best_trial\n",
    "    \n",
    "    print(\"\\nBest RandomForest Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_rf.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_rf.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_rf.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_rf.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_rf.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_rf.params\n",
    "    best_params.update({\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    best_rf = RandomForestClassifier(**best_params)\n",
    "    best_rf.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5b1c00b7-5c8d-4a50-983d-cda7ebcd19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем лучшие результаты\n",
    "best_results_total = []\n",
    "logistic = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['model'][0],\n",
    "        'best_f1':  logistic_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  logistic_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['indices'][0])\n",
    "    }\n",
    "best_results_total.append(logistic)\n",
    "svm = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'SVM']['model'][1],\n",
    "        'best_f1':  svm_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  svm_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'SVM']['indices'][1])\n",
    "    }\n",
    "best_results_total.append(svm)\n",
    "knn = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'KNN']['model'][2],\n",
    "        'best_f1':  knn_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  knn_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'KNN']['indices'][2])\n",
    "    }\n",
    "best_results_total.append(knn)\n",
    "rf = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'RandomForest']['model'][3],\n",
    "        'best_f1':  best_trial_rf.value,\n",
    "        'best_accuracy':  best_trial_rf.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['indices'][3])\n",
    "    }\n",
    "best_results_total.append(rf)\n",
    "\n",
    "xg = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'XGBoost']['model'][4],\n",
    "        'best_f1':  best_trial_xgb.value,\n",
    "        'best_accuracy':  best_trial_xgb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['indices'][4])\n",
    "    }\n",
    "best_results_total.append(xg)\n",
    "cb = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'CatBoost']['model'][5],\n",
    "        'best_f1':  best_trial_cb.value,\n",
    "        'best_accuracy':  best_trial_cb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['indices'][5])\n",
    "    }\n",
    "best_results_total.append(cb)\n",
    "\n",
    "best_results_total_df = pd.DataFrame(best_results_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b12ad85d-f5c6-490c-950b-c6e38a73a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>count_features</th>\n",
       "      <th>count_strings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.767986</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>198</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.783482</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>203</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.855609</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>198</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.767739</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>204</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.778203</td>\n",
       "      <td>0.778351</td>\n",
       "      <td>20</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.814789</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>17</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  count_features  count_strings\n",
       "0  LogisticRegression  0.767986       0.768041             198            763\n",
       "1                 SVM  0.783482       0.783505             203            769\n",
       "2                 KNN  0.855609       0.855670             198            765\n",
       "3        RandomForest  0.767739       0.768041             204            762\n",
       "4             XGBoost  0.778203       0.778351              20            765\n",
       "5            CatBoost  0.814789       0.814433              17            765"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лучшие результаты\n",
    "best_results_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f059f47-0250-4ccd-9494-64142ba4112c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
