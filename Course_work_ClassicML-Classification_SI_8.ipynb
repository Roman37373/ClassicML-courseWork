{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.937578Z",
     "start_time": "2025-05-28T16:44:43.537577Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shap\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from scipy import stats\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, \n",
    "                           r2_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, classification_report,\n",
    "                           roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Other ML libraries\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267e6757ca7a5130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.960272Z",
     "start_time": "2025-05-28T16:44:45.957343Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ad5b2279b2b7ab49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:11.783119Z",
     "start_time": "2025-05-28T16:57:11.742132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "df = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3d4d43a0b8f2e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:26.787706Z",
     "start_time": "2025-05-28T17:02:26.674195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>IC50, mM</th>\n",
       "      <th>CC50, mM</th>\n",
       "      <th>SI</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>NumValenceElectrons</th>\n",
       "      <th>NumRadicalElectrons</th>\n",
       "      <th>MaxPartialCharge</th>\n",
       "      <th>MinPartialCharge</th>\n",
       "      <th>MaxAbsPartialCharge</th>\n",
       "      <th>MinAbsPartialCharge</th>\n",
       "      <th>FpDensityMorgan1</th>\n",
       "      <th>FpDensityMorgan2</th>\n",
       "      <th>FpDensityMorgan3</th>\n",
       "      <th>BCUT2D_MWHI</th>\n",
       "      <th>BCUT2D_MWLOW</th>\n",
       "      <th>BCUT2D_CHGHI</th>\n",
       "      <th>BCUT2D_CHGLO</th>\n",
       "      <th>BCUT2D_LOGPHI</th>\n",
       "      <th>BCUT2D_LOGPLOW</th>\n",
       "      <th>BCUT2D_MRHI</th>\n",
       "      <th>BCUT2D_MRLOW</th>\n",
       "      <th>AvgIpc</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi0n</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi1n</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>Chi2n</th>\n",
       "      <th>Chi2v</th>\n",
       "      <th>Chi3n</th>\n",
       "      <th>Chi3v</th>\n",
       "      <th>Chi4n</th>\n",
       "      <th>Chi4v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Ipc</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>LabuteASA</th>\n",
       "      <th>PEOE_VSA1</th>\n",
       "      <th>PEOE_VSA10</th>\n",
       "      <th>PEOE_VSA11</th>\n",
       "      <th>PEOE_VSA12</th>\n",
       "      <th>PEOE_VSA13</th>\n",
       "      <th>PEOE_VSA14</th>\n",
       "      <th>PEOE_VSA2</th>\n",
       "      <th>PEOE_VSA3</th>\n",
       "      <th>PEOE_VSA4</th>\n",
       "      <th>PEOE_VSA5</th>\n",
       "      <th>PEOE_VSA6</th>\n",
       "      <th>PEOE_VSA7</th>\n",
       "      <th>PEOE_VSA8</th>\n",
       "      <th>PEOE_VSA9</th>\n",
       "      <th>SMR_VSA1</th>\n",
       "      <th>SMR_VSA10</th>\n",
       "      <th>SMR_VSA2</th>\n",
       "      <th>SMR_VSA3</th>\n",
       "      <th>SMR_VSA4</th>\n",
       "      <th>SMR_VSA5</th>\n",
       "      <th>SMR_VSA6</th>\n",
       "      <th>SMR_VSA7</th>\n",
       "      <th>SMR_VSA8</th>\n",
       "      <th>SMR_VSA9</th>\n",
       "      <th>SlogP_VSA1</th>\n",
       "      <th>SlogP_VSA10</th>\n",
       "      <th>SlogP_VSA11</th>\n",
       "      <th>SlogP_VSA12</th>\n",
       "      <th>SlogP_VSA2</th>\n",
       "      <th>SlogP_VSA3</th>\n",
       "      <th>SlogP_VSA4</th>\n",
       "      <th>SlogP_VSA5</th>\n",
       "      <th>SlogP_VSA6</th>\n",
       "      <th>SlogP_VSA7</th>\n",
       "      <th>SlogP_VSA8</th>\n",
       "      <th>SlogP_VSA9</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>EState_VSA1</th>\n",
       "      <th>EState_VSA10</th>\n",
       "      <th>EState_VSA11</th>\n",
       "      <th>EState_VSA2</th>\n",
       "      <th>EState_VSA3</th>\n",
       "      <th>EState_VSA4</th>\n",
       "      <th>EState_VSA5</th>\n",
       "      <th>EState_VSA6</th>\n",
       "      <th>EState_VSA7</th>\n",
       "      <th>EState_VSA8</th>\n",
       "      <th>EState_VSA9</th>\n",
       "      <th>VSA_EState1</th>\n",
       "      <th>VSA_EState10</th>\n",
       "      <th>VSA_EState2</th>\n",
       "      <th>VSA_EState3</th>\n",
       "      <th>VSA_EState4</th>\n",
       "      <th>VSA_EState5</th>\n",
       "      <th>VSA_EState6</th>\n",
       "      <th>VSA_EState7</th>\n",
       "      <th>VSA_EState8</th>\n",
       "      <th>VSA_EState9</th>\n",
       "      <th>FractionCSP3</th>\n",
       "      <th>HeavyAtomCount</th>\n",
       "      <th>NHOHCount</th>\n",
       "      <th>NOCount</th>\n",
       "      <th>NumAliphaticCarbocycles</th>\n",
       "      <th>NumAliphaticHeterocycles</th>\n",
       "      <th>NumAliphaticRings</th>\n",
       "      <th>NumAromaticCarbocycles</th>\n",
       "      <th>NumAromaticHeterocycles</th>\n",
       "      <th>NumAromaticRings</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHeteroatoms</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>NumSaturatedCarbocycles</th>\n",
       "      <th>NumSaturatedHeterocycles</th>\n",
       "      <th>NumSaturatedRings</th>\n",
       "      <th>RingCount</th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>MolMR</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>fr_Ar_OH</th>\n",
       "      <th>fr_COO</th>\n",
       "      <th>fr_COO2</th>\n",
       "      <th>fr_C_O</th>\n",
       "      <th>fr_C_O_noCOO</th>\n",
       "      <th>fr_C_S</th>\n",
       "      <th>fr_HOCCN</th>\n",
       "      <th>fr_Imine</th>\n",
       "      <th>fr_NH0</th>\n",
       "      <th>fr_NH1</th>\n",
       "      <th>fr_NH2</th>\n",
       "      <th>fr_N_O</th>\n",
       "      <th>fr_Ndealkylation1</th>\n",
       "      <th>fr_Ndealkylation2</th>\n",
       "      <th>fr_Nhpyrrole</th>\n",
       "      <th>fr_SH</th>\n",
       "      <th>fr_aldehyde</th>\n",
       "      <th>fr_alkyl_carbamate</th>\n",
       "      <th>fr_alkyl_halide</th>\n",
       "      <th>fr_allylic_oxid</th>\n",
       "      <th>fr_amide</th>\n",
       "      <th>fr_amidine</th>\n",
       "      <th>fr_aniline</th>\n",
       "      <th>fr_aryl_methyl</th>\n",
       "      <th>fr_azide</th>\n",
       "      <th>fr_azo</th>\n",
       "      <th>fr_barbitur</th>\n",
       "      <th>fr_benzene</th>\n",
       "      <th>fr_benzodiazepine</th>\n",
       "      <th>fr_bicyclic</th>\n",
       "      <th>fr_diazo</th>\n",
       "      <th>fr_dihydropyridine</th>\n",
       "      <th>fr_epoxide</th>\n",
       "      <th>fr_ester</th>\n",
       "      <th>fr_ether</th>\n",
       "      <th>fr_furan</th>\n",
       "      <th>fr_guanido</th>\n",
       "      <th>fr_halogen</th>\n",
       "      <th>fr_hdrzine</th>\n",
       "      <th>fr_hdrzone</th>\n",
       "      <th>fr_imidazole</th>\n",
       "      <th>fr_imide</th>\n",
       "      <th>fr_isocyan</th>\n",
       "      <th>fr_isothiocyan</th>\n",
       "      <th>fr_ketone</th>\n",
       "      <th>fr_ketone_Topliss</th>\n",
       "      <th>fr_lactam</th>\n",
       "      <th>fr_lactone</th>\n",
       "      <th>fr_methoxy</th>\n",
       "      <th>fr_morpholine</th>\n",
       "      <th>fr_nitrile</th>\n",
       "      <th>fr_nitro</th>\n",
       "      <th>fr_nitro_arom</th>\n",
       "      <th>fr_nitro_arom_nonortho</th>\n",
       "      <th>fr_nitroso</th>\n",
       "      <th>fr_oxazole</th>\n",
       "      <th>fr_oxime</th>\n",
       "      <th>fr_para_hydroxylation</th>\n",
       "      <th>fr_phenol</th>\n",
       "      <th>fr_phenol_noOrthoHbond</th>\n",
       "      <th>fr_phos_acid</th>\n",
       "      <th>fr_phos_ester</th>\n",
       "      <th>fr_piperdine</th>\n",
       "      <th>fr_piperzine</th>\n",
       "      <th>fr_priamide</th>\n",
       "      <th>fr_prisulfonamd</th>\n",
       "      <th>fr_pyridine</th>\n",
       "      <th>fr_quatN</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.239374</td>\n",
       "      <td>175.482382</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.417362</td>\n",
       "      <td>42.928571</td>\n",
       "      <td>384.652</td>\n",
       "      <td>340.300</td>\n",
       "      <td>384.350449</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>1.321429</td>\n",
       "      <td>14.822266</td>\n",
       "      <td>9.700470</td>\n",
       "      <td>2.600532</td>\n",
       "      <td>-2.343082</td>\n",
       "      <td>2.644698</td>\n",
       "      <td>-2.322229</td>\n",
       "      <td>5.944519</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.164038</td>\n",
       "      <td>611.920301</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>20.606247</td>\n",
       "      <td>6.947534</td>\n",
       "      <td>2.868737</td>\n",
       "      <td>173.630124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>64.208216</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.188192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.807589</td>\n",
       "      <td>1.764908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.258223</td>\n",
       "      <td>16.981087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1212</td>\n",
       "      <td>121.5300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.771831</td>\n",
       "      <td>5.402819</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.462473</td>\n",
       "      <td>45.214286</td>\n",
       "      <td>388.684</td>\n",
       "      <td>340.300</td>\n",
       "      <td>388.381750</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>-0.313407</td>\n",
       "      <td>0.313407</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>14.975110</td>\n",
       "      <td>9.689226</td>\n",
       "      <td>2.614066</td>\n",
       "      <td>-2.394690</td>\n",
       "      <td>2.658342</td>\n",
       "      <td>-2.444817</td>\n",
       "      <td>5.134527</td>\n",
       "      <td>0.120322</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.080362</td>\n",
       "      <td>516.780124</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>21.163454</td>\n",
       "      <td>7.257648</td>\n",
       "      <td>3.027177</td>\n",
       "      <td>174.939204</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>97.951860</td>\n",
       "      <td>12.083682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.173194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>77.297729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.922833</td>\n",
       "      <td>2.153503</td>\n",
       "      <td>1.914377</td>\n",
       "      <td>1.536674</td>\n",
       "      <td>14.135381</td>\n",
       "      <td>17.670565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1556</td>\n",
       "      <td>120.5074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>223.808778</td>\n",
       "      <td>161.142320</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.260923</td>\n",
       "      <td>42.187500</td>\n",
       "      <td>446.808</td>\n",
       "      <td>388.344</td>\n",
       "      <td>446.458903</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>-0.325573</td>\n",
       "      <td>0.325573</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.156250</td>\n",
       "      <td>15.353938</td>\n",
       "      <td>9.681293</td>\n",
       "      <td>2.665274</td>\n",
       "      <td>-2.477203</td>\n",
       "      <td>2.679014</td>\n",
       "      <td>-2.565224</td>\n",
       "      <td>5.117187</td>\n",
       "      <td>-0.922902</td>\n",
       "      <td>3.214947</td>\n",
       "      <td>1.219066</td>\n",
       "      <td>643.620154</td>\n",
       "      <td>23.794682</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>14.595754</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>8.610751e+06</td>\n",
       "      <td>25.026112</td>\n",
       "      <td>7.709373</td>\n",
       "      <td>3.470070</td>\n",
       "      <td>201.238858</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>23.671624</td>\n",
       "      <td>53.363882</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>41.280201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.329944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>86.263791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.733111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.517630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.184127</td>\n",
       "      <td>1.930720</td>\n",
       "      <td>1.738402</td>\n",
       "      <td>14.491619</td>\n",
       "      <td>18.287216</td>\n",
       "      <td>10.183618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1292</td>\n",
       "      <td>138.4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.705624</td>\n",
       "      <td>107.855654</td>\n",
       "      <td>63.235294</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.377846</td>\n",
       "      <td>41.862069</td>\n",
       "      <td>398.679</td>\n",
       "      <td>352.311</td>\n",
       "      <td>398.366099</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>14.821216</td>\n",
       "      <td>9.700497</td>\n",
       "      <td>2.600529</td>\n",
       "      <td>-2.342885</td>\n",
       "      <td>2.644709</td>\n",
       "      <td>-2.322030</td>\n",
       "      <td>5.944502</td>\n",
       "      <td>0.193510</td>\n",
       "      <td>3.179270</td>\n",
       "      <td>1.120513</td>\n",
       "      <td>626.651366</td>\n",
       "      <td>20.916003</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>13.627794</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>3.572142e+06</td>\n",
       "      <td>21.567454</td>\n",
       "      <td>7.485204</td>\n",
       "      <td>3.263848</td>\n",
       "      <td>179.995066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.804888</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>70.629038</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.194720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.827852</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.695439</td>\n",
       "      <td>17.012013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.5113</td>\n",
       "      <td>126.1470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>107.131532</td>\n",
       "      <td>139.270991</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.429038</td>\n",
       "      <td>36.514286</td>\n",
       "      <td>466.713</td>\n",
       "      <td>424.377</td>\n",
       "      <td>466.334799</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>-0.257239</td>\n",
       "      <td>0.257239</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.257143</td>\n",
       "      <td>14.831112</td>\n",
       "      <td>9.700386</td>\n",
       "      <td>2.602486</td>\n",
       "      <td>-2.342009</td>\n",
       "      <td>2.648473</td>\n",
       "      <td>-2.318893</td>\n",
       "      <td>5.963448</td>\n",
       "      <td>0.193687</td>\n",
       "      <td>3.337074</td>\n",
       "      <td>1.136678</td>\n",
       "      <td>1101.164252</td>\n",
       "      <td>24.639617</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>16.526773</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>1.053758e+08</td>\n",
       "      <td>23.194917</td>\n",
       "      <td>7.639211</td>\n",
       "      <td>3.345855</td>\n",
       "      <td>211.919602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.807891</td>\n",
       "      <td>103.003916</td>\n",
       "      <td>22.253351</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.798143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>86.488175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.657840</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>91.194256</td>\n",
       "      <td>58.515746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>29.631406</td>\n",
       "      <td>61.075203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.073360</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.301020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.071783</td>\n",
       "      <td>1.605178</td>\n",
       "      <td>17.869058</td>\n",
       "      <td>8.627311</td>\n",
       "      <td>14.692318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9.1148</td>\n",
       "      <td>148.3380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>996</td>\n",
       "      <td>31.000104</td>\n",
       "      <td>34.999650</td>\n",
       "      <td>1.129017</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>-0.476142</td>\n",
       "      <td>0.382752</td>\n",
       "      <td>49.133333</td>\n",
       "      <td>414.542</td>\n",
       "      <td>380.270</td>\n",
       "      <td>414.240624</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>1.866667</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>16.586886</td>\n",
       "      <td>9.344314</td>\n",
       "      <td>2.726237</td>\n",
       "      <td>-2.677345</td>\n",
       "      <td>2.739076</td>\n",
       "      <td>-2.646743</td>\n",
       "      <td>5.980114</td>\n",
       "      <td>-0.196385</td>\n",
       "      <td>3.023764</td>\n",
       "      <td>1.646946</td>\n",
       "      <td>857.600295</td>\n",
       "      <td>21.637464</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>14.097861</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>6.242348e+06</td>\n",
       "      <td>20.263719</td>\n",
       "      <td>6.198453</td>\n",
       "      <td>2.219273</td>\n",
       "      <td>178.490760</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>24.360600</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.017713</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.67</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.409521</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.503509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.515343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498752</td>\n",
       "      <td>-0.405436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.985276</td>\n",
       "      <td>8.824371</td>\n",
       "      <td>1.494852</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.3002</td>\n",
       "      <td>109.8350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>997</td>\n",
       "      <td>31.999934</td>\n",
       "      <td>33.999415</td>\n",
       "      <td>1.062484</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>0.369425</td>\n",
       "      <td>44.542857</td>\n",
       "      <td>485.621</td>\n",
       "      <td>446.309</td>\n",
       "      <td>485.277738</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>-0.467493</td>\n",
       "      <td>0.467493</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.457143</td>\n",
       "      <td>16.586914</td>\n",
       "      <td>9.343622</td>\n",
       "      <td>2.725543</td>\n",
       "      <td>-2.679467</td>\n",
       "      <td>2.738755</td>\n",
       "      <td>-2.655659</td>\n",
       "      <td>5.980828</td>\n",
       "      <td>-0.187625</td>\n",
       "      <td>3.130958</td>\n",
       "      <td>1.535171</td>\n",
       "      <td>1016.917688</td>\n",
       "      <td>25.499271</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>16.402391</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>5.897229e+07</td>\n",
       "      <td>24.511583</td>\n",
       "      <td>7.908743</td>\n",
       "      <td>3.147136</td>\n",
       "      <td>207.296970</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>23.815096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.185457</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.966734</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>73.143616</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>23.344043</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.347395</td>\n",
       "      <td>5.917906</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.086396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.516353</td>\n",
       "      <td>2.923049</td>\n",
       "      <td>0.162524</td>\n",
       "      <td>-1.300354</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>7.521836</td>\n",
       "      <td>10.378794</td>\n",
       "      <td>1.327425</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3.8049</td>\n",
       "      <td>127.4397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>998</td>\n",
       "      <td>30.999883</td>\n",
       "      <td>33.999458</td>\n",
       "      <td>1.096761</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>0.026535</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>41.973684</td>\n",
       "      <td>545.742</td>\n",
       "      <td>502.398</td>\n",
       "      <td>545.281109</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>-0.467485</td>\n",
       "      <td>0.467485</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>1.894737</td>\n",
       "      <td>2.552632</td>\n",
       "      <td>32.166365</td>\n",
       "      <td>9.343613</td>\n",
       "      <td>2.725818</td>\n",
       "      <td>-2.679527</td>\n",
       "      <td>2.738943</td>\n",
       "      <td>-2.656447</td>\n",
       "      <td>7.980998</td>\n",
       "      <td>-0.187687</td>\n",
       "      <td>3.204255</td>\n",
       "      <td>1.493776</td>\n",
       "      <td>1070.961298</td>\n",
       "      <td>27.620591</td>\n",
       "      <td>23.633394</td>\n",
       "      <td>24.449891</td>\n",
       "      <td>17.940396</td>\n",
       "      <td>14.301838</td>\n",
       "      <td>15.695685</td>\n",
       "      <td>13.248561</td>\n",
       "      <td>14.234160</td>\n",
       "      <td>11.326709</td>\n",
       "      <td>11.970659</td>\n",
       "      <td>9.725583</td>\n",
       "      <td>10.196987</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>2.627956e+08</td>\n",
       "      <td>27.726151</td>\n",
       "      <td>9.668673</td>\n",
       "      <td>3.822745</td>\n",
       "      <td>230.149965</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>79.620167</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>35.576981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>78.682541</td>\n",
       "      <td>19.118420</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>48.975357</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>72.640700</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>28.759033</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.932405</td>\n",
       "      <td>12.338728</td>\n",
       "      <td>44.277783</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>6.255769</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.305031</td>\n",
       "      <td>1.639399</td>\n",
       "      <td>52.527620</td>\n",
       "      <td>3.081660</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>-0.487671</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>10.055493</td>\n",
       "      <td>8.774745</td>\n",
       "      <td>1.364715</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.5381</td>\n",
       "      <td>144.7647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>999</td>\n",
       "      <td>31.998959</td>\n",
       "      <td>32.999644</td>\n",
       "      <td>1.031272</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>0.146522</td>\n",
       "      <td>-1.408652</td>\n",
       "      <td>0.381559</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>522.635</td>\n",
       "      <td>480.299</td>\n",
       "      <td>522.282883</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>-0.468755</td>\n",
       "      <td>0.468755</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.351351</td>\n",
       "      <td>1.864865</td>\n",
       "      <td>16.540061</td>\n",
       "      <td>9.364015</td>\n",
       "      <td>2.730109</td>\n",
       "      <td>-2.652209</td>\n",
       "      <td>2.704027</td>\n",
       "      <td>-2.678553</td>\n",
       "      <td>5.950258</td>\n",
       "      <td>-0.225309</td>\n",
       "      <td>2.887043</td>\n",
       "      <td>2.325807</td>\n",
       "      <td>957.299494</td>\n",
       "      <td>27.921921</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>17.309003</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>7.702780e+07</td>\n",
       "      <td>29.111081</td>\n",
       "      <td>10.369092</td>\n",
       "      <td>4.164473</td>\n",
       "      <td>218.836986</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>5.783245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.877221</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.192033</td>\n",
       "      <td>56.278648</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>51.104983</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>29.660466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.099656</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.27</td>\n",
       "      <td>63.742418</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.512100</td>\n",
       "      <td>19.262465</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>13.847474</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>20.885832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.303382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.785593</td>\n",
       "      <td>-6.848660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.955837</td>\n",
       "      <td>7.488627</td>\n",
       "      <td>5.083909</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.3649</td>\n",
       "      <td>131.7080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1000</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>-0.447978</td>\n",
       "      <td>0.452565</td>\n",
       "      <td>48.580645</td>\n",
       "      <td>426.597</td>\n",
       "      <td>388.293</td>\n",
       "      <td>426.277010</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>1.064516</td>\n",
       "      <td>1.774194</td>\n",
       "      <td>2.451613</td>\n",
       "      <td>16.525216</td>\n",
       "      <td>9.330327</td>\n",
       "      <td>2.704274</td>\n",
       "      <td>-2.696212</td>\n",
       "      <td>2.737481</td>\n",
       "      <td>-2.668850</td>\n",
       "      <td>5.978085</td>\n",
       "      <td>-0.201381</td>\n",
       "      <td>2.740713</td>\n",
       "      <td>1.651446</td>\n",
       "      <td>870.462214</td>\n",
       "      <td>22.344571</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>14.597861</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>9.086032e+06</td>\n",
       "      <td>21.398566</td>\n",
       "      <td>6.776167</td>\n",
       "      <td>2.566599</td>\n",
       "      <td>186.107099</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>11.566490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.969305</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>30.092446</td>\n",
       "      <td>12.524788</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>17.535795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.645593</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.44</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.470910</td>\n",
       "      <td>36.243945</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>5.298868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.472742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776999</td>\n",
       "      <td>1.605640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.247616</td>\n",
       "      <td>8.999949</td>\n",
       "      <td>1.514853</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.1488</td>\n",
       "      <td>117.9840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    IC50, mM    CC50, mM         SI  MaxAbsEStateIndex  \\\n",
       "0             0    6.239374  175.482382  28.125000           5.094096   \n",
       "1             1    0.771831    5.402819   7.000000           3.961417   \n",
       "2             2  223.808778  161.142320   0.720000           2.627117   \n",
       "3             3    1.705624  107.855654  63.235294           5.097360   \n",
       "4             4  107.131532  139.270991   1.300000           5.150510   \n",
       "..          ...         ...         ...        ...                ...   \n",
       "993         996   31.000104   34.999650   1.129017          12.934891   \n",
       "994         997   31.999934   33.999415   1.062484          13.635345   \n",
       "995         998   30.999883   33.999458   1.096761          13.991690   \n",
       "996         999   31.998959   32.999644   1.031272          13.830180   \n",
       "997        1000   99.999531   99.999531   1.000000          13.380863   \n",
       "\n",
       "     MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex       qed        SPS  \\\n",
       "0          5.094096           0.387225        0.387225  0.417362  42.928571   \n",
       "1          3.961417           0.533868        0.533868  0.462473  45.214286   \n",
       "2          2.627117           0.543231        0.543231  0.260923  42.187500   \n",
       "3          5.097360           0.390603        0.390603  0.377846  41.862069   \n",
       "4          5.150510           0.270476        0.270476  0.429038  36.514286   \n",
       "..              ...                ...             ...       ...        ...   \n",
       "993       12.934891           0.048029       -0.476142  0.382752  49.133333   \n",
       "994       13.635345           0.030329       -0.699355  0.369425  44.542857   \n",
       "995       13.991690           0.026535       -0.650790  0.284923  41.973684   \n",
       "996       13.830180           0.146522       -1.408652  0.381559  39.000000   \n",
       "997       13.380863           0.002425       -0.447978  0.452565  48.580645   \n",
       "\n",
       "       MolWt  HeavyAtomMolWt  ExactMolWt  NumValenceElectrons  \\\n",
       "0    384.652         340.300  384.350449                  158   \n",
       "1    388.684         340.300  388.381750                  162   \n",
       "2    446.808         388.344  446.458903                  186   \n",
       "3    398.679         352.311  398.366099                  164   \n",
       "4    466.713         424.377  466.334799                  184   \n",
       "..       ...             ...         ...                  ...   \n",
       "993  414.542         380.270  414.240624                  164   \n",
       "994  485.621         446.309  485.277738                  192   \n",
       "995  545.742         502.398  545.281109                  210   \n",
       "996  522.635         480.299  522.282883                  208   \n",
       "997  426.597         388.293  426.277010                  170   \n",
       "\n",
       "     NumRadicalElectrons  MaxPartialCharge  MinPartialCharge  \\\n",
       "0                      0          0.038844         -0.293526   \n",
       "1                      0          0.012887         -0.313407   \n",
       "2                      0          0.094802         -0.325573   \n",
       "3                      0          0.038844         -0.293526   \n",
       "4                      0          0.062897         -0.257239   \n",
       "..                   ...               ...               ...   \n",
       "993                    0          0.317890         -0.468587   \n",
       "994                    0          0.327562         -0.467493   \n",
       "995                    0          0.327887         -0.467485   \n",
       "996                    0          0.312509         -0.468755   \n",
       "997                    0          0.311311         -0.468587   \n",
       "\n",
       "     MaxAbsPartialCharge  MinAbsPartialCharge  FpDensityMorgan1  \\\n",
       "0               0.293526             0.038844          0.642857   \n",
       "1               0.313407             0.012887          0.607143   \n",
       "2               0.325573             0.094802          0.562500   \n",
       "3               0.293526             0.038844          0.620690   \n",
       "4               0.257239             0.062897          0.600000   \n",
       "..                   ...                  ...               ...   \n",
       "993             0.468587             0.317890          1.133333   \n",
       "994             0.467493             0.327562          1.085714   \n",
       "995             0.467485             0.327887          1.157895   \n",
       "996             0.468755             0.312509          0.756757   \n",
       "997             0.468587             0.311311          1.064516   \n",
       "\n",
       "     FpDensityMorgan2  FpDensityMorgan3  BCUT2D_MWHI  BCUT2D_MWLOW  \\\n",
       "0            1.035714          1.321429    14.822266      9.700470   \n",
       "1            1.000000          1.285714    14.975110      9.689226   \n",
       "2            0.906250          1.156250    15.353938      9.681293   \n",
       "3            1.000000          1.310345    14.821216      9.700497   \n",
       "4            0.971429          1.257143    14.831112      9.700386   \n",
       "..                ...               ...          ...           ...   \n",
       "993          1.866667          2.533333    16.586886      9.344314   \n",
       "994          1.800000          2.457143    16.586914      9.343622   \n",
       "995          1.894737          2.552632    32.166365      9.343613   \n",
       "996          1.351351          1.864865    16.540061      9.364015   \n",
       "997          1.774194          2.451613    16.525216      9.330327   \n",
       "\n",
       "     BCUT2D_CHGHI  BCUT2D_CHGLO  BCUT2D_LOGPHI  BCUT2D_LOGPLOW  BCUT2D_MRHI  \\\n",
       "0        2.600532     -2.343082       2.644698       -2.322229     5.944519   \n",
       "1        2.614066     -2.394690       2.658342       -2.444817     5.134527   \n",
       "2        2.665274     -2.477203       2.679014       -2.565224     5.117187   \n",
       "3        2.600529     -2.342885       2.644709       -2.322030     5.944502   \n",
       "4        2.602486     -2.342009       2.648473       -2.318893     5.963448   \n",
       "..            ...           ...            ...             ...          ...   \n",
       "993      2.726237     -2.677345       2.739076       -2.646743     5.980114   \n",
       "994      2.725543     -2.679467       2.738755       -2.655659     5.980828   \n",
       "995      2.725818     -2.679527       2.738943       -2.656447     7.980998   \n",
       "996      2.730109     -2.652209       2.704027       -2.678553     5.950258   \n",
       "997      2.704274     -2.696212       2.737481       -2.668850     5.978085   \n",
       "\n",
       "     BCUT2D_MRLOW    AvgIpc  BalabanJ      BertzCT       Chi0      Chi0n  \\\n",
       "0        0.193481  3.150503  1.164038   611.920301  20.208896  19.534409   \n",
       "1        0.120322  3.150503  1.080362   516.780124  20.208896  19.794682   \n",
       "2       -0.922902  3.214947  1.219066   643.620154  23.794682  23.689110   \n",
       "3        0.193510  3.179270  1.120513   626.651366  20.916003  20.241516   \n",
       "4        0.193687  3.337074  1.136678  1101.164252  24.639617  22.617677   \n",
       "..            ...       ...       ...          ...        ...        ...   \n",
       "993     -0.196385  3.023764  1.646946   857.600295  21.637464  18.825334   \n",
       "994     -0.187625  3.130958  1.535171  1016.917688  25.499271  21.810933   \n",
       "995     -0.187687  3.204255  1.493776  1070.961298  27.620591  23.633394   \n",
       "996     -0.225309  2.887043  2.325807   957.299494  27.921921  23.380977   \n",
       "997     -0.201381  2.740713  1.651446   870.462214  22.344571  19.831299   \n",
       "\n",
       "         Chi0v       Chi1      Chi1n      Chi1v      Chi2n      Chi2v  \\\n",
       "0    19.534409  13.127794  12.204226  12.204226  12.058078  12.058078   \n",
       "1    19.794682  13.127794  12.595754  12.595754  12.648545  12.648545   \n",
       "2    23.689110  14.595754  14.249005  14.249005  15.671216  15.671216   \n",
       "3    20.241516  13.627794  12.704226  12.704226  12.411631  12.411631   \n",
       "4    22.617677  16.526773  13.868825  13.868825  13.613700  13.613700   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "993  18.825334  14.097861  11.665192  11.665192  11.409461  11.409461   \n",
       "994  21.810933  16.402391  13.274017  13.274017  12.636189  12.636189   \n",
       "995  24.449891  17.940396  14.301838  15.695685  13.248561  14.234160   \n",
       "996  23.380977  17.309003  13.174959  13.174959  11.893254  11.893254   \n",
       "997  19.831299  14.597861  12.464050  12.464050  12.101953  12.101953   \n",
       "\n",
       "         Chi3n      Chi3v      Chi4n      Chi4v  HallKierAlpha           Ipc  \\\n",
       "0    10.695991  10.695991   7.340247   7.340247          -0.66  2.187750e+06   \n",
       "1    11.473090  11.473090   8.180905   8.180905          -0.08  2.187750e+06   \n",
       "2    13.402236  13.402236  10.140303  10.140303          -0.08  8.610751e+06   \n",
       "3    10.945991  10.945991   7.517023   7.517023          -0.66  3.572142e+06   \n",
       "4    11.833480  11.833480   8.119076   8.119076          -2.22  1.053758e+08   \n",
       "..         ...        ...        ...        ...            ...           ...   \n",
       "993  10.058026  10.058026   8.981266   8.981266          -1.65  6.242348e+06   \n",
       "994  10.827369  10.827369   9.372775   9.372775          -2.18  5.897229e+07   \n",
       "995  11.326709  11.970659   9.725583  10.196987          -1.83  2.627956e+08   \n",
       "996  10.158570  10.158570   8.609327   8.609327          -2.45  7.702780e+07   \n",
       "997  10.667206  10.667206   9.563076   9.563076          -1.45  9.086032e+06   \n",
       "\n",
       "        Kappa1     Kappa2    Kappa3   LabuteASA  PEOE_VSA1  PEOE_VSA10  \\\n",
       "0    20.606247   6.947534  2.868737  173.630124   0.000000    0.000000   \n",
       "1    21.163454   7.257648  3.027177  174.939204  10.633577    0.000000   \n",
       "2    25.026112   7.709373  3.470070  201.238858   8.966062    0.000000   \n",
       "3    21.567454   7.485204  3.263848  179.995066   0.000000    0.000000   \n",
       "4    23.194917   7.639211  3.345855  211.919602   0.000000    0.000000   \n",
       "..         ...        ...       ...         ...        ...         ...   \n",
       "993  20.263719   6.198453  2.219273  178.490760   9.473726    0.000000   \n",
       "994  24.511583   7.908743  3.147136  207.296970  14.790515    6.041841   \n",
       "995  27.726151   9.668673  3.822745  230.149965  14.790515    6.041841   \n",
       "996  29.111081  10.369092  4.164473  218.836986  18.947452    5.783245   \n",
       "997  21.398566   6.776167  2.566599  186.107099   4.736863   11.566490   \n",
       "\n",
       "     PEOE_VSA11  PEOE_VSA12  PEOE_VSA13  PEOE_VSA14  PEOE_VSA2  PEOE_VSA3  \\\n",
       "0           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "1           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "2           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "3           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "4           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "..          ...         ...         ...         ...        ...        ...   \n",
       "993         0.0     0.00000         0.0   17.907916  14.383612   0.000000   \n",
       "994         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "995         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "996         0.0     0.00000         0.0   23.877221  23.972686   0.000000   \n",
       "997         0.0     0.00000         0.0    5.969305  14.383612   0.000000   \n",
       "\n",
       "     PEOE_VSA4  PEOE_VSA5  PEOE_VSA6   PEOE_VSA7  PEOE_VSA8  PEOE_VSA9  \\\n",
       "0     0.000000        0.0  54.384066   74.032366  35.342864   0.000000   \n",
       "1     0.000000        0.0  54.384066   97.951860  12.083682   0.000000   \n",
       "2     0.000000        0.0  41.542423   74.032366  23.671624  53.363882   \n",
       "3     0.000000        0.0  60.804888   74.032366  35.342864   0.000000   \n",
       "4     0.000000        0.0  65.807891  103.003916  22.253351  11.374773   \n",
       "..         ...        ...        ...         ...        ...        ...   \n",
       "993   0.000000        0.0  38.841158   68.114460   5.414990  24.360600   \n",
       "994   0.000000        0.0  45.764895   68.114460  10.829981  18.945610   \n",
       "995  11.761885        0.0  45.764895   79.620167  10.829981  18.945610   \n",
       "996   0.000000        0.0  27.192033   56.278648  11.835812  51.104983   \n",
       "997   0.000000        0.0  38.841158   68.114460  30.092446  12.524788   \n",
       "\n",
       "      SMR_VSA1  SMR_VSA10  SMR_VSA2   SMR_VSA3   SMR_VSA4    SMR_VSA5  \\\n",
       "0     0.000000  11.423370       0.0   0.000000  43.480583  105.750639   \n",
       "1     0.000000   0.000000       0.0  10.633577  33.495774  117.834321   \n",
       "2     8.966062   0.000000       0.0   0.000000  33.495774  117.834321   \n",
       "3     0.000000  11.423370       0.0   0.000000  43.480583  112.171461   \n",
       "4     0.000000  22.798143       0.0   0.000000  43.480583   86.488175   \n",
       "..         ...        ...       ...        ...        ...         ...   \n",
       "993  23.857337  17.907916       0.0   0.000000  51.752408   66.219879   \n",
       "994  28.651875  23.815096       0.0   5.316789  51.752408   79.185457   \n",
       "995  28.651875  35.576981       0.0   5.316789  51.752408   78.682541   \n",
       "996  42.920138  29.660466       0.0   0.000000  51.752408   66.219879   \n",
       "997  19.120475  17.535795       0.0   0.000000  51.752408   79.061522   \n",
       "\n",
       "      SMR_VSA6   SMR_VSA7  SMR_VSA8  SMR_VSA9  SlogP_VSA1  SlogP_VSA10  \\\n",
       "0    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "1    13.089513   0.000000         0       0.0   10.633577     0.000000   \n",
       "2    41.280201   0.000000         0       0.0    0.000000     0.000000   \n",
       "3    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "4     0.000000  59.657840         0       0.0    0.000000    11.374773   \n",
       "..         ...        ...       ...       ...         ...          ...   \n",
       "993   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "994   7.109798  11.649125         0       0.0    5.316789     0.000000   \n",
       "995  19.118420  11.649125         0       0.0    5.316789     0.000000   \n",
       "996  28.439190   0.000000         0       0.0    0.000000     0.000000   \n",
       "997   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "\n",
       "     SlogP_VSA11  SlogP_VSA12  SlogP_VSA2  SlogP_VSA3  SlogP_VSA4  SlogP_VSA5  \\\n",
       "0            0.0     0.000000   24.512883    0.000000   33.495774  105.750639   \n",
       "1            0.0     0.000000   25.173194    0.000000   33.495774  105.750639   \n",
       "2            0.0     0.000000   62.329944    0.000000   33.495774  105.750639   \n",
       "3            0.0     0.000000   24.512883    0.000000   33.495774  112.171461   \n",
       "4            0.0     0.000000   11.423370    6.420822   33.495774   91.194256   \n",
       "..           ...          ...         ...         ...         ...         ...   \n",
       "993          0.0     0.000000   25.017713   23.857337   51.752408   66.219879   \n",
       "994          0.0     0.000000   36.966734   28.651875   51.752408   73.143616   \n",
       "995          0.0    11.761885   48.975357   28.651875   51.752408   72.640700   \n",
       "996          0.0     0.000000   58.099656   42.920138   51.752408   66.219879   \n",
       "997          0.0     0.000000   24.645593   19.120475   51.752408   79.061522   \n",
       "\n",
       "     SlogP_VSA6  SlogP_VSA7  SlogP_VSA8  SlogP_VSA9    TPSA  EState_VSA1  \\\n",
       "0      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "1      0.000000         0.0         0.0           0   24.06     0.000000   \n",
       "2      0.000000         0.0         0.0           0    0.00     0.000000   \n",
       "3      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "4     58.515746         0.0         0.0           0   24.72     0.000000   \n",
       "..          ...         ...         ...         ...     ...          ...   \n",
       "993   11.649125         0.0         0.0           0   69.67     5.414990   \n",
       "994   11.649125         0.0         0.0           0   98.77    23.344043   \n",
       "995   11.649125         0.0         0.0           0   98.77    28.759033   \n",
       "996    0.000000         0.0         0.0           0  122.27    63.742418   \n",
       "997   11.649125         0.0         0.0           0   60.44     5.414990   \n",
       "\n",
       "     EState_VSA10  EState_VSA11  EState_VSA2  EState_VSA3  EState_VSA4  \\\n",
       "0        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "1        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "2        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "3        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "4        0.000000           0.0    10.829981    10.829981    29.631406   \n",
       "..            ...           ...          ...          ...          ...   \n",
       "993     14.383612           0.0    52.409521    11.835812    38.524930   \n",
       "994     19.178149           0.0    52.347395     5.917906    38.524930   \n",
       "995     19.178149           0.0    46.932405    12.338728    44.277783   \n",
       "996     23.972686           0.0    30.512100    19.262465     6.420822   \n",
       "997     14.383612           0.0    40.470910    36.243945    38.524930   \n",
       "\n",
       "     EState_VSA5  EState_VSA6  EState_VSA7  EState_VSA8  EState_VSA9  \\\n",
       "0      64.208216    11.423370     0.000000    41.542423     9.984809   \n",
       "1      77.297729     0.000000     0.000000    52.176000     0.000000   \n",
       "2      86.263791     0.000000     0.000000    69.733111     0.000000   \n",
       "3      70.629038    11.423370     0.000000    41.542423     9.984809   \n",
       "4      61.075203     0.000000     0.000000    90.073360     9.984809   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    12.682902     0.000000     0.000000    33.770969     9.473726   \n",
       "994    12.682902     6.923737     0.000000    39.087758     9.473726   \n",
       "995    12.682902    11.761885     6.255769    39.087758     9.473726   \n",
       "996    28.439190    13.847474     6.923737     6.923737    18.947452   \n",
       "997    12.682902     0.000000     0.000000    33.770969     4.736863   \n",
       "\n",
       "     VSA_EState1  VSA_EState10  VSA_EState2  VSA_EState3  VSA_EState4  \\\n",
       "0       0.000000      0.000000    10.188192     0.000000     4.807589   \n",
       "1       0.000000      0.000000     0.000000     7.922833     2.153503   \n",
       "2       2.517630      0.000000     0.000000     0.000000     2.184127   \n",
       "3       0.000000      0.000000    10.194720     0.000000     4.827852   \n",
       "4       0.000000      0.000000    10.301020     0.000000     9.071783   \n",
       "..           ...           ...          ...          ...          ...   \n",
       "993    10.503509      0.000000    38.515343     0.000000     0.498752   \n",
       "994    10.086396      0.000000    51.516353     2.923049     0.162524   \n",
       "995    10.305031      1.639399    52.527620     3.081660     0.139799   \n",
       "996    20.885832      0.000000    67.303382     0.000000    -2.785593   \n",
       "997     5.298868      0.000000    39.472742     0.000000     0.776999   \n",
       "\n",
       "     VSA_EState5  VSA_EState6  VSA_EState7  VSA_EState8  VSA_EState9  \\\n",
       "0       1.764908     0.000000    13.258223    16.981087     0.000000   \n",
       "1       1.914377     1.536674    14.135381    17.670565     0.000000   \n",
       "2       1.930720     1.738402    14.491619    18.287216    10.183618   \n",
       "3       1.769975     0.000000    14.695439    17.012013     0.000000   \n",
       "4       1.605178    17.869058     8.627311    14.692318     0.000000   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    -0.405436     0.000000     7.985276     8.824371     1.494852   \n",
       "994    -1.300354    -0.699355     7.521836    10.378794     1.327425   \n",
       "995    -0.487671    -0.650790    10.055493     8.774745     1.364715   \n",
       "996    -6.848660     0.000000     2.955837     7.488627     5.083909   \n",
       "997     1.605640     0.000000     9.247616     8.999949     1.514853   \n",
       "\n",
       "     FractionCSP3  HeavyAtomCount  NHOHCount  NOCount  \\\n",
       "0        0.923077              28          0        2   \n",
       "1        1.000000              28          2        2   \n",
       "2        1.000000              32          0        2   \n",
       "3        0.925926              29          0        2   \n",
       "4        0.575758              35          0        2   \n",
       "..            ...             ...        ...      ...   \n",
       "993      0.800000              30          0        5   \n",
       "994      0.785714              35          1        7   \n",
       "995      0.800000              38          1        7   \n",
       "996      0.821429              37          0        9   \n",
       "997      0.814815              31          0        4   \n",
       "\n",
       "     NumAliphaticCarbocycles  NumAliphaticHeterocycles  NumAliphaticRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        5                         1                  6   \n",
       "994                        5                         1                  6   \n",
       "995                        5                         1                  6   \n",
       "996                        3                         0                  3   \n",
       "997                        6                         0                  6   \n",
       "\n",
       "     NumAromaticCarbocycles  NumAromaticHeterocycles  NumAromaticRings  \\\n",
       "0                         0                        0                 0   \n",
       "1                         0                        0                 0   \n",
       "2                         0                        0                 0   \n",
       "3                         0                        0                 0   \n",
       "4                         2                        0                 2   \n",
       "..                      ...                      ...               ...   \n",
       "993                       0                        0                 0   \n",
       "994                       0                        0                 0   \n",
       "995                       0                        0                 0   \n",
       "996                       0                        0                 0   \n",
       "997                       0                        0                 0   \n",
       "\n",
       "     NumHAcceptors  NumHDonors  NumHeteroatoms  NumRotatableBonds  \\\n",
       "0                2           0               2                  7   \n",
       "1                2           2               2                  9   \n",
       "2                0           0               2                  9   \n",
       "3                2           0               2                  8   \n",
       "4                2           0               2                  4   \n",
       "..             ...         ...             ...                ...   \n",
       "993              5           0               5                  2   \n",
       "994              6           1               7                  4   \n",
       "995              7           1               8                  7   \n",
       "996              9           0               9                  6   \n",
       "997              4           0               4                  2   \n",
       "\n",
       "     NumSaturatedCarbocycles  NumSaturatedHeterocycles  NumSaturatedRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        3                         1                  4   \n",
       "994                        3                         1                  4   \n",
       "995                        3                         1                  4   \n",
       "996                        3                         0                  3   \n",
       "997                        4                         0                  4   \n",
       "\n",
       "     RingCount  MolLogP     MolMR  fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  \\\n",
       "0            4   7.1212  121.5300          0         0                0   \n",
       "1            4   6.1556  120.5074          0         0                0   \n",
       "2            4   7.1292  138.4528          0         0                0   \n",
       "3            4   7.5113  126.1470          0         0                0   \n",
       "4            6   9.1148  148.3380          0         0                0   \n",
       "..         ...      ...       ...        ...       ...              ...   \n",
       "993          6   4.3002  109.8350          0         0                0   \n",
       "994          6   3.8049  127.4397          0         0                0   \n",
       "995          6   4.5381  144.7647          0         0                0   \n",
       "996          3   3.3649  131.7080          0         0                0   \n",
       "997          6   5.1488  117.9840          0         0                0   \n",
       "\n",
       "     fr_ArN  fr_Ar_COO  fr_Ar_N  fr_Ar_NH  fr_Ar_OH  fr_COO  fr_COO2  fr_C_O  \\\n",
       "0         0          0        0         0         0       0        0       0   \n",
       "1         0          0        0         0         0       0        0       0   \n",
       "2         0          0        0         0         0       0        0       0   \n",
       "3         0          0        0         0         0       0        0       0   \n",
       "4         0          0        0         0         0       0        0       0   \n",
       "..      ...        ...      ...       ...       ...     ...      ...     ...   \n",
       "993       0          0        0         0         0       0        0       3   \n",
       "994       0          0        0         0         0       0        0       4   \n",
       "995       0          0        0         0         0       0        0       4   \n",
       "996       0          0        0         0         0       0        0       5   \n",
       "997       0          0        0         0         0       0        0       3   \n",
       "\n",
       "     fr_C_O_noCOO  fr_C_S  fr_HOCCN  fr_Imine  fr_NH0  fr_NH1  fr_NH2  fr_N_O  \\\n",
       "0               0       0         0         2       2       0       0       0   \n",
       "1               0       0         0         0       0       2       0       0   \n",
       "2               0       0         0         0       2       0       0       0   \n",
       "3               0       0         0         2       2       0       0       0   \n",
       "4               0       0         0         2       2       0       0       0   \n",
       "..            ...     ...       ...       ...     ...     ...     ...     ...   \n",
       "993             3       0         0         0       0       0       0       0   \n",
       "994             4       0         0         0       0       1       0       0   \n",
       "995             4       0         0         0       0       1       0       0   \n",
       "996             5       0         0         0       0       0       0       0   \n",
       "997             3       0         0         0       0       0       0       0   \n",
       "\n",
       "     fr_Ndealkylation1  fr_Ndealkylation2  fr_Nhpyrrole  fr_SH  fr_aldehyde  \\\n",
       "0                    0                  0             0      0            0   \n",
       "1                    0                  0             0      0            0   \n",
       "2                    0                  0             0      0            0   \n",
       "3                    0                  0             0      0            0   \n",
       "4                    0                  0             0      0            0   \n",
       "..                 ...                ...           ...    ...          ...   \n",
       "993                  0                  0             0      0            0   \n",
       "994                  0                  0             0      0            0   \n",
       "995                  0                  0             0      0            0   \n",
       "996                  0                  0             0      0            0   \n",
       "997                  0                  0             0      0            0   \n",
       "\n",
       "     fr_alkyl_carbamate  fr_alkyl_halide  fr_allylic_oxid  fr_amide  \\\n",
       "0                     0                0                0         0   \n",
       "1                     0                0                0         0   \n",
       "2                     0                0                0         0   \n",
       "3                     0                0                0         0   \n",
       "4                     0                0                0         0   \n",
       "..                  ...              ...              ...       ...   \n",
       "993                   0                0                2         0   \n",
       "994                   0                0                2         1   \n",
       "995                   0                0                2         1   \n",
       "996                   0                0                0         0   \n",
       "997                   0                0                2         0   \n",
       "\n",
       "     fr_amidine  fr_aniline  fr_aryl_methyl  fr_azide  fr_azo  fr_barbitur  \\\n",
       "0             0           0               0         0       0            0   \n",
       "1             0           0               0         0       0            0   \n",
       "2             0           0               0         0       0            0   \n",
       "3             0           0               0         0       0            0   \n",
       "4             0           0               0         0       0            0   \n",
       "..          ...         ...             ...       ...     ...          ...   \n",
       "993           0           0               0         0       0            0   \n",
       "994           0           0               0         0       0            0   \n",
       "995           0           0               0         0       0            0   \n",
       "996           0           0               0         0       0            0   \n",
       "997           0           0               0         0       0            0   \n",
       "\n",
       "     fr_benzene  fr_benzodiazepine  fr_bicyclic  fr_diazo  fr_dihydropyridine  \\\n",
       "0             0                  0            4         0                   0   \n",
       "1             0                  0            4         0                   0   \n",
       "2             0                  0            4         0                   0   \n",
       "3             0                  0            4         0                   0   \n",
       "4             2                  0            4         0                   0   \n",
       "..          ...                ...          ...       ...                 ...   \n",
       "993           0                  0            1         0                   0   \n",
       "994           0                  0            1         0                   0   \n",
       "995           0                  0            1         0                   0   \n",
       "996           0                  0            3         0                   0   \n",
       "997           0                  0            1         0                   0   \n",
       "\n",
       "     fr_epoxide  fr_ester  fr_ether  fr_furan  fr_guanido  fr_halogen  \\\n",
       "0             0         0         0         0           0           0   \n",
       "1             0         0         0         0           0           0   \n",
       "2             0         0         0         0           0           0   \n",
       "3             0         0         0         0           0           0   \n",
       "4             0         0         0         0           0           0   \n",
       "..          ...       ...       ...       ...         ...         ...   \n",
       "993           0         3         2         0           0           0   \n",
       "994           0         3         2         0           0           0   \n",
       "995           0         3         2         0           0           0   \n",
       "996           0         4         4         0           0           0   \n",
       "997           0         1         1         0           0           0   \n",
       "\n",
       "     fr_hdrzine  fr_hdrzone  fr_imidazole  fr_imide  fr_isocyan  \\\n",
       "0             0           0             0         0           0   \n",
       "1             0           0             0         0           0   \n",
       "2             0           0             0         0           0   \n",
       "3             0           0             0         0           0   \n",
       "4             0           0             0         0           0   \n",
       "..          ...         ...           ...       ...         ...   \n",
       "993           0           0             0         0           0   \n",
       "994           0           0             0         0           0   \n",
       "995           0           0             0         0           0   \n",
       "996           0           0             0         0           0   \n",
       "997           0           0             0         0           0   \n",
       "\n",
       "     fr_isothiocyan  fr_ketone  fr_ketone_Topliss  fr_lactam  fr_lactone  \\\n",
       "0                 0          0                  0          0           0   \n",
       "1                 0          0                  0          0           0   \n",
       "2                 0          0                  0          0           0   \n",
       "3                 0          0                  0          0           0   \n",
       "4                 0          0                  0          0           0   \n",
       "..              ...        ...                ...        ...         ...   \n",
       "993               0          0                  0          0           2   \n",
       "994               0          0                  0          0           2   \n",
       "995               0          0                  0          0           2   \n",
       "996               0          1                  1          0           0   \n",
       "997               0          2                  2          0           0   \n",
       "\n",
       "     fr_methoxy  fr_morpholine  fr_nitrile  fr_nitro  fr_nitro_arom  \\\n",
       "0             0              0           0         0              0   \n",
       "1             0              0           0         0              0   \n",
       "2             0              0           0         0              0   \n",
       "3             0              0           0         0              0   \n",
       "4             0              0           0         0              0   \n",
       "..          ...            ...         ...       ...            ...   \n",
       "993           1              0           0         0              0   \n",
       "994           1              0           0         0              0   \n",
       "995           1              0           0         0              0   \n",
       "996           4              0           0         0              0   \n",
       "997           1              0           0         0              0   \n",
       "\n",
       "     fr_nitro_arom_nonortho  fr_nitroso  fr_oxazole  fr_oxime  \\\n",
       "0                         0           0           0         0   \n",
       "1                         0           0           0         0   \n",
       "2                         0           0           0         0   \n",
       "3                         0           0           0         0   \n",
       "4                         0           0           0         0   \n",
       "..                      ...         ...         ...       ...   \n",
       "993                       0           0           0         0   \n",
       "994                       0           0           0         0   \n",
       "995                       0           0           0         0   \n",
       "996                       0           0           0         0   \n",
       "997                       0           0           0         0   \n",
       "\n",
       "     fr_para_hydroxylation  fr_phenol  fr_phenol_noOrthoHbond  fr_phos_acid  \\\n",
       "0                        0          0                       0             0   \n",
       "1                        0          0                       0             0   \n",
       "2                        0          0                       0             0   \n",
       "3                        0          0                       0             0   \n",
       "4                        0          0                       0             0   \n",
       "..                     ...        ...                     ...           ...   \n",
       "993                      0          0                       0             0   \n",
       "994                      0          0                       0             0   \n",
       "995                      0          0                       0             0   \n",
       "996                      0          0                       0             0   \n",
       "997                      0          0                       0             0   \n",
       "\n",
       "     fr_phos_ester  fr_piperdine  fr_piperzine  fr_priamide  fr_prisulfonamd  \\\n",
       "0                0             0             0            0                0   \n",
       "1                0             0             0            0                0   \n",
       "2                0             0             0            0                0   \n",
       "3                0             0             0            0                0   \n",
       "4                0             0             0            0                0   \n",
       "..             ...           ...           ...          ...              ...   \n",
       "993              0             0             0            0                0   \n",
       "994              0             0             0            0                0   \n",
       "995              0             0             0            0                0   \n",
       "996              0             0             0            0                0   \n",
       "997              0             0             0            0                0   \n",
       "\n",
       "     fr_pyridine  fr_quatN  fr_sulfide  fr_sulfonamd  fr_sulfone  \\\n",
       "0              0         0           0             0           0   \n",
       "1              0         0           0             0           0   \n",
       "2              0         2           0             0           0   \n",
       "3              0         0           0             0           0   \n",
       "4              0         0           0             0           0   \n",
       "..           ...       ...         ...           ...         ...   \n",
       "993            0         0           0             0           0   \n",
       "994            0         0           0             0           0   \n",
       "995            0         0           1             0           0   \n",
       "996            0         0           0             0           0   \n",
       "997            0         0           0             0           0   \n",
       "\n",
       "     fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  \\\n",
       "0                    0             0            0            0             0   \n",
       "1                    0             0            0            0             0   \n",
       "2                    0             0            0            0             0   \n",
       "3                    0             0            0            0             0   \n",
       "4                    0             0            0            0             0   \n",
       "..                 ...           ...          ...          ...           ...   \n",
       "993                  0             0            0            0             0   \n",
       "994                  0             0            0            0             0   \n",
       "995                  0             0            0            0             0   \n",
       "996                  0             0            0            0             0   \n",
       "997                  0             0            0            0             0   \n",
       "\n",
       "     fr_unbrch_alkane  fr_urea  \n",
       "0                   3        0  \n",
       "1                   3        0  \n",
       "2                   3        0  \n",
       "3                   4        0  \n",
       "4                   0        0  \n",
       "..                ...      ...  \n",
       "993                 0        0  \n",
       "994                 0        0  \n",
       "995                 0        0  \n",
       "996                 0        0  \n",
       "997                 0        0  \n",
       "\n",
       "[998 rows x 214 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "94a946941faca937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:13.720652Z",
     "start_time": "2025-05-28T16:57:13.716638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем немнформативный признак\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f22040ef150ffc8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.281372Z",
     "start_time": "2025-05-28T16:57:14.276214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем пропуски\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4e0bb2972cbaa906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.915613Z",
     "start_time": "2025-05-28T16:57:14.885015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем дубликаты\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7089b7fa5f494e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:16.413719Z",
     "start_time": "2025-05-28T16:57:16.409041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(966, 213)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4529ddf3083cdcc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:00:51.840374Z",
     "start_time": "2025-05-28T17:00:51.836087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вычисляем медиану столбца\n",
    "median_value_ic50 = df['IC50, mM'].median()\n",
    "median_value_cc50 = df['CC50, mM'].median()\n",
    "median_value_si = df['SI'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "400a82221f828fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:21.052804Z",
     "start_time": "2025-05-28T17:02:21.047421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем целевые признаки\n",
    "df['SI'] = (df['SI'] > 8).astype(int)\n",
    "df['IC50, mM'] = (df['IC50, mM'] > median_value_ic50).astype(int)\n",
    "df['CC50, mM'] = (df['CC50, mM'] > median_value_cc50).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "502ea21feffa4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:05:44.334975Z",
     "start_time": "2025-05-28T17:05:44.331153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем данные и целевую переменную\n",
    "X = df.drop(columns = ['IC50, mM', 'CC50, mM', 'SI'])\n",
    "y = df['SI']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0ae82771-5320-4ecd-a016-d937810c06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество выбросов по признакам:\n",
      "IC50, mM              0\n",
      "CC50, mM              0\n",
      "SI                    0\n",
      "MaxAbsEStateIndex    60\n",
      "MaxEStateIndex       60\n",
      "                     ..\n",
      "fr_thiazole          52\n",
      "fr_thiocyan           0\n",
      "fr_thiophene         68\n",
      "fr_unbrch_alkane     49\n",
      "fr_urea               7\n",
      "Length: 213, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers(df, alpha=0.05, method='iqr', normality_test='shapiro', add_sum_column=False):\n",
    "    \"\"\"\n",
    "    Обнаружение выбросов в DataFrame с использованием различных статистических методов.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Входной DataFrame с данными для анализа\n",
    "    alpha : float, по умолчанию 0.05\n",
    "        Уровень значимости для тестов на нормальность\n",
    "    method : str, по умолчанию 'iqr'\n",
    "        Метод обнаружения выбросов для ненормальных данных:\n",
    "        - 'iqr' - метод межквартильного размаха\n",
    "        - 'zscore' - модифицированный Z-score\n",
    "    normality_test : str, по умолчанию 'shapiro'\n",
    "        Тест на нормальность распределения:\n",
    "        - 'shapiro' - тест Шапиро-Уилка\n",
    "        - 'normaltest' - тест на нормальность D'Agostino-Pearson\n",
    "        - 'anderson' - тест Андерсона-Дарлинга\n",
    "    add_sum_column : bool, по умолчанию False\n",
    "        Если True, добавляет столбец с общим количеством выбросов для каждой строки\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        DataFrame с булевыми значениями, где True указывает на выброс\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем DataFrame для хранения результатов (по умолчанию все значения False)\n",
    "    outliers = pd.DataFrame(False, index=df.index, columns=df.columns)\n",
    "    \n",
    "    # Анализируем каждый столбец отдельно\n",
    "    for col in df.columns:\n",
    "        # Удаляем пропущенные значения для текущего столбца\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Если в столбце меньше 3 значений, пропускаем его\n",
    "        if len(data) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Проверяем нормальность распределения\n",
    "        normal = False  # Флаг нормальности распределения\n",
    "        \n",
    "        try:\n",
    "            # Выбираем тест на нормальность в зависимости от параметра normality_test\n",
    "            if normality_test == 'shapiro':\n",
    "                # Тест Шапиро-Уилка (подходит для небольших выборок < 5000)\n",
    "                _, p = stats.shapiro(data)\n",
    "                normal = p > alpha  # Если p-value > alpha, распределение считается нормальным\n",
    "                \n",
    "            elif normality_test == 'normaltest':\n",
    "                # Тест D'Agostino-Pearson (работает для выборок > 20)\n",
    "                _, p = stats.normaltest(data)\n",
    "                normal = p > alpha\n",
    "                \n",
    "            elif normality_test == 'anderson':\n",
    "                # Тест Андерсона-Дарлинга (более строгий)\n",
    "                result = stats.anderson(data)\n",
    "                # Сравниваем статистику с критическим значением для выбранного alpha\n",
    "                normal = result.statistic < result.critical_values[np.where(result.significance_level == int(alpha*100))[0][0]]\n",
    "        except:\n",
    "            # В случае ошибки в тесте считаем распределение ненормальным\n",
    "            pass\n",
    "        \n",
    "        # Если распределение нормальное, используем стандартный Z-score\n",
    "        if normal:\n",
    "            z = np.abs(stats.zscore(data))  # Вычисляем Z-оценки\n",
    "            outliers.loc[data.index, col] = z > 3  # Выбросы > 3 стандартных отклонений\n",
    "            \n",
    "        # Для ненормальных распределений используем выбранный метод\n",
    "        else:\n",
    "            if method == 'iqr':\n",
    "                # Метод межквартильного размаха (IQR)\n",
    "                q1 = data.quantile(0.25)  # Первый квартиль (25-й перцентиль)\n",
    "                q3 = data.quantile(0.75)  # Третий квартиль (75-й перцентиль)\n",
    "                iqr = q3 - q1  # Межквартильный размах\n",
    "                \n",
    "                # Границы для выбросов\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                # Отмечаем выбросы\n",
    "                outliers.loc[data.index, col] = (data < lower_bound) | (data > upper_bound)\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                # Модифицированный Z-score (более устойчивый к выбросам)\n",
    "                median = data.median()  # Медиана вместо среднего\n",
    "                mad = stats.median_abs_deviation(data, scale='normal')  # Медианное абсолютное отклонение\n",
    "                modified_z = np.abs(0.6745 * (data - median) / mad)  # Модифицированный Z-score\n",
    "                \n",
    "                # Выбросы при modified_z > 3.5\n",
    "                outliers.loc[data.index, col] = modified_z > 3.5\n",
    "    \n",
    "    # Добавляем столбец с суммой выбросов по строкам, если нужно\n",
    "    if add_sum_column:\n",
    "        outliers['outliers_sum'] = outliers.sum(axis=1)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Находим выбросы\n",
    "outliers = detect_outliers(df)\n",
    "\n",
    "# Выводим количество выбросов по каждому признаку\n",
    "print(\"\\nКоличество выбросов по признакам:\")\n",
    "print(outliers.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a0ea30522688a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:06:25.366761Z",
     "start_time": "2025-05-28T17:06:15.792777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Обучение Logistic Regression...\n",
      " Logistic Regression\n",
      "Accuracy: 0.7113 | Precision: 0.7079 | Recall: 0.7113 | F1: 0.7093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       125\n",
      "           1       0.60      0.57      0.58        69\n",
      "\n",
      "    accuracy                           0.71       194\n",
      "   macro avg       0.68      0.68      0.68       194\n",
      "weighted avg       0.71      0.71      0.71       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение SVM...\n",
      " SVM\n",
      "Accuracy: 0.7062 | Precision: 0.6945 | Recall: 0.7062 | F1: 0.6912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.86      0.79       125\n",
      "           1       0.62      0.43      0.51        69\n",
      "\n",
      "    accuracy                           0.71       194\n",
      "   macro avg       0.68      0.65      0.65       194\n",
      "weighted avg       0.69      0.71      0.69       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение KNN...\n",
      " KNN\n",
      "Accuracy: 0.6495 | Precision: 0.6495 | Recall: 0.6495 | F1: 0.6495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73       125\n",
      "           1       0.51      0.51      0.51        69\n",
      "\n",
      "    accuracy                           0.65       194\n",
      "   macro avg       0.62      0.62      0.62       194\n",
      "weighted avg       0.65      0.65      0.65       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение Random Forest...\n",
      " Random Forest\n",
      "Accuracy: 0.7010 | Precision: 0.6944 | Recall: 0.7010 | F1: 0.6965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.78       125\n",
      "           1       0.59      0.52      0.55        69\n",
      "\n",
      "    accuracy                           0.70       194\n",
      "   macro avg       0.67      0.66      0.66       194\n",
      "weighted avg       0.69      0.70      0.70       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение XGBoost...\n",
      " XGBoost\n",
      "Accuracy: 0.6907 | Precision: 0.6888 | Recall: 0.6907 | F1: 0.6897\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.76       125\n",
      "           1       0.57      0.55      0.56        69\n",
      "\n",
      "    accuracy                           0.69       194\n",
      "   macro avg       0.66      0.66      0.66       194\n",
      "weighted avg       0.69      0.69      0.69       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение CatBoost...\n",
      " CatBoost\n",
      "Accuracy: 0.7010 | Precision: 0.6893 | Recall: 0.7010 | F1: 0.6886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.84      0.78       125\n",
      "           1       0.61      0.45      0.52        69\n",
      "\n",
      "    accuracy                           0.70       194\n",
      "   macro avg       0.67      0.64      0.65       194\n",
      "weighted avg       0.69      0.70      0.69       194\n",
      "\n",
      "──────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def test_classification_models_with_scaling(X, y, test_size=0.2, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Тестирует классификационные модели со стандартизацией данных и возвращает результаты.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame или np.array\n",
    "        Матрица признаков\n",
    "    y : pd.Series или np.array\n",
    "        Целевая переменная\n",
    "    test_size : float, optional\n",
    "        Размер тестовой выборки (по умолчанию 0.2)\n",
    "    random_state : int, optional\n",
    "        Seed для воспроизводимости (по умолчанию 42)\n",
    "    verbose : bool, optional\n",
    "        Выводить ли прогресс (по умолчанию True)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Таблица с результатами метрик и временем обучения для каждой модели\n",
    "    \"\"\"\n",
    "\n",
    "    # Разделение данных\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Инициализация моделей с пайплайнами (StandardScaler + модель)\n",
    "    models = {\n",
    "        'Logistic Regression': make_pipeline(StandardScaler(),\n",
    "                                           LogisticRegression(random_state=random_state, max_iter=1000)),\n",
    "        'SVM': make_pipeline(StandardScaler(),\n",
    "                            SVC(random_state=random_state)),\n",
    "        'KNN': make_pipeline(StandardScaler(),\n",
    "                            KNeighborsClassifier()),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(random_state=random_state, n_jobs=-1, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(random_state=random_state, verbose=False)\n",
    "    }\n",
    "\n",
    "    # Словарь для хранения результатов\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Train Time (s)': [],\n",
    "        'Scaler Used': []\n",
    "    }\n",
    "\n",
    "    # Обучение и оценка моделей\n",
    "    for name, model in models.items():\n",
    "        if verbose:\n",
    "            print(f\"🔍 Обучение {name}...\")\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Обучение модели\n",
    "            model.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            # Предсказание\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Расчет метрик\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            # Определение использования StandardScaler\n",
    "            scaler_used = 'StandardScaler' if 'standardscaler' in str(model).lower() else 'No'\n",
    "\n",
    "            # Сохранение результатов\n",
    "            results['Model'].append(name)\n",
    "            results['Accuracy'].append(accuracy)\n",
    "            results['Precision'].append(precision)\n",
    "            results['Recall'].append(recall)\n",
    "            results['F1 Score'].append(f1)\n",
    "            results['Train Time (s)'].append(train_time)\n",
    "            results['Scaler Used'].append(scaler_used)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\" {name}\\nAccuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                print(\"─\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Ошибка в {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создание DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results).sort_values('F1 Score', ascending=False)\n",
    "\n",
    "    # Визуализация результатов\n",
    "    if verbose:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "        x = np.arange(len(results_df['Model']))\n",
    "        width = 0.2\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "        plt.title('Сравнение моделей классификации', pad=20)\n",
    "        plt.xlabel('Модели')\n",
    "        plt.ylabel('Оценка')\n",
    "        plt.xticks(x + width*1.5, results_df['Model'], rotation=45, ha='right')\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Запуск расчета\n",
    "results = test_classification_models_with_scaling(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294a5ea4c764740",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-28T17:09:08.860255Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Initial F1: 0.7093, Accuracy: 0.7113 with 210 features\n",
      "Best F1: 0.7232, Accuracy: 0.7268 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== SVM ===\n",
      "Initial F1: 0.6912, Accuracy: 0.7062 with 210 features\n",
      "Best F1: 0.7283, Accuracy: 0.7423 with 206 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== KNN ===\n",
      "Initial F1: 0.6495, Accuracy: 0.6495 with 210 features\n",
      "Best F1: 0.7043, Accuracy: 0.7010 with 205 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane']\n",
      "\n",
      "=== RandomForest ===\n",
      "Initial F1: 0.6965, Accuracy: 0.7010 with 210 features\n",
      "Best F1: 0.7174, Accuracy: 0.7216 with 209 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== XGBoost ===\n",
      "Initial F1: 0.6897, Accuracy: 0.6907 with 210 features\n",
      "Best F1: 0.7405, Accuracy: 0.7423 with 205 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== CatBoost ===\n",
      "Initial F1: 0.6886, Accuracy: 0.7010 with 210 features\n",
      "Best F1: 0.7056, Accuracy: 0.7165 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации\n",
    "\n",
    "def evaluate_model(X, y, model, test_size=0.2, random_state=42):\n",
    "    \"\"\"Функция оценки модели классификации с добавлением StandardScaler\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Для моделей, чувствительных к масштабу, используем Pipeline со StandardScaler\n",
    "    if isinstance(model, (LogisticRegression, SVC, KNeighborsClassifier)):\n",
    "        model = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "def find_best_feature_subset(X, y, model, model_name):\n",
    "    \"\"\"Модифицированная версия для классификации с указанием имени модели\"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    current_features = X.columns.tolist()\n",
    "    initial_scores = evaluate_model(X[current_features], y, model)\n",
    "    best_f1 = initial_scores['f1']\n",
    "    best_accuracy = initial_scores['accuracy']\n",
    "    best_features = current_features.copy()\n",
    "    history = []\n",
    "\n",
    "    history.append({\n",
    "        'features': current_features.copy(),\n",
    "        'f1': best_f1,\n",
    "        'accuracy': best_accuracy,\n",
    "        'action': 'initial'\n",
    "    })\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Initial F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(current_features)} features\")\n",
    "\n",
    "    improved = True\n",
    "    while improved and len(current_features) > 1:\n",
    "        improved = False\n",
    "        worst_feature = None\n",
    "\n",
    "        for feature in current_features:\n",
    "            trial_features = [f for f in current_features if f != feature]\n",
    "            current_scores = evaluate_model(X[trial_features], y, model)\n",
    "\n",
    "            history.append({\n",
    "                'features': trial_features.copy(),\n",
    "                'f1': current_scores['f1'],\n",
    "                'accuracy': current_scores['accuracy'],\n",
    "                'action': f'removed {feature}'\n",
    "            })\n",
    "\n",
    "            if current_scores['f1'] > best_f1:\n",
    "                best_f1 = current_scores['f1']\n",
    "                best_accuracy = current_scores['accuracy']\n",
    "                best_features = trial_features.copy()\n",
    "                worst_feature = feature\n",
    "                improved = True\n",
    "\n",
    "        if improved:\n",
    "            current_features.remove(worst_feature)\n",
    "\n",
    "    print(f\"Best F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(best_features)} features\")\n",
    "    print(\"Optimal features:\", best_features)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_f1,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей по очереди\"\"\"\n",
    "    # Создаем модели с дефолтными параметрами\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = find_best_feature_subset(X, y, model, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создаем DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Запуск расчета:\n",
    "results_col_combination = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "53e9f1a111a073db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: {'accuracy': 0.7525773195876289, 'f1': 0.7391208883126659, 'roc_auc': 0.6892753623188406}\n",
      "Selected features: 89 (Removed 121 outliers)\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: {'accuracy': 0.7422680412371134, 'f1': 0.7358973494174564, 'roc_auc': 0.689159420289855}\n",
      "Selected features: 44 (Removed 166 outliers)\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: {'accuracy': 0.6958762886597938, 'f1': 0.6943002413195152, 'roc_auc': 0.6894492753623188}\n",
      "Selected features: 32 (Removed 178 outliers)\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: {'accuracy': 0.7164948453608248, 'f1': 0.711530895899153, 'roc_auc': 0.7195362318840579}\n",
      "Selected features: 170 (Removed 40 outliers)\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: {'accuracy': 0.7164948453608248, 'f1': 0.711530895899153, 'roc_auc': 0.7284637681159419}\n",
      "Selected features: 151 (Removed 59 outliers)\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: {'accuracy': 0.7216494845360825, 'f1': 0.7147691373708529, 'roc_auc': 0.7211014492753623}\n",
      "Selected features: 31 (Removed 179 outliers)\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации, через последовательное удаление признаков, упорядоченных по количеству возможных выбросов\n",
    "outliers_count = pd.DataFrame({\n",
    "        'feature': df.columns,\n",
    "        'outliers': outliers.sum(axis=0)\n",
    "    }).sort_values('outliers', ascending=False)\n",
    "all_features_outliers = outliers_count['feature'][outliers_count['outliers']>0].tolist()\n",
    "\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = get_best_features(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_best_features(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с удалением выбросов для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    removed = []\n",
    "    # Перебираем количество удаляемых признаков с выбросами\n",
    "    for n in range(1, len(all_features_outliers)+1):\n",
    "        \n",
    "        current_features = X.drop(columns = all_features_outliers[:n]).columns.tolist()\n",
    "\n",
    "        metrics = evaluate_metrics(X[current_features], y, model_name)\n",
    "\n",
    "        if metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = metrics\n",
    "            best_features = current_features.copy()\n",
    "            removed = all_features_outliers[:n]\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: {best_metrics}\")\n",
    "    print(f\"Selected features: {len(best_features)} (Removed {len(removed)} outliers)\")\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'outliers'\n",
    "    }\n",
    "   \n",
    "\n",
    "def evaluate_metrics(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        metrics['roc_auc'] = None\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_2 = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8277aa7f99158c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7216, F1=0.7174, ROC-AUC=0.7188405797101449\n",
      "Optimal features (15): ['BCUT2D_MRLOW', 'FractionCSP3', 'BCUT2D_LOGPHI', 'SMR_VSA7', 'VSA_EState8']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7165, F1=0.7160, ROC-AUC=0.744\n",
      "Optimal features (18): ['FractionCSP3', 'BCUT2D_MRLOW', 'VSA_EState5', 'BCUT2D_LOGPHI', 'BCUT2D_CHGLO']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7371, F1=0.7270, ROC-AUC=0.7642898550724638\n",
      "Optimal features (18): ['BCUT2D_MRLOW', 'RingCount', 'NumSaturatedCarbocycles', 'SlogP_VSA6', 'Chi1']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью SHAP\n",
    "\n",
    "def test_tree_models_sh(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с SHAP-анализом\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_shap_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_shap_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с помощью SHAP для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostRegressor(\n",
    "        iterations=100,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "   \n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': importance['feature'],\n",
    "        'importance': importance['mean_abs_shap']\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'][feat_importance['importance']>0].tolist()\n",
    "    \n",
    "    \n",
    "    # Находим оптимальное количество признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "\n",
    "    for n in range(1, min(20, len(all_features)+1)):  # Ограничиваем до 20 признаков для скорости\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'shap'\n",
    "    }\n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_significant_shap_features(shap_values, feature_names, threshold=0):\n",
    "    \"\"\"\n",
    "    Возвращает отсортированный по убыванию список признаков с SHAP-значимостью\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    shap_values : shap.Explanation или np.ndarray\n",
    "        SHAP значения для всех наблюдений\n",
    "    feature_names : list или pd.Index\n",
    "        Список названий признаков\n",
    "    threshold : float, optional\n",
    "        Порог значимости (по умолчанию 0)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame: DataFrame с колонками 'feature' и 'mean_abs_shap',\n",
    "                  отсортированный по убыванию важности\n",
    "    \"\"\"\n",
    "    if isinstance(shap_values, shap.Explanation):\n",
    "        shap_array = shap_values.values\n",
    "    else:\n",
    "        shap_array = shap_values\n",
    "\n",
    "    # Для многоклассовой классификации берем среднее по всем классам\n",
    "    if len(shap_array.shape) == 3:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=(0, 1))\n",
    "    else:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=0)\n",
    "\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': mean_abs_shap\n",
    "    })\n",
    "\n",
    "    significant_features = shap_importance[shap_importance['mean_abs_shap'] > threshold] \\\n",
    "        .sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    return significant_features.reset_index(drop=True)\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_3 = test_tree_models_sh(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13843390031221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7165, F1=0.7128, ROC-AUC=0.7172753623188406\n",
      "Optimal features (20): ['VSA_EState4', 'SMR_VSA7', 'BCUT2D_CHGLO', 'BertzCT', 'BCUT2D_MRLOW']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7062, F1=0.7047, ROC-AUC=0.7442318840579709\n",
      "Optimal features (17): ['fr_halogen', 'fr_allylic_oxid', 'SMR_VSA1', 'NHOHCount', 'fr_Al_OH']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7320, F1=0.7209, ROC-AUC=0.7308985507246376\n",
      "Optimal features (10): ['PEOE_VSA8', 'FractionCSP3', 'MaxAbsEStateIndex', 'SlogP_VSA5', 'SMR_VSA7']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью features importance\n",
    "\n",
    "def test_tree_models_fs(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с отбором признаков\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_feature_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_feature_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков для классификации на основе важности признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Инициализация и обучение модели\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostClassifier(iterations=100, random_seed=42, verbose=False)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Получение важности признаков\n",
    "    if name == 'XGBoost':\n",
    "        importance = model.feature_importances_\n",
    "    else:\n",
    "        importance = model.feature_importances_\n",
    "    \n",
    "    # Сортировка признаков по важности\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'].tolist()\n",
    "    \n",
    "    # Поиск оптимального набора признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Ограничиваем количество проверяемых комбинаций для скорости\n",
    "    max_features_to_test = min(20, len(all_features))\n",
    "    \n",
    "    for n in range(1, max_features_to_test + 1):\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'feature_importance'\n",
    "    }\n",
    "    \n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_4  = test_tree_models_fs(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5860f10-8bb9-4002-8bf4-4c0889b7f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем все лучшие результаты\n",
    "results_df = pd.concat([results_col_combination,results_col_combination_2, results_col_combination_3, results_col_combination_4], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943b2ea9-ff60-444e-be72-a4bd2ff042ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.723226</td>\n",
       "      <td>0.726804</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.728251</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>206</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.704280</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>205</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.717400</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>209</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.740451</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>205</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.705605</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinEStateI...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.685939</td>\n",
       "      <td>0.695876</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>195</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.706511</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>198</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.661820</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>193</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.699653</td>\n",
       "      <td>0.706186</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>206</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.689675</td>\n",
       "      <td>0.690722</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>210</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.694899</td>\n",
       "      <td>0.706186</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>204</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.717400</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>[BCUT2D_MRLOW, FractionCSP3, BCUT2D_LOGPHI, SM...</td>\n",
       "      <td>15</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.716025</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[FractionCSP3, BCUT2D_MRLOW, VSA_EState5, BCUT...</td>\n",
       "      <td>18</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.727015</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[BCUT2D_MRLOW, RingCount, NumSaturatedCarbocyc...</td>\n",
       "      <td>18</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.712781</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[VSA_EState4, SMR_VSA7, BCUT2D_CHGLO, BertzCT,...</td>\n",
       "      <td>20</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.704663</td>\n",
       "      <td>0.706186</td>\n",
       "      <td>[fr_halogen, fr_allylic_oxid, SMR_VSA1, NHOHCo...</td>\n",
       "      <td>17</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.720857</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[PEOE_VSA8, FractionCSP3, MaxAbsEStateIndex, S...</td>\n",
       "      <td>10</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model   best_f1  best_accuracy  \\\n",
       "0   LogisticRegression  0.723226       0.726804   \n",
       "1                  SVM  0.728251       0.742268   \n",
       "2                  KNN  0.704280       0.701031   \n",
       "3         RandomForest  0.717400       0.721649   \n",
       "4              XGBoost  0.740451       0.742268   \n",
       "5             CatBoost  0.705605       0.716495   \n",
       "6   LogisticRegression  0.685939       0.695876   \n",
       "7                  SVM  0.706511       0.721649   \n",
       "8                  KNN  0.661820       0.659794   \n",
       "9         RandomForest  0.699653       0.706186   \n",
       "10             XGBoost  0.689675       0.690722   \n",
       "11            CatBoost  0.694899       0.706186   \n",
       "12        RandomForest  0.717400       0.721649   \n",
       "13             XGBoost  0.716025       0.716495   \n",
       "14            CatBoost  0.727015       0.737113   \n",
       "15        RandomForest  0.712781       0.716495   \n",
       "16             XGBoost  0.704663       0.706186   \n",
       "17            CatBoost  0.720857       0.731959   \n",
       "\n",
       "                                        best_features  num_features  \\\n",
       "0   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "1   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           206   \n",
       "2   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           205   \n",
       "3   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           209   \n",
       "4   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           205   \n",
       "5   [MaxAbsEStateIndex, MaxEStateIndex, MinEStateI...           208   \n",
       "6   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           195   \n",
       "7   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           198   \n",
       "8   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           193   \n",
       "9   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           206   \n",
       "10  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           210   \n",
       "11  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           204   \n",
       "12  [BCUT2D_MRLOW, FractionCSP3, BCUT2D_LOGPHI, SM...            15   \n",
       "13  [FractionCSP3, BCUT2D_MRLOW, VSA_EState5, BCUT...            18   \n",
       "14  [BCUT2D_MRLOW, RingCount, NumSaturatedCarbocyc...            18   \n",
       "15  [VSA_EState4, SMR_VSA7, BCUT2D_CHGLO, BertzCT,...            20   \n",
       "16  [fr_halogen, fr_allylic_oxid, SMR_VSA1, NHOHCo...            17   \n",
       "17  [PEOE_VSA8, FractionCSP3, MaxAbsEStateIndex, S...            10   \n",
       "\n",
       "              selector  \n",
       "0    without selection  \n",
       "1    without selection  \n",
       "2    without selection  \n",
       "3    without selection  \n",
       "4    without selection  \n",
       "5    without selection  \n",
       "6             outliers  \n",
       "7             outliers  \n",
       "8             outliers  \n",
       "9             outliers  \n",
       "10            outliers  \n",
       "11            outliers  \n",
       "12                shap  \n",
       "13                shap  \n",
       "14                shap  \n",
       "15  feature_importance  \n",
       "16  feature_importance  \n",
       "17  feature_importance  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e05de657-40c5-4224-8a4d-a4d6ee638132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.7216, F1=0.6954, ROC-AUC=0.6672463768115942\n",
      "Optimal features (18): ['SlogP_VSA6', 'Chi2n', 'EState_VSA8', 'VSA_EState4', 'SMR_VSA6']...\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7732, F1=0.7609, ROC-AUC=0.7349565217391305\n",
      "Optimal features (18): ['SlogP_VSA6', 'Chi2n', 'EState_VSA8', 'VSA_EState4', 'SMR_VSA6']...\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.7165, F1=0.7150, ROC-AUC=0.7083478260869565\n",
      "Optimal features (18): ['SlogP_VSA6', 'Chi2n', 'EState_VSA8', 'VSA_EState4', 'SMR_VSA6']...\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7320, F1=0.7266, ROC-AUC=0.7361159420289856\n",
      "Optimal features (18): ['FractionCSP3', 'VSA_EState5', 'EState_VSA8', 'MinPartialCharge', 'MinAbsEStateIndex']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7371, F1=0.7325, ROC-AUC=0.7254492753623187\n",
      "Optimal features (10): ['FractionCSP3', 'MinPartialCharge', 'SMR_VSA6', 'fr_Imine', 'MaxAbsEStateIndex']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7371, F1=0.7270, ROC-AUC=0.7249855072463768\n",
      "Optimal features (10): ['FractionCSP3', 'MinPartialCharge', 'SMR_VSA6', 'fr_Imine', 'MaxAbsEStateIndex']...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Определяем налучший результат работы моделей\n",
    "def test_all_classifiers(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = evaluate_classifier(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_classifier(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка классификатора с оптимальным набором признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    # Находим оптимальные метрики\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Получаем все наборы признаков из предыдущих результатов\n",
    "    feature_sets = [set(features) for features in results_df['best_features']]\n",
    "    \n",
    "    # Добавляем все признаки для сравнения\n",
    "    feature_sets.append(set(X.columns))\n",
    "    \n",
    "    # Удаляем дубликаты\n",
    "    unique_feature_sets = []\n",
    "    seen = set()\n",
    "    for fs in feature_sets:\n",
    "        frozen = frozenset(fs)\n",
    "        if frozen not in seen:\n",
    "            seen.add(frozen)\n",
    "            unique_feature_sets.append(list(fs))\n",
    "    \n",
    "    for current_features in unique_feature_sets:\n",
    "        current_metrics = get_classification_metrics(X[current_features], y, model_name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features)\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "def get_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_features_selection  = test_all_classifiers(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e76424a1-6c08-4999-b289-7a6dd18ad9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.695427</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>[SlogP_VSA6, Chi2n, EState_VSA8, VSA_EState4, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.760861</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>[SlogP_VSA6, Chi2n, EState_VSA8, VSA_EState4, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[SlogP_VSA6, Chi2n, EState_VSA8, VSA_EState4, ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.726643</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[FractionCSP3, VSA_EState5, EState_VSA8, MinPa...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[FractionCSP3, MinPartialCharge, SMR_VSA6, fr_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.727015</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[FractionCSP3, MinPartialCharge, SMR_VSA6, fr_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.695427       0.721649   \n",
       "1                 SVM  0.760861       0.773196   \n",
       "2                 KNN  0.715026       0.716495   \n",
       "3        RandomForest  0.726643       0.731959   \n",
       "4             XGBoost  0.732510       0.737113   \n",
       "5            CatBoost  0.727015       0.737113   \n",
       "\n",
       "                                       best_features  num_features  \n",
       "0  [SlogP_VSA6, Chi2n, EState_VSA8, VSA_EState4, ...            18  \n",
       "1  [SlogP_VSA6, Chi2n, EState_VSA8, VSA_EState4, ...            18  \n",
       "2  [SlogP_VSA6, Chi2n, EState_VSA8, VSA_EState4, ...            18  \n",
       "3  [FractionCSP3, VSA_EState5, EState_VSA8, MinPa...            18  \n",
       "4  [FractionCSP3, MinPartialCharge, SMR_VSA6, fr_...            10  \n",
       "5  [FractionCSP3, MinPartialCharge, SMR_VSA6, fr_...            10  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_features_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58a92eb3-7eb3-464f-b484-e89cf9b9702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing LogisticRegression: 100%|██████████| 772/772 [00:05<00:00, 136.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.7320, F1=0.7067, ROC-AUC=0.6750144927536232\n",
      "Удалено строк: 4 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing SVM: 100%|██████████| 772/772 [01:04<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7887, F1=0.7764, ROC-AUC=0.736\n",
      "Удалено строк: 4 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing KNN: 100%|██████████| 772/772 [00:13<00:00, 55.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.7629, F1=0.7629, ROC-AUC=0.7299710144927536\n",
      "Удалено строк: 13 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest: 100%|██████████| 772/772 [02:28<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7474, F1=0.7441, ROC-AUC=0.7361739130434782\n",
      "Удалено строк: 9 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost: 100%|██████████| 772/772 [01:07<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7474, F1=0.7430, ROC-AUC=0.719304347826087\n",
      "Удалено строк: 7 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost: 100%|██████████| 772/772 [16:06<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7526, F1=0.7438, ROC-AUC=0.7325217391304348\n",
      "Удалено строк: 2 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Определяем лучшую комбинацию строк для повышения качества работы моделей на всех данных\n",
    "def test_all_classifiers_optimized(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов с оптимизацией набора данных\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = optimize_classifier_data(X, y, name, model)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def optimize_classifier_data(X, y, name, model, verbose=True):\n",
    "    \"\"\"\n",
    "    Оптимизирует набор данных для классификатора путем последовательного удаления строк\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Признаки\n",
    "    y : pd.Series\n",
    "        Целевая переменная\n",
    "    name : str\n",
    "        Имя модели\n",
    "    model : sklearn classifier\n",
    "        Модель классификатора\n",
    "    verbose : bool\n",
    "        Выводить ли прогресс\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации\n",
    "    \"\"\"\n",
    "    # Используем оптимальные признаки для модели\n",
    "    models = {\n",
    "        'LogisticRegression': make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    if name == 'LogisticRegression':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][0]]\n",
    "        \n",
    "    elif name == 'SVM':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][1]]\n",
    "        \n",
    "    elif name == 'KNN':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][2]]\n",
    "    elif name == 'RandomForest':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][3]]\n",
    "    elif name == 'XGBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][4]]\n",
    "    elif name == 'CatBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][5]]\n",
    "    \n",
    "    \n",
    "    # Разделение данных со стратификацией\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Инициализация\n",
    "    X_opt = X_train.copy()\n",
    "    y_opt = y_train.copy()\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    removed_indices = []\n",
    "    \n",
    "    # Прогресс-бар для наглядности\n",
    "    iterator = tqdm(X_train.index, desc=f\"Optimizing {name}\") if verbose else X_train.index\n",
    "    \n",
    "    for idx in iterator:\n",
    "        # Временно удаляем строку\n",
    "        X_temp = X_opt.drop(index=idx)\n",
    "        y_temp = y_opt.drop(index=idx)\n",
    "        \n",
    "        # Обучаем и оцениваем\n",
    "        model = models[name]\n",
    "        model.fit(X_temp, y_temp)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Рассчитываем метрики\n",
    "        current_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y)) == 2:\n",
    "            current_metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Решение о сохранении/удалении строки\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            X_opt = X_temp\n",
    "            y_opt = y_temp\n",
    "            removed_indices.append(idx)\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Удалено строк: {len(removed_indices)} из {len(X_train)}\")\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'removed_indices': removed_indices,\n",
    "        'num_removed': len(removed_indices),\n",
    "        'indices': X_opt.index.tolist(),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "    \n",
    "# Запуск расчета\n",
    "results_indices_selection = test_all_classifiers_optimized(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d9c0bc-20b0-429e-93ef-d9e9d1fe0044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>removed_indices</th>\n",
       "      <th>num_removed</th>\n",
       "      <th>indices</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.706708</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>[110, 62, 271, 714]</td>\n",
       "      <td>4</td>\n",
       "      <td>[528, 780, 196, 175, 788, 799, 851, 502, 272, ...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.776428</td>\n",
       "      <td>0.788660</td>\n",
       "      <td>[110, 196, 739, 282]</td>\n",
       "      <td>4</td>\n",
       "      <td>[528, 780, 175, 788, 799, 851, 502, 272, 797, ...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>[110, 851, 300, 499, 80, 364, 562, 681, 687, 9...</td>\n",
       "      <td>13</td>\n",
       "      <td>[528, 780, 196, 175, 788, 799, 502, 272, 797, ...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.744114</td>\n",
       "      <td>0.747423</td>\n",
       "      <td>[110, 528, 780, 570, 421, 184, 262, 423, 835]</td>\n",
       "      <td>9</td>\n",
       "      <td>[196, 175, 788, 799, 851, 502, 272, 797, 792, ...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>0.747423</td>\n",
       "      <td>[110, 528, 797, 432, 695, 499, 882]</td>\n",
       "      <td>7</td>\n",
       "      <td>[780, 196, 175, 788, 799, 851, 502, 272, 792, ...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.743794</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>[110, 25]</td>\n",
       "      <td>2</td>\n",
       "      <td>[528, 780, 196, 175, 788, 799, 851, 502, 272, ...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.706708       0.731959   \n",
       "1                 SVM  0.776428       0.788660   \n",
       "2                 KNN  0.762887       0.762887   \n",
       "3        RandomForest  0.744114       0.747423   \n",
       "4             XGBoost  0.743000       0.747423   \n",
       "5            CatBoost  0.743794       0.752577   \n",
       "\n",
       "                                     removed_indices  num_removed  \\\n",
       "0                                [110, 62, 271, 714]            4   \n",
       "1                               [110, 196, 739, 282]            4   \n",
       "2  [110, 851, 300, 499, 80, 364, 562, 681, 687, 9...           13   \n",
       "3      [110, 528, 780, 570, 421, 184, 262, 423, 835]            9   \n",
       "4                [110, 528, 797, 432, 695, 499, 882]            7   \n",
       "5                                          [110, 25]            2   \n",
       "\n",
       "                                             indices           selector  \n",
       "0  [528, 780, 196, 175, 788, 799, 851, 502, 272, ...  without selection  \n",
       "1  [528, 780, 175, 788, 799, 851, 502, 272, 797, ...  without selection  \n",
       "2  [528, 780, 196, 175, 788, 799, 502, 272, 797, ...  without selection  \n",
       "3  [196, 175, 788, 799, 851, 502, 272, 797, 792, ...  without selection  \n",
       "4  [780, 196, 175, 788, 799, 851, 502, 272, 792, ...  without selection  \n",
       "5  [528, 780, 196, 175, 788, 799, 851, 502, 272, ...  without selection  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_indices_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10259f61-a633-4394-88f7-24551659a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем налучшую комбинацию строк для повышения качества работы моделей\n",
    "\n",
    "def optimize_classifier(X_train, X_test, y_train, y_test, model_type='logistic', n_trials=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Оптимизирует гиперпараметры для классификаторов с улучшенной обработкой ошибок SVM\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Тип модели: 'logistic', 'svm' или 'knn'\n",
    "    n_trials : int\n",
    "        Количество испытаний для Optuna\n",
    "    random_state : int\n",
    "        Seed для воспроизводимости\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации (лучшие параметры, метрики, study)\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "    # Проверяем, является ли это первым trial (нулевым по индексу)\n",
    "        \n",
    "            # Обычная логика подбора параметров\n",
    "        try:\n",
    "            if model_type == 'logistic':\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['l2', 'l1']),\n",
    "                    'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),LogisticRegression(**params, random_state=random_state))\n",
    "                \n",
    "            elif model_type == 'svm':\n",
    "                kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'kernel': kernel,\n",
    "                    'gamma': trial.suggest_float('gamma', 1e-5, 10, log=True),\n",
    "                    'tol': trial.suggest_float('tol', 1e-5, 1e-1, log=True),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                if kernel == 'poly':\n",
    "                    params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "                \n",
    "                model = make_pipeline(StandardScaler(),SVC(**params, random_state=random_state, probability=True))\n",
    "                \n",
    "            elif model_type == 'knn':\n",
    "                params = {\n",
    "                    'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
    "                    'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "                    'p': trial.suggest_int('p', 1, 3)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),KNeighborsClassifier(**params))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if model_type == 'svm':\n",
    "                error_info = f\"SVM failed with params: {params}. Error: {str(e)}\"\n",
    "                trial.set_user_attr(\"svm_error\", error_info)\n",
    "            return float('-inf')\n",
    "        \n",
    "        # Общая часть для всех моделей (дефолтных и оптимизированных)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y_train)) == 2:\n",
    "            trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return f1\n",
    "        \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    # Анализ результатов\n",
    "    best_params = study.best_params\n",
    "    best_metrics = {\n",
    "        'f1': study.best_value,\n",
    "        'accuracy': study.best_trial.user_attrs['accuracy'],\n",
    "    }\n",
    "    \n",
    "    if 'roc_auc' in study.best_trial.user_attrs:\n",
    "        best_metrics['roc_auc'] = study.best_trial.user_attrs['roc_auc']\n",
    "    \n",
    "    # Добавляем информацию о неудачных trials для SVM\n",
    "    if model_type == 'svm':\n",
    "        failed_trials = [t for t in study.trials if 'svm_error' in t.user_attrs]\n",
    "        if failed_trials:\n",
    "            best_metrics['failed_trials_count'] = len(failed_trials)\n",
    "            best_metrics['last_error'] = failed_trials[-1].user_attrs['svm_error']\n",
    "    \n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'best_params': best_params,\n",
    "        'best_metrics': best_metrics,\n",
    "        'study': study\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44b5a03c-04b9-48de-81be-5b1fb9e7eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск подбора гиперпараметров\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train_logistic = X_train[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]].drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "y_train_logistic = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "X_test_logistic = X_test[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]]\n",
    "logistic_results = optimize_classifier(X_train_logistic, X_test_logistic, y_train_logistic, y_test, 'logistic', n_trials=200)\n",
    "\n",
    "X_train_svm = X_train[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]].drop(index=results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "y_train_svm = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "X_test_svm = X_test[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]]\n",
    "svm_results = optimize_classifier(X_train_svm, X_test_svm, y_train_svm, y_test, 'svm', n_trials=200)\n",
    "X_train_knn = X_train[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]].drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "y_train_knn = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "X_test_knn = X_test[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]]\n",
    "knn_results = optimize_classifier(X_train_knn, X_test_knn, y_train_knn, y_test, 'knn', n_trials=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e4f328-a7fb-40ed-b301-9daaee300865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best params: {'C': 1.0550446890007186, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 887}\n",
      "LogisticRegression best f1: 0.7067\n",
      "LogisticRegression best accuracy: 0.7320\n",
      "SVM best params: {'kernel': 'rbf', 'C': 1.2923030272742622, 'gamma': 0.03960740398847891, 'tol': 0.014127865635648013, 'max_iter': 553}\n",
      "SVM best f1: 0.7764\n",
      "SVM best accuracy: 0.7887\n",
      "KNN best params: {'n_neighbors': 11, 'weights': 'uniform', 'p': 3}\n",
      "KNN best f1: 0.7605\n",
      "KNN best accuracy: 0.7680\n"
     ]
    }
   ],
   "source": [
    "# Вывод результатов подбора гиперпараметров для линейных моделей\n",
    "print(f\"LogisticRegression best params: {logistic_results['best_params']}\")\n",
    "print(f\"LogisticRegression best f1: {logistic_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"LogisticRegression best accuracy: {logistic_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"SVM best params: {svm_results['best_params']}\")\n",
    "print(f\"SVM best f1: {svm_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"SVM best accuracy: {svm_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"KNN best params: {knn_results['best_params']}\")\n",
    "print(f\"KNN best f1: {knn_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"KNN best accuracy: {knn_results['best_metrics']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f55b546d-50e7-4fd4-9f1a-daf3824d6ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost Classifier...\n",
      "\n",
      "Best trial:\n",
      "  F1-score: 0.721899\n",
      "  Accuracy: 0.737113\n",
      "  ROC-AUC: 0.734667\n",
      "\n",
      "Best params:\n",
      "  iterations: 789\n",
      "  depth: 5\n",
      "  learning_rate: 0.08664298340376464\n",
      "  l2_leaf_reg: 3.5873306994955336\n",
      "  random_strength: 0.2393462349333718\n",
      "  bagging_temperature: 0.22062987672488085\n",
      "  border_count: 120\n",
      "  min_data_in_leaf: 38\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для CatBoost\n",
    "# Подготовка данных для CatBoost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_cb = X_train[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]].drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "y_train_cb = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "X_test_cb = X_test[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]]\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров CatBoostClassifier\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42,\n",
    "        'thread_count': 4\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(\n",
    "        X_train_cb, \n",
    "        y_train_cb,\n",
    "        eval_set=(X_test_cb, y_test),\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Предсказание и метрики\n",
    "    y_pred = model.predict(X_test_cb)\n",
    "    y_proba = model.predict_proba(X_test_cb)[:, 1] if len(y_train_cb.unique()) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing CatBoost Classifier...\")\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study.trials) > 0:\n",
    "    best_trial_cb = study.best_trial\n",
    "    \n",
    "    print(\"\\nBest trial:\")\n",
    "    print(f\"  F1-score: {best_trial_cb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_cb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_cb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_cb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_cb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_cb.params\n",
    "    best_params.update({\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42\n",
    "    })\n",
    "    \n",
    "    best_model = CatBoostClassifier(**best_params)\n",
    "    best_model.fit(\n",
    "        pd.concat([X_train_cb, X_test_cb]),\n",
    "        pd.concat([y_train_cb, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cc0194b-d65f-4c27-98bf-4258b6634371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost Classifier...\n",
      "\n",
      "Best XGBoost Classifier:\n",
      "  F1-score: 0.762375\n",
      "  Accuracy: 0.773196\n",
      "  ROC-AUC: 0.753159\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 259\n",
      "  max_depth: 9\n",
      "  learning_rate: 0.07247249842645875\n",
      "  subsample: 0.9919973104611004\n",
      "  colsample_bytree: 0.8474067262937713\n",
      "  gamma: 0.09380605835915679\n",
      "  reg_alpha: 1.0646265088629532\n",
      "  reg_lambda: 1.7570607423759232\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для XGBoost\n",
    "# Подготовка данных для XGBoost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]].drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]]\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров XGBoost (классификация)\"\"\"\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'eval_metric': 'logloss',\n",
    "        'early_stopping_rounds': 20  # Перенесено сюда\n",
    "    }\n",
    "    \n",
    "    # Автоматическое определение типа задачи\n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        xgb_params['objective'] = 'multi:softmax'\n",
    "        xgb_params['num_class'] = len(np.unique(y_train))\n",
    "    else:\n",
    "        xgb_params['objective'] = 'binary:logistic'\n",
    "\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)],  # Оставлено здесь\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Остальной код без изменений\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_xgb = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing XGBoost Classifier...\")\n",
    "    study_xgb.optimize(\n",
    "        objective_xgb,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_xgb.trials) > 0:\n",
    "    best_trial_xgb = study_xgb.best_trial\n",
    "    \n",
    "    print(\"\\nBest XGBoost Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_xgb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_xgb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_xgb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_xgb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_xgb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_xgb.params\n",
    "    best_params.update({\n",
    "        'objective': 'multi:softmax' if len(np.unique(y_train)) > 2 else 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        best_params['num_class'] = len(np.unique(y_train))\n",
    "    \n",
    "    best_xgb = XGBClassifier(**best_params)\n",
    "    best_xgb.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4b9f77b-8173-459f-bd02-b586ef3f9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest Classifier...\n",
      "\n",
      "Best RandomForest Classifier:\n",
      "  F1-score: 0.746947\n",
      "  Accuracy: 0.757732\n",
      "  ROC-AUC: 0.731130\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 50\n",
      "  max_depth: 15\n",
      "  min_samples_split: 16\n",
      "  min_samples_leaf: 10\n",
      "  max_features: 0.5250032272972257\n",
      "  bootstrap: True\n",
      "  class_weight: None\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для RandomForest\n",
    "# Подготовка данных для RandomForest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]].drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]]\n",
    "\n",
    "def objective_rf(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров RandomForest (классификация)\"\"\"\n",
    "    \n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'criterion': 'gini'  # Можно добавить выбор между 'gini' и 'entropy'\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный для многоклассовой)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_rf = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing RandomForest Classifier...\")\n",
    "    study_rf.optimize(\n",
    "        objective_rf,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_rf.trials) > 0:\n",
    "    best_trial_rf = study_rf.best_trial\n",
    "    \n",
    "    print(\"\\nBest RandomForest Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_rf.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_rf.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_rf.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_rf.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_rf.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_rf.params\n",
    "    best_params.update({\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    best_rf = RandomForestClassifier(**best_params)\n",
    "    best_rf.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b1c00b7-5c8d-4a50-983d-cda7ebcd19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем лучшие результаты\n",
    "best_results_total = []\n",
    "logistic = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['model'][0],\n",
    "        'best_f1':  logistic_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  logistic_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['indices'][0])\n",
    "    }\n",
    "best_results_total.append(logistic)\n",
    "svm = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'SVM']['model'][1],\n",
    "        'best_f1':  svm_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  svm_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'SVM']['indices'][1])\n",
    "    }\n",
    "best_results_total.append(svm)\n",
    "knn = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'KNN']['model'][2],\n",
    "        'best_f1':  knn_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  knn_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'KNN']['indices'][2])\n",
    "    }\n",
    "best_results_total.append(knn)\n",
    "rf = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'RandomForest']['model'][3],\n",
    "        'best_f1':  best_trial_rf.value,\n",
    "        'best_accuracy':  best_trial_rf.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['indices'][3])\n",
    "    }\n",
    "best_results_total.append(rf)\n",
    "\n",
    "xg = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'XGBoost']['model'][4],\n",
    "        'best_f1':  best_trial_xgb.value,\n",
    "        'best_accuracy':  best_trial_xgb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['indices'][4])\n",
    "    }\n",
    "best_results_total.append(xg)\n",
    "cb = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'CatBoost']['model'][5],\n",
    "        'best_f1':  best_trial_cb.value,\n",
    "        'best_accuracy':  best_trial_cb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['indices'][5])\n",
    "    }\n",
    "best_results_total.append(cb)\n",
    "\n",
    "best_results_total_df = pd.DataFrame(best_results_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b12ad85d-f5c6-490c-950b-c6e38a73a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>count_features</th>\n",
       "      <th>count_strings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.706708</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>18</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.776428</td>\n",
       "      <td>0.788660</td>\n",
       "      <td>18</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.760462</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>18</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.746947</td>\n",
       "      <td>0.757732</td>\n",
       "      <td>18</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.762375</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>10</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.721899</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>10</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  count_features  count_strings\n",
       "0  LogisticRegression  0.706708       0.731959              18            768\n",
       "1                 SVM  0.776428       0.788660              18            768\n",
       "2                 KNN  0.760462       0.768041              18            759\n",
       "3        RandomForest  0.746947       0.757732              18            763\n",
       "4             XGBoost  0.762375       0.773196              10            765\n",
       "5            CatBoost  0.721899       0.737113              10            770"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лучшие результаты\n",
    "best_results_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f059f47-0250-4ccd-9494-64142ba4112c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
