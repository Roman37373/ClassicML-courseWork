{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.937578Z",
     "start_time": "2025-05-28T16:44:43.537577Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shap\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from scipy import stats\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, \n",
    "                           r2_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, classification_report,\n",
    "                           roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Other ML libraries\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267e6757ca7a5130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.960272Z",
     "start_time": "2025-05-28T16:44:45.957343Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5b2279b2b7ab49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:11.783119Z",
     "start_time": "2025-05-28T16:57:11.742132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "df = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d4d43a0b8f2e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:26.787706Z",
     "start_time": "2025-05-28T17:02:26.674195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>IC50, mM</th>\n",
       "      <th>CC50, mM</th>\n",
       "      <th>SI</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>NumValenceElectrons</th>\n",
       "      <th>NumRadicalElectrons</th>\n",
       "      <th>MaxPartialCharge</th>\n",
       "      <th>MinPartialCharge</th>\n",
       "      <th>MaxAbsPartialCharge</th>\n",
       "      <th>MinAbsPartialCharge</th>\n",
       "      <th>FpDensityMorgan1</th>\n",
       "      <th>FpDensityMorgan2</th>\n",
       "      <th>FpDensityMorgan3</th>\n",
       "      <th>BCUT2D_MWHI</th>\n",
       "      <th>BCUT2D_MWLOW</th>\n",
       "      <th>BCUT2D_CHGHI</th>\n",
       "      <th>BCUT2D_CHGLO</th>\n",
       "      <th>BCUT2D_LOGPHI</th>\n",
       "      <th>BCUT2D_LOGPLOW</th>\n",
       "      <th>BCUT2D_MRHI</th>\n",
       "      <th>BCUT2D_MRLOW</th>\n",
       "      <th>AvgIpc</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi0n</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi1n</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>Chi2n</th>\n",
       "      <th>Chi2v</th>\n",
       "      <th>Chi3n</th>\n",
       "      <th>Chi3v</th>\n",
       "      <th>Chi4n</th>\n",
       "      <th>Chi4v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Ipc</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>LabuteASA</th>\n",
       "      <th>PEOE_VSA1</th>\n",
       "      <th>PEOE_VSA10</th>\n",
       "      <th>PEOE_VSA11</th>\n",
       "      <th>PEOE_VSA12</th>\n",
       "      <th>PEOE_VSA13</th>\n",
       "      <th>PEOE_VSA14</th>\n",
       "      <th>PEOE_VSA2</th>\n",
       "      <th>PEOE_VSA3</th>\n",
       "      <th>PEOE_VSA4</th>\n",
       "      <th>PEOE_VSA5</th>\n",
       "      <th>PEOE_VSA6</th>\n",
       "      <th>PEOE_VSA7</th>\n",
       "      <th>PEOE_VSA8</th>\n",
       "      <th>PEOE_VSA9</th>\n",
       "      <th>SMR_VSA1</th>\n",
       "      <th>SMR_VSA10</th>\n",
       "      <th>SMR_VSA2</th>\n",
       "      <th>SMR_VSA3</th>\n",
       "      <th>SMR_VSA4</th>\n",
       "      <th>SMR_VSA5</th>\n",
       "      <th>SMR_VSA6</th>\n",
       "      <th>SMR_VSA7</th>\n",
       "      <th>SMR_VSA8</th>\n",
       "      <th>SMR_VSA9</th>\n",
       "      <th>SlogP_VSA1</th>\n",
       "      <th>SlogP_VSA10</th>\n",
       "      <th>SlogP_VSA11</th>\n",
       "      <th>SlogP_VSA12</th>\n",
       "      <th>SlogP_VSA2</th>\n",
       "      <th>SlogP_VSA3</th>\n",
       "      <th>SlogP_VSA4</th>\n",
       "      <th>SlogP_VSA5</th>\n",
       "      <th>SlogP_VSA6</th>\n",
       "      <th>SlogP_VSA7</th>\n",
       "      <th>SlogP_VSA8</th>\n",
       "      <th>SlogP_VSA9</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>EState_VSA1</th>\n",
       "      <th>EState_VSA10</th>\n",
       "      <th>EState_VSA11</th>\n",
       "      <th>EState_VSA2</th>\n",
       "      <th>EState_VSA3</th>\n",
       "      <th>EState_VSA4</th>\n",
       "      <th>EState_VSA5</th>\n",
       "      <th>EState_VSA6</th>\n",
       "      <th>EState_VSA7</th>\n",
       "      <th>EState_VSA8</th>\n",
       "      <th>EState_VSA9</th>\n",
       "      <th>VSA_EState1</th>\n",
       "      <th>VSA_EState10</th>\n",
       "      <th>VSA_EState2</th>\n",
       "      <th>VSA_EState3</th>\n",
       "      <th>VSA_EState4</th>\n",
       "      <th>VSA_EState5</th>\n",
       "      <th>VSA_EState6</th>\n",
       "      <th>VSA_EState7</th>\n",
       "      <th>VSA_EState8</th>\n",
       "      <th>VSA_EState9</th>\n",
       "      <th>FractionCSP3</th>\n",
       "      <th>HeavyAtomCount</th>\n",
       "      <th>NHOHCount</th>\n",
       "      <th>NOCount</th>\n",
       "      <th>NumAliphaticCarbocycles</th>\n",
       "      <th>NumAliphaticHeterocycles</th>\n",
       "      <th>NumAliphaticRings</th>\n",
       "      <th>NumAromaticCarbocycles</th>\n",
       "      <th>NumAromaticHeterocycles</th>\n",
       "      <th>NumAromaticRings</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHeteroatoms</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>NumSaturatedCarbocycles</th>\n",
       "      <th>NumSaturatedHeterocycles</th>\n",
       "      <th>NumSaturatedRings</th>\n",
       "      <th>RingCount</th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>MolMR</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>fr_Ar_OH</th>\n",
       "      <th>fr_COO</th>\n",
       "      <th>fr_COO2</th>\n",
       "      <th>fr_C_O</th>\n",
       "      <th>fr_C_O_noCOO</th>\n",
       "      <th>fr_C_S</th>\n",
       "      <th>fr_HOCCN</th>\n",
       "      <th>fr_Imine</th>\n",
       "      <th>fr_NH0</th>\n",
       "      <th>fr_NH1</th>\n",
       "      <th>fr_NH2</th>\n",
       "      <th>fr_N_O</th>\n",
       "      <th>fr_Ndealkylation1</th>\n",
       "      <th>fr_Ndealkylation2</th>\n",
       "      <th>fr_Nhpyrrole</th>\n",
       "      <th>fr_SH</th>\n",
       "      <th>fr_aldehyde</th>\n",
       "      <th>fr_alkyl_carbamate</th>\n",
       "      <th>fr_alkyl_halide</th>\n",
       "      <th>fr_allylic_oxid</th>\n",
       "      <th>fr_amide</th>\n",
       "      <th>fr_amidine</th>\n",
       "      <th>fr_aniline</th>\n",
       "      <th>fr_aryl_methyl</th>\n",
       "      <th>fr_azide</th>\n",
       "      <th>fr_azo</th>\n",
       "      <th>fr_barbitur</th>\n",
       "      <th>fr_benzene</th>\n",
       "      <th>fr_benzodiazepine</th>\n",
       "      <th>fr_bicyclic</th>\n",
       "      <th>fr_diazo</th>\n",
       "      <th>fr_dihydropyridine</th>\n",
       "      <th>fr_epoxide</th>\n",
       "      <th>fr_ester</th>\n",
       "      <th>fr_ether</th>\n",
       "      <th>fr_furan</th>\n",
       "      <th>fr_guanido</th>\n",
       "      <th>fr_halogen</th>\n",
       "      <th>fr_hdrzine</th>\n",
       "      <th>fr_hdrzone</th>\n",
       "      <th>fr_imidazole</th>\n",
       "      <th>fr_imide</th>\n",
       "      <th>fr_isocyan</th>\n",
       "      <th>fr_isothiocyan</th>\n",
       "      <th>fr_ketone</th>\n",
       "      <th>fr_ketone_Topliss</th>\n",
       "      <th>fr_lactam</th>\n",
       "      <th>fr_lactone</th>\n",
       "      <th>fr_methoxy</th>\n",
       "      <th>fr_morpholine</th>\n",
       "      <th>fr_nitrile</th>\n",
       "      <th>fr_nitro</th>\n",
       "      <th>fr_nitro_arom</th>\n",
       "      <th>fr_nitro_arom_nonortho</th>\n",
       "      <th>fr_nitroso</th>\n",
       "      <th>fr_oxazole</th>\n",
       "      <th>fr_oxime</th>\n",
       "      <th>fr_para_hydroxylation</th>\n",
       "      <th>fr_phenol</th>\n",
       "      <th>fr_phenol_noOrthoHbond</th>\n",
       "      <th>fr_phos_acid</th>\n",
       "      <th>fr_phos_ester</th>\n",
       "      <th>fr_piperdine</th>\n",
       "      <th>fr_piperzine</th>\n",
       "      <th>fr_priamide</th>\n",
       "      <th>fr_prisulfonamd</th>\n",
       "      <th>fr_pyridine</th>\n",
       "      <th>fr_quatN</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.239374</td>\n",
       "      <td>175.482382</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.417362</td>\n",
       "      <td>42.928571</td>\n",
       "      <td>384.652</td>\n",
       "      <td>340.300</td>\n",
       "      <td>384.350449</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>1.321429</td>\n",
       "      <td>14.822266</td>\n",
       "      <td>9.700470</td>\n",
       "      <td>2.600532</td>\n",
       "      <td>-2.343082</td>\n",
       "      <td>2.644698</td>\n",
       "      <td>-2.322229</td>\n",
       "      <td>5.944519</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.164038</td>\n",
       "      <td>611.920301</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>20.606247</td>\n",
       "      <td>6.947534</td>\n",
       "      <td>2.868737</td>\n",
       "      <td>173.630124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>64.208216</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.188192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.807589</td>\n",
       "      <td>1.764908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.258223</td>\n",
       "      <td>16.981087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1212</td>\n",
       "      <td>121.5300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.771831</td>\n",
       "      <td>5.402819</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.462473</td>\n",
       "      <td>45.214286</td>\n",
       "      <td>388.684</td>\n",
       "      <td>340.300</td>\n",
       "      <td>388.381750</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>-0.313407</td>\n",
       "      <td>0.313407</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>14.975110</td>\n",
       "      <td>9.689226</td>\n",
       "      <td>2.614066</td>\n",
       "      <td>-2.394690</td>\n",
       "      <td>2.658342</td>\n",
       "      <td>-2.444817</td>\n",
       "      <td>5.134527</td>\n",
       "      <td>0.120322</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.080362</td>\n",
       "      <td>516.780124</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>21.163454</td>\n",
       "      <td>7.257648</td>\n",
       "      <td>3.027177</td>\n",
       "      <td>174.939204</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>97.951860</td>\n",
       "      <td>12.083682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.173194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>77.297729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.922833</td>\n",
       "      <td>2.153503</td>\n",
       "      <td>1.914377</td>\n",
       "      <td>1.536674</td>\n",
       "      <td>14.135381</td>\n",
       "      <td>17.670565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1556</td>\n",
       "      <td>120.5074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>223.808778</td>\n",
       "      <td>161.142320</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.260923</td>\n",
       "      <td>42.187500</td>\n",
       "      <td>446.808</td>\n",
       "      <td>388.344</td>\n",
       "      <td>446.458903</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>-0.325573</td>\n",
       "      <td>0.325573</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.156250</td>\n",
       "      <td>15.353938</td>\n",
       "      <td>9.681293</td>\n",
       "      <td>2.665274</td>\n",
       "      <td>-2.477203</td>\n",
       "      <td>2.679014</td>\n",
       "      <td>-2.565224</td>\n",
       "      <td>5.117187</td>\n",
       "      <td>-0.922902</td>\n",
       "      <td>3.214947</td>\n",
       "      <td>1.219066</td>\n",
       "      <td>643.620154</td>\n",
       "      <td>23.794682</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>14.595754</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>8.610751e+06</td>\n",
       "      <td>25.026112</td>\n",
       "      <td>7.709373</td>\n",
       "      <td>3.470070</td>\n",
       "      <td>201.238858</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>23.671624</td>\n",
       "      <td>53.363882</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>41.280201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.329944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>86.263791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.733111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.517630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.184127</td>\n",
       "      <td>1.930720</td>\n",
       "      <td>1.738402</td>\n",
       "      <td>14.491619</td>\n",
       "      <td>18.287216</td>\n",
       "      <td>10.183618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1292</td>\n",
       "      <td>138.4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.705624</td>\n",
       "      <td>107.855654</td>\n",
       "      <td>63.235294</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.377846</td>\n",
       "      <td>41.862069</td>\n",
       "      <td>398.679</td>\n",
       "      <td>352.311</td>\n",
       "      <td>398.366099</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>14.821216</td>\n",
       "      <td>9.700497</td>\n",
       "      <td>2.600529</td>\n",
       "      <td>-2.342885</td>\n",
       "      <td>2.644709</td>\n",
       "      <td>-2.322030</td>\n",
       "      <td>5.944502</td>\n",
       "      <td>0.193510</td>\n",
       "      <td>3.179270</td>\n",
       "      <td>1.120513</td>\n",
       "      <td>626.651366</td>\n",
       "      <td>20.916003</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>13.627794</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>3.572142e+06</td>\n",
       "      <td>21.567454</td>\n",
       "      <td>7.485204</td>\n",
       "      <td>3.263848</td>\n",
       "      <td>179.995066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.804888</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>70.629038</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.194720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.827852</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.695439</td>\n",
       "      <td>17.012013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.5113</td>\n",
       "      <td>126.1470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>107.131532</td>\n",
       "      <td>139.270991</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.429038</td>\n",
       "      <td>36.514286</td>\n",
       "      <td>466.713</td>\n",
       "      <td>424.377</td>\n",
       "      <td>466.334799</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>-0.257239</td>\n",
       "      <td>0.257239</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.257143</td>\n",
       "      <td>14.831112</td>\n",
       "      <td>9.700386</td>\n",
       "      <td>2.602486</td>\n",
       "      <td>-2.342009</td>\n",
       "      <td>2.648473</td>\n",
       "      <td>-2.318893</td>\n",
       "      <td>5.963448</td>\n",
       "      <td>0.193687</td>\n",
       "      <td>3.337074</td>\n",
       "      <td>1.136678</td>\n",
       "      <td>1101.164252</td>\n",
       "      <td>24.639617</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>16.526773</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>1.053758e+08</td>\n",
       "      <td>23.194917</td>\n",
       "      <td>7.639211</td>\n",
       "      <td>3.345855</td>\n",
       "      <td>211.919602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.807891</td>\n",
       "      <td>103.003916</td>\n",
       "      <td>22.253351</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.798143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>86.488175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.657840</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>91.194256</td>\n",
       "      <td>58.515746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>29.631406</td>\n",
       "      <td>61.075203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.073360</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.301020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.071783</td>\n",
       "      <td>1.605178</td>\n",
       "      <td>17.869058</td>\n",
       "      <td>8.627311</td>\n",
       "      <td>14.692318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9.1148</td>\n",
       "      <td>148.3380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>996</td>\n",
       "      <td>31.000104</td>\n",
       "      <td>34.999650</td>\n",
       "      <td>1.129017</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>-0.476142</td>\n",
       "      <td>0.382752</td>\n",
       "      <td>49.133333</td>\n",
       "      <td>414.542</td>\n",
       "      <td>380.270</td>\n",
       "      <td>414.240624</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>1.866667</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>16.586886</td>\n",
       "      <td>9.344314</td>\n",
       "      <td>2.726237</td>\n",
       "      <td>-2.677345</td>\n",
       "      <td>2.739076</td>\n",
       "      <td>-2.646743</td>\n",
       "      <td>5.980114</td>\n",
       "      <td>-0.196385</td>\n",
       "      <td>3.023764</td>\n",
       "      <td>1.646946</td>\n",
       "      <td>857.600295</td>\n",
       "      <td>21.637464</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>14.097861</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>6.242348e+06</td>\n",
       "      <td>20.263719</td>\n",
       "      <td>6.198453</td>\n",
       "      <td>2.219273</td>\n",
       "      <td>178.490760</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>24.360600</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.017713</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.67</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.409521</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.503509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.515343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498752</td>\n",
       "      <td>-0.405436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.985276</td>\n",
       "      <td>8.824371</td>\n",
       "      <td>1.494852</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.3002</td>\n",
       "      <td>109.8350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>997</td>\n",
       "      <td>31.999934</td>\n",
       "      <td>33.999415</td>\n",
       "      <td>1.062484</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>0.369425</td>\n",
       "      <td>44.542857</td>\n",
       "      <td>485.621</td>\n",
       "      <td>446.309</td>\n",
       "      <td>485.277738</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>-0.467493</td>\n",
       "      <td>0.467493</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.457143</td>\n",
       "      <td>16.586914</td>\n",
       "      <td>9.343622</td>\n",
       "      <td>2.725543</td>\n",
       "      <td>-2.679467</td>\n",
       "      <td>2.738755</td>\n",
       "      <td>-2.655659</td>\n",
       "      <td>5.980828</td>\n",
       "      <td>-0.187625</td>\n",
       "      <td>3.130958</td>\n",
       "      <td>1.535171</td>\n",
       "      <td>1016.917688</td>\n",
       "      <td>25.499271</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>16.402391</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>5.897229e+07</td>\n",
       "      <td>24.511583</td>\n",
       "      <td>7.908743</td>\n",
       "      <td>3.147136</td>\n",
       "      <td>207.296970</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>23.815096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.185457</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.966734</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>73.143616</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>23.344043</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.347395</td>\n",
       "      <td>5.917906</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.086396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.516353</td>\n",
       "      <td>2.923049</td>\n",
       "      <td>0.162524</td>\n",
       "      <td>-1.300354</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>7.521836</td>\n",
       "      <td>10.378794</td>\n",
       "      <td>1.327425</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3.8049</td>\n",
       "      <td>127.4397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>998</td>\n",
       "      <td>30.999883</td>\n",
       "      <td>33.999458</td>\n",
       "      <td>1.096761</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>0.026535</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>41.973684</td>\n",
       "      <td>545.742</td>\n",
       "      <td>502.398</td>\n",
       "      <td>545.281109</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>-0.467485</td>\n",
       "      <td>0.467485</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>1.894737</td>\n",
       "      <td>2.552632</td>\n",
       "      <td>32.166365</td>\n",
       "      <td>9.343613</td>\n",
       "      <td>2.725818</td>\n",
       "      <td>-2.679527</td>\n",
       "      <td>2.738943</td>\n",
       "      <td>-2.656447</td>\n",
       "      <td>7.980998</td>\n",
       "      <td>-0.187687</td>\n",
       "      <td>3.204255</td>\n",
       "      <td>1.493776</td>\n",
       "      <td>1070.961298</td>\n",
       "      <td>27.620591</td>\n",
       "      <td>23.633394</td>\n",
       "      <td>24.449891</td>\n",
       "      <td>17.940396</td>\n",
       "      <td>14.301838</td>\n",
       "      <td>15.695685</td>\n",
       "      <td>13.248561</td>\n",
       "      <td>14.234160</td>\n",
       "      <td>11.326709</td>\n",
       "      <td>11.970659</td>\n",
       "      <td>9.725583</td>\n",
       "      <td>10.196987</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>2.627956e+08</td>\n",
       "      <td>27.726151</td>\n",
       "      <td>9.668673</td>\n",
       "      <td>3.822745</td>\n",
       "      <td>230.149965</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>79.620167</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>35.576981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>78.682541</td>\n",
       "      <td>19.118420</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>48.975357</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>72.640700</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>28.759033</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.932405</td>\n",
       "      <td>12.338728</td>\n",
       "      <td>44.277783</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>6.255769</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.305031</td>\n",
       "      <td>1.639399</td>\n",
       "      <td>52.527620</td>\n",
       "      <td>3.081660</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>-0.487671</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>10.055493</td>\n",
       "      <td>8.774745</td>\n",
       "      <td>1.364715</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.5381</td>\n",
       "      <td>144.7647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>999</td>\n",
       "      <td>31.998959</td>\n",
       "      <td>32.999644</td>\n",
       "      <td>1.031272</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>0.146522</td>\n",
       "      <td>-1.408652</td>\n",
       "      <td>0.381559</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>522.635</td>\n",
       "      <td>480.299</td>\n",
       "      <td>522.282883</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>-0.468755</td>\n",
       "      <td>0.468755</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.351351</td>\n",
       "      <td>1.864865</td>\n",
       "      <td>16.540061</td>\n",
       "      <td>9.364015</td>\n",
       "      <td>2.730109</td>\n",
       "      <td>-2.652209</td>\n",
       "      <td>2.704027</td>\n",
       "      <td>-2.678553</td>\n",
       "      <td>5.950258</td>\n",
       "      <td>-0.225309</td>\n",
       "      <td>2.887043</td>\n",
       "      <td>2.325807</td>\n",
       "      <td>957.299494</td>\n",
       "      <td>27.921921</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>17.309003</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>7.702780e+07</td>\n",
       "      <td>29.111081</td>\n",
       "      <td>10.369092</td>\n",
       "      <td>4.164473</td>\n",
       "      <td>218.836986</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>5.783245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.877221</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.192033</td>\n",
       "      <td>56.278648</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>51.104983</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>29.660466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.099656</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.27</td>\n",
       "      <td>63.742418</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.512100</td>\n",
       "      <td>19.262465</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>13.847474</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>20.885832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.303382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.785593</td>\n",
       "      <td>-6.848660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.955837</td>\n",
       "      <td>7.488627</td>\n",
       "      <td>5.083909</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.3649</td>\n",
       "      <td>131.7080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1000</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>-0.447978</td>\n",
       "      <td>0.452565</td>\n",
       "      <td>48.580645</td>\n",
       "      <td>426.597</td>\n",
       "      <td>388.293</td>\n",
       "      <td>426.277010</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>1.064516</td>\n",
       "      <td>1.774194</td>\n",
       "      <td>2.451613</td>\n",
       "      <td>16.525216</td>\n",
       "      <td>9.330327</td>\n",
       "      <td>2.704274</td>\n",
       "      <td>-2.696212</td>\n",
       "      <td>2.737481</td>\n",
       "      <td>-2.668850</td>\n",
       "      <td>5.978085</td>\n",
       "      <td>-0.201381</td>\n",
       "      <td>2.740713</td>\n",
       "      <td>1.651446</td>\n",
       "      <td>870.462214</td>\n",
       "      <td>22.344571</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>14.597861</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>9.086032e+06</td>\n",
       "      <td>21.398566</td>\n",
       "      <td>6.776167</td>\n",
       "      <td>2.566599</td>\n",
       "      <td>186.107099</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>11.566490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.969305</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>30.092446</td>\n",
       "      <td>12.524788</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>17.535795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.645593</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.44</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.470910</td>\n",
       "      <td>36.243945</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>5.298868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.472742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776999</td>\n",
       "      <td>1.605640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.247616</td>\n",
       "      <td>8.999949</td>\n",
       "      <td>1.514853</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.1488</td>\n",
       "      <td>117.9840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    IC50, mM    CC50, mM         SI  MaxAbsEStateIndex  \\\n",
       "0             0    6.239374  175.482382  28.125000           5.094096   \n",
       "1             1    0.771831    5.402819   7.000000           3.961417   \n",
       "2             2  223.808778  161.142320   0.720000           2.627117   \n",
       "3             3    1.705624  107.855654  63.235294           5.097360   \n",
       "4             4  107.131532  139.270991   1.300000           5.150510   \n",
       "..          ...         ...         ...        ...                ...   \n",
       "993         996   31.000104   34.999650   1.129017          12.934891   \n",
       "994         997   31.999934   33.999415   1.062484          13.635345   \n",
       "995         998   30.999883   33.999458   1.096761          13.991690   \n",
       "996         999   31.998959   32.999644   1.031272          13.830180   \n",
       "997        1000   99.999531   99.999531   1.000000          13.380863   \n",
       "\n",
       "     MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex       qed        SPS  \\\n",
       "0          5.094096           0.387225        0.387225  0.417362  42.928571   \n",
       "1          3.961417           0.533868        0.533868  0.462473  45.214286   \n",
       "2          2.627117           0.543231        0.543231  0.260923  42.187500   \n",
       "3          5.097360           0.390603        0.390603  0.377846  41.862069   \n",
       "4          5.150510           0.270476        0.270476  0.429038  36.514286   \n",
       "..              ...                ...             ...       ...        ...   \n",
       "993       12.934891           0.048029       -0.476142  0.382752  49.133333   \n",
       "994       13.635345           0.030329       -0.699355  0.369425  44.542857   \n",
       "995       13.991690           0.026535       -0.650790  0.284923  41.973684   \n",
       "996       13.830180           0.146522       -1.408652  0.381559  39.000000   \n",
       "997       13.380863           0.002425       -0.447978  0.452565  48.580645   \n",
       "\n",
       "       MolWt  HeavyAtomMolWt  ExactMolWt  NumValenceElectrons  \\\n",
       "0    384.652         340.300  384.350449                  158   \n",
       "1    388.684         340.300  388.381750                  162   \n",
       "2    446.808         388.344  446.458903                  186   \n",
       "3    398.679         352.311  398.366099                  164   \n",
       "4    466.713         424.377  466.334799                  184   \n",
       "..       ...             ...         ...                  ...   \n",
       "993  414.542         380.270  414.240624                  164   \n",
       "994  485.621         446.309  485.277738                  192   \n",
       "995  545.742         502.398  545.281109                  210   \n",
       "996  522.635         480.299  522.282883                  208   \n",
       "997  426.597         388.293  426.277010                  170   \n",
       "\n",
       "     NumRadicalElectrons  MaxPartialCharge  MinPartialCharge  \\\n",
       "0                      0          0.038844         -0.293526   \n",
       "1                      0          0.012887         -0.313407   \n",
       "2                      0          0.094802         -0.325573   \n",
       "3                      0          0.038844         -0.293526   \n",
       "4                      0          0.062897         -0.257239   \n",
       "..                   ...               ...               ...   \n",
       "993                    0          0.317890         -0.468587   \n",
       "994                    0          0.327562         -0.467493   \n",
       "995                    0          0.327887         -0.467485   \n",
       "996                    0          0.312509         -0.468755   \n",
       "997                    0          0.311311         -0.468587   \n",
       "\n",
       "     MaxAbsPartialCharge  MinAbsPartialCharge  FpDensityMorgan1  \\\n",
       "0               0.293526             0.038844          0.642857   \n",
       "1               0.313407             0.012887          0.607143   \n",
       "2               0.325573             0.094802          0.562500   \n",
       "3               0.293526             0.038844          0.620690   \n",
       "4               0.257239             0.062897          0.600000   \n",
       "..                   ...                  ...               ...   \n",
       "993             0.468587             0.317890          1.133333   \n",
       "994             0.467493             0.327562          1.085714   \n",
       "995             0.467485             0.327887          1.157895   \n",
       "996             0.468755             0.312509          0.756757   \n",
       "997             0.468587             0.311311          1.064516   \n",
       "\n",
       "     FpDensityMorgan2  FpDensityMorgan3  BCUT2D_MWHI  BCUT2D_MWLOW  \\\n",
       "0            1.035714          1.321429    14.822266      9.700470   \n",
       "1            1.000000          1.285714    14.975110      9.689226   \n",
       "2            0.906250          1.156250    15.353938      9.681293   \n",
       "3            1.000000          1.310345    14.821216      9.700497   \n",
       "4            0.971429          1.257143    14.831112      9.700386   \n",
       "..                ...               ...          ...           ...   \n",
       "993          1.866667          2.533333    16.586886      9.344314   \n",
       "994          1.800000          2.457143    16.586914      9.343622   \n",
       "995          1.894737          2.552632    32.166365      9.343613   \n",
       "996          1.351351          1.864865    16.540061      9.364015   \n",
       "997          1.774194          2.451613    16.525216      9.330327   \n",
       "\n",
       "     BCUT2D_CHGHI  BCUT2D_CHGLO  BCUT2D_LOGPHI  BCUT2D_LOGPLOW  BCUT2D_MRHI  \\\n",
       "0        2.600532     -2.343082       2.644698       -2.322229     5.944519   \n",
       "1        2.614066     -2.394690       2.658342       -2.444817     5.134527   \n",
       "2        2.665274     -2.477203       2.679014       -2.565224     5.117187   \n",
       "3        2.600529     -2.342885       2.644709       -2.322030     5.944502   \n",
       "4        2.602486     -2.342009       2.648473       -2.318893     5.963448   \n",
       "..            ...           ...            ...             ...          ...   \n",
       "993      2.726237     -2.677345       2.739076       -2.646743     5.980114   \n",
       "994      2.725543     -2.679467       2.738755       -2.655659     5.980828   \n",
       "995      2.725818     -2.679527       2.738943       -2.656447     7.980998   \n",
       "996      2.730109     -2.652209       2.704027       -2.678553     5.950258   \n",
       "997      2.704274     -2.696212       2.737481       -2.668850     5.978085   \n",
       "\n",
       "     BCUT2D_MRLOW    AvgIpc  BalabanJ      BertzCT       Chi0      Chi0n  \\\n",
       "0        0.193481  3.150503  1.164038   611.920301  20.208896  19.534409   \n",
       "1        0.120322  3.150503  1.080362   516.780124  20.208896  19.794682   \n",
       "2       -0.922902  3.214947  1.219066   643.620154  23.794682  23.689110   \n",
       "3        0.193510  3.179270  1.120513   626.651366  20.916003  20.241516   \n",
       "4        0.193687  3.337074  1.136678  1101.164252  24.639617  22.617677   \n",
       "..            ...       ...       ...          ...        ...        ...   \n",
       "993     -0.196385  3.023764  1.646946   857.600295  21.637464  18.825334   \n",
       "994     -0.187625  3.130958  1.535171  1016.917688  25.499271  21.810933   \n",
       "995     -0.187687  3.204255  1.493776  1070.961298  27.620591  23.633394   \n",
       "996     -0.225309  2.887043  2.325807   957.299494  27.921921  23.380977   \n",
       "997     -0.201381  2.740713  1.651446   870.462214  22.344571  19.831299   \n",
       "\n",
       "         Chi0v       Chi1      Chi1n      Chi1v      Chi2n      Chi2v  \\\n",
       "0    19.534409  13.127794  12.204226  12.204226  12.058078  12.058078   \n",
       "1    19.794682  13.127794  12.595754  12.595754  12.648545  12.648545   \n",
       "2    23.689110  14.595754  14.249005  14.249005  15.671216  15.671216   \n",
       "3    20.241516  13.627794  12.704226  12.704226  12.411631  12.411631   \n",
       "4    22.617677  16.526773  13.868825  13.868825  13.613700  13.613700   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "993  18.825334  14.097861  11.665192  11.665192  11.409461  11.409461   \n",
       "994  21.810933  16.402391  13.274017  13.274017  12.636189  12.636189   \n",
       "995  24.449891  17.940396  14.301838  15.695685  13.248561  14.234160   \n",
       "996  23.380977  17.309003  13.174959  13.174959  11.893254  11.893254   \n",
       "997  19.831299  14.597861  12.464050  12.464050  12.101953  12.101953   \n",
       "\n",
       "         Chi3n      Chi3v      Chi4n      Chi4v  HallKierAlpha           Ipc  \\\n",
       "0    10.695991  10.695991   7.340247   7.340247          -0.66  2.187750e+06   \n",
       "1    11.473090  11.473090   8.180905   8.180905          -0.08  2.187750e+06   \n",
       "2    13.402236  13.402236  10.140303  10.140303          -0.08  8.610751e+06   \n",
       "3    10.945991  10.945991   7.517023   7.517023          -0.66  3.572142e+06   \n",
       "4    11.833480  11.833480   8.119076   8.119076          -2.22  1.053758e+08   \n",
       "..         ...        ...        ...        ...            ...           ...   \n",
       "993  10.058026  10.058026   8.981266   8.981266          -1.65  6.242348e+06   \n",
       "994  10.827369  10.827369   9.372775   9.372775          -2.18  5.897229e+07   \n",
       "995  11.326709  11.970659   9.725583  10.196987          -1.83  2.627956e+08   \n",
       "996  10.158570  10.158570   8.609327   8.609327          -2.45  7.702780e+07   \n",
       "997  10.667206  10.667206   9.563076   9.563076          -1.45  9.086032e+06   \n",
       "\n",
       "        Kappa1     Kappa2    Kappa3   LabuteASA  PEOE_VSA1  PEOE_VSA10  \\\n",
       "0    20.606247   6.947534  2.868737  173.630124   0.000000    0.000000   \n",
       "1    21.163454   7.257648  3.027177  174.939204  10.633577    0.000000   \n",
       "2    25.026112   7.709373  3.470070  201.238858   8.966062    0.000000   \n",
       "3    21.567454   7.485204  3.263848  179.995066   0.000000    0.000000   \n",
       "4    23.194917   7.639211  3.345855  211.919602   0.000000    0.000000   \n",
       "..         ...        ...       ...         ...        ...         ...   \n",
       "993  20.263719   6.198453  2.219273  178.490760   9.473726    0.000000   \n",
       "994  24.511583   7.908743  3.147136  207.296970  14.790515    6.041841   \n",
       "995  27.726151   9.668673  3.822745  230.149965  14.790515    6.041841   \n",
       "996  29.111081  10.369092  4.164473  218.836986  18.947452    5.783245   \n",
       "997  21.398566   6.776167  2.566599  186.107099   4.736863   11.566490   \n",
       "\n",
       "     PEOE_VSA11  PEOE_VSA12  PEOE_VSA13  PEOE_VSA14  PEOE_VSA2  PEOE_VSA3  \\\n",
       "0           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "1           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "2           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "3           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "4           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "..          ...         ...         ...         ...        ...        ...   \n",
       "993         0.0     0.00000         0.0   17.907916  14.383612   0.000000   \n",
       "994         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "995         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "996         0.0     0.00000         0.0   23.877221  23.972686   0.000000   \n",
       "997         0.0     0.00000         0.0    5.969305  14.383612   0.000000   \n",
       "\n",
       "     PEOE_VSA4  PEOE_VSA5  PEOE_VSA6   PEOE_VSA7  PEOE_VSA8  PEOE_VSA9  \\\n",
       "0     0.000000        0.0  54.384066   74.032366  35.342864   0.000000   \n",
       "1     0.000000        0.0  54.384066   97.951860  12.083682   0.000000   \n",
       "2     0.000000        0.0  41.542423   74.032366  23.671624  53.363882   \n",
       "3     0.000000        0.0  60.804888   74.032366  35.342864   0.000000   \n",
       "4     0.000000        0.0  65.807891  103.003916  22.253351  11.374773   \n",
       "..         ...        ...        ...         ...        ...        ...   \n",
       "993   0.000000        0.0  38.841158   68.114460   5.414990  24.360600   \n",
       "994   0.000000        0.0  45.764895   68.114460  10.829981  18.945610   \n",
       "995  11.761885        0.0  45.764895   79.620167  10.829981  18.945610   \n",
       "996   0.000000        0.0  27.192033   56.278648  11.835812  51.104983   \n",
       "997   0.000000        0.0  38.841158   68.114460  30.092446  12.524788   \n",
       "\n",
       "      SMR_VSA1  SMR_VSA10  SMR_VSA2   SMR_VSA3   SMR_VSA4    SMR_VSA5  \\\n",
       "0     0.000000  11.423370       0.0   0.000000  43.480583  105.750639   \n",
       "1     0.000000   0.000000       0.0  10.633577  33.495774  117.834321   \n",
       "2     8.966062   0.000000       0.0   0.000000  33.495774  117.834321   \n",
       "3     0.000000  11.423370       0.0   0.000000  43.480583  112.171461   \n",
       "4     0.000000  22.798143       0.0   0.000000  43.480583   86.488175   \n",
       "..         ...        ...       ...        ...        ...         ...   \n",
       "993  23.857337  17.907916       0.0   0.000000  51.752408   66.219879   \n",
       "994  28.651875  23.815096       0.0   5.316789  51.752408   79.185457   \n",
       "995  28.651875  35.576981       0.0   5.316789  51.752408   78.682541   \n",
       "996  42.920138  29.660466       0.0   0.000000  51.752408   66.219879   \n",
       "997  19.120475  17.535795       0.0   0.000000  51.752408   79.061522   \n",
       "\n",
       "      SMR_VSA6   SMR_VSA7  SMR_VSA8  SMR_VSA9  SlogP_VSA1  SlogP_VSA10  \\\n",
       "0    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "1    13.089513   0.000000         0       0.0   10.633577     0.000000   \n",
       "2    41.280201   0.000000         0       0.0    0.000000     0.000000   \n",
       "3    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "4     0.000000  59.657840         0       0.0    0.000000    11.374773   \n",
       "..         ...        ...       ...       ...         ...          ...   \n",
       "993   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "994   7.109798  11.649125         0       0.0    5.316789     0.000000   \n",
       "995  19.118420  11.649125         0       0.0    5.316789     0.000000   \n",
       "996  28.439190   0.000000         0       0.0    0.000000     0.000000   \n",
       "997   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "\n",
       "     SlogP_VSA11  SlogP_VSA12  SlogP_VSA2  SlogP_VSA3  SlogP_VSA4  SlogP_VSA5  \\\n",
       "0            0.0     0.000000   24.512883    0.000000   33.495774  105.750639   \n",
       "1            0.0     0.000000   25.173194    0.000000   33.495774  105.750639   \n",
       "2            0.0     0.000000   62.329944    0.000000   33.495774  105.750639   \n",
       "3            0.0     0.000000   24.512883    0.000000   33.495774  112.171461   \n",
       "4            0.0     0.000000   11.423370    6.420822   33.495774   91.194256   \n",
       "..           ...          ...         ...         ...         ...         ...   \n",
       "993          0.0     0.000000   25.017713   23.857337   51.752408   66.219879   \n",
       "994          0.0     0.000000   36.966734   28.651875   51.752408   73.143616   \n",
       "995          0.0    11.761885   48.975357   28.651875   51.752408   72.640700   \n",
       "996          0.0     0.000000   58.099656   42.920138   51.752408   66.219879   \n",
       "997          0.0     0.000000   24.645593   19.120475   51.752408   79.061522   \n",
       "\n",
       "     SlogP_VSA6  SlogP_VSA7  SlogP_VSA8  SlogP_VSA9    TPSA  EState_VSA1  \\\n",
       "0      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "1      0.000000         0.0         0.0           0   24.06     0.000000   \n",
       "2      0.000000         0.0         0.0           0    0.00     0.000000   \n",
       "3      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "4     58.515746         0.0         0.0           0   24.72     0.000000   \n",
       "..          ...         ...         ...         ...     ...          ...   \n",
       "993   11.649125         0.0         0.0           0   69.67     5.414990   \n",
       "994   11.649125         0.0         0.0           0   98.77    23.344043   \n",
       "995   11.649125         0.0         0.0           0   98.77    28.759033   \n",
       "996    0.000000         0.0         0.0           0  122.27    63.742418   \n",
       "997   11.649125         0.0         0.0           0   60.44     5.414990   \n",
       "\n",
       "     EState_VSA10  EState_VSA11  EState_VSA2  EState_VSA3  EState_VSA4  \\\n",
       "0        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "1        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "2        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "3        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "4        0.000000           0.0    10.829981    10.829981    29.631406   \n",
       "..            ...           ...          ...          ...          ...   \n",
       "993     14.383612           0.0    52.409521    11.835812    38.524930   \n",
       "994     19.178149           0.0    52.347395     5.917906    38.524930   \n",
       "995     19.178149           0.0    46.932405    12.338728    44.277783   \n",
       "996     23.972686           0.0    30.512100    19.262465     6.420822   \n",
       "997     14.383612           0.0    40.470910    36.243945    38.524930   \n",
       "\n",
       "     EState_VSA5  EState_VSA6  EState_VSA7  EState_VSA8  EState_VSA9  \\\n",
       "0      64.208216    11.423370     0.000000    41.542423     9.984809   \n",
       "1      77.297729     0.000000     0.000000    52.176000     0.000000   \n",
       "2      86.263791     0.000000     0.000000    69.733111     0.000000   \n",
       "3      70.629038    11.423370     0.000000    41.542423     9.984809   \n",
       "4      61.075203     0.000000     0.000000    90.073360     9.984809   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    12.682902     0.000000     0.000000    33.770969     9.473726   \n",
       "994    12.682902     6.923737     0.000000    39.087758     9.473726   \n",
       "995    12.682902    11.761885     6.255769    39.087758     9.473726   \n",
       "996    28.439190    13.847474     6.923737     6.923737    18.947452   \n",
       "997    12.682902     0.000000     0.000000    33.770969     4.736863   \n",
       "\n",
       "     VSA_EState1  VSA_EState10  VSA_EState2  VSA_EState3  VSA_EState4  \\\n",
       "0       0.000000      0.000000    10.188192     0.000000     4.807589   \n",
       "1       0.000000      0.000000     0.000000     7.922833     2.153503   \n",
       "2       2.517630      0.000000     0.000000     0.000000     2.184127   \n",
       "3       0.000000      0.000000    10.194720     0.000000     4.827852   \n",
       "4       0.000000      0.000000    10.301020     0.000000     9.071783   \n",
       "..           ...           ...          ...          ...          ...   \n",
       "993    10.503509      0.000000    38.515343     0.000000     0.498752   \n",
       "994    10.086396      0.000000    51.516353     2.923049     0.162524   \n",
       "995    10.305031      1.639399    52.527620     3.081660     0.139799   \n",
       "996    20.885832      0.000000    67.303382     0.000000    -2.785593   \n",
       "997     5.298868      0.000000    39.472742     0.000000     0.776999   \n",
       "\n",
       "     VSA_EState5  VSA_EState6  VSA_EState7  VSA_EState8  VSA_EState9  \\\n",
       "0       1.764908     0.000000    13.258223    16.981087     0.000000   \n",
       "1       1.914377     1.536674    14.135381    17.670565     0.000000   \n",
       "2       1.930720     1.738402    14.491619    18.287216    10.183618   \n",
       "3       1.769975     0.000000    14.695439    17.012013     0.000000   \n",
       "4       1.605178    17.869058     8.627311    14.692318     0.000000   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    -0.405436     0.000000     7.985276     8.824371     1.494852   \n",
       "994    -1.300354    -0.699355     7.521836    10.378794     1.327425   \n",
       "995    -0.487671    -0.650790    10.055493     8.774745     1.364715   \n",
       "996    -6.848660     0.000000     2.955837     7.488627     5.083909   \n",
       "997     1.605640     0.000000     9.247616     8.999949     1.514853   \n",
       "\n",
       "     FractionCSP3  HeavyAtomCount  NHOHCount  NOCount  \\\n",
       "0        0.923077              28          0        2   \n",
       "1        1.000000              28          2        2   \n",
       "2        1.000000              32          0        2   \n",
       "3        0.925926              29          0        2   \n",
       "4        0.575758              35          0        2   \n",
       "..            ...             ...        ...      ...   \n",
       "993      0.800000              30          0        5   \n",
       "994      0.785714              35          1        7   \n",
       "995      0.800000              38          1        7   \n",
       "996      0.821429              37          0        9   \n",
       "997      0.814815              31          0        4   \n",
       "\n",
       "     NumAliphaticCarbocycles  NumAliphaticHeterocycles  NumAliphaticRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        5                         1                  6   \n",
       "994                        5                         1                  6   \n",
       "995                        5                         1                  6   \n",
       "996                        3                         0                  3   \n",
       "997                        6                         0                  6   \n",
       "\n",
       "     NumAromaticCarbocycles  NumAromaticHeterocycles  NumAromaticRings  \\\n",
       "0                         0                        0                 0   \n",
       "1                         0                        0                 0   \n",
       "2                         0                        0                 0   \n",
       "3                         0                        0                 0   \n",
       "4                         2                        0                 2   \n",
       "..                      ...                      ...               ...   \n",
       "993                       0                        0                 0   \n",
       "994                       0                        0                 0   \n",
       "995                       0                        0                 0   \n",
       "996                       0                        0                 0   \n",
       "997                       0                        0                 0   \n",
       "\n",
       "     NumHAcceptors  NumHDonors  NumHeteroatoms  NumRotatableBonds  \\\n",
       "0                2           0               2                  7   \n",
       "1                2           2               2                  9   \n",
       "2                0           0               2                  9   \n",
       "3                2           0               2                  8   \n",
       "4                2           0               2                  4   \n",
       "..             ...         ...             ...                ...   \n",
       "993              5           0               5                  2   \n",
       "994              6           1               7                  4   \n",
       "995              7           1               8                  7   \n",
       "996              9           0               9                  6   \n",
       "997              4           0               4                  2   \n",
       "\n",
       "     NumSaturatedCarbocycles  NumSaturatedHeterocycles  NumSaturatedRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        3                         1                  4   \n",
       "994                        3                         1                  4   \n",
       "995                        3                         1                  4   \n",
       "996                        3                         0                  3   \n",
       "997                        4                         0                  4   \n",
       "\n",
       "     RingCount  MolLogP     MolMR  fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  \\\n",
       "0            4   7.1212  121.5300          0         0                0   \n",
       "1            4   6.1556  120.5074          0         0                0   \n",
       "2            4   7.1292  138.4528          0         0                0   \n",
       "3            4   7.5113  126.1470          0         0                0   \n",
       "4            6   9.1148  148.3380          0         0                0   \n",
       "..         ...      ...       ...        ...       ...              ...   \n",
       "993          6   4.3002  109.8350          0         0                0   \n",
       "994          6   3.8049  127.4397          0         0                0   \n",
       "995          6   4.5381  144.7647          0         0                0   \n",
       "996          3   3.3649  131.7080          0         0                0   \n",
       "997          6   5.1488  117.9840          0         0                0   \n",
       "\n",
       "     fr_ArN  fr_Ar_COO  fr_Ar_N  fr_Ar_NH  fr_Ar_OH  fr_COO  fr_COO2  fr_C_O  \\\n",
       "0         0          0        0         0         0       0        0       0   \n",
       "1         0          0        0         0         0       0        0       0   \n",
       "2         0          0        0         0         0       0        0       0   \n",
       "3         0          0        0         0         0       0        0       0   \n",
       "4         0          0        0         0         0       0        0       0   \n",
       "..      ...        ...      ...       ...       ...     ...      ...     ...   \n",
       "993       0          0        0         0         0       0        0       3   \n",
       "994       0          0        0         0         0       0        0       4   \n",
       "995       0          0        0         0         0       0        0       4   \n",
       "996       0          0        0         0         0       0        0       5   \n",
       "997       0          0        0         0         0       0        0       3   \n",
       "\n",
       "     fr_C_O_noCOO  fr_C_S  fr_HOCCN  fr_Imine  fr_NH0  fr_NH1  fr_NH2  fr_N_O  \\\n",
       "0               0       0         0         2       2       0       0       0   \n",
       "1               0       0         0         0       0       2       0       0   \n",
       "2               0       0         0         0       2       0       0       0   \n",
       "3               0       0         0         2       2       0       0       0   \n",
       "4               0       0         0         2       2       0       0       0   \n",
       "..            ...     ...       ...       ...     ...     ...     ...     ...   \n",
       "993             3       0         0         0       0       0       0       0   \n",
       "994             4       0         0         0       0       1       0       0   \n",
       "995             4       0         0         0       0       1       0       0   \n",
       "996             5       0         0         0       0       0       0       0   \n",
       "997             3       0         0         0       0       0       0       0   \n",
       "\n",
       "     fr_Ndealkylation1  fr_Ndealkylation2  fr_Nhpyrrole  fr_SH  fr_aldehyde  \\\n",
       "0                    0                  0             0      0            0   \n",
       "1                    0                  0             0      0            0   \n",
       "2                    0                  0             0      0            0   \n",
       "3                    0                  0             0      0            0   \n",
       "4                    0                  0             0      0            0   \n",
       "..                 ...                ...           ...    ...          ...   \n",
       "993                  0                  0             0      0            0   \n",
       "994                  0                  0             0      0            0   \n",
       "995                  0                  0             0      0            0   \n",
       "996                  0                  0             0      0            0   \n",
       "997                  0                  0             0      0            0   \n",
       "\n",
       "     fr_alkyl_carbamate  fr_alkyl_halide  fr_allylic_oxid  fr_amide  \\\n",
       "0                     0                0                0         0   \n",
       "1                     0                0                0         0   \n",
       "2                     0                0                0         0   \n",
       "3                     0                0                0         0   \n",
       "4                     0                0                0         0   \n",
       "..                  ...              ...              ...       ...   \n",
       "993                   0                0                2         0   \n",
       "994                   0                0                2         1   \n",
       "995                   0                0                2         1   \n",
       "996                   0                0                0         0   \n",
       "997                   0                0                2         0   \n",
       "\n",
       "     fr_amidine  fr_aniline  fr_aryl_methyl  fr_azide  fr_azo  fr_barbitur  \\\n",
       "0             0           0               0         0       0            0   \n",
       "1             0           0               0         0       0            0   \n",
       "2             0           0               0         0       0            0   \n",
       "3             0           0               0         0       0            0   \n",
       "4             0           0               0         0       0            0   \n",
       "..          ...         ...             ...       ...     ...          ...   \n",
       "993           0           0               0         0       0            0   \n",
       "994           0           0               0         0       0            0   \n",
       "995           0           0               0         0       0            0   \n",
       "996           0           0               0         0       0            0   \n",
       "997           0           0               0         0       0            0   \n",
       "\n",
       "     fr_benzene  fr_benzodiazepine  fr_bicyclic  fr_diazo  fr_dihydropyridine  \\\n",
       "0             0                  0            4         0                   0   \n",
       "1             0                  0            4         0                   0   \n",
       "2             0                  0            4         0                   0   \n",
       "3             0                  0            4         0                   0   \n",
       "4             2                  0            4         0                   0   \n",
       "..          ...                ...          ...       ...                 ...   \n",
       "993           0                  0            1         0                   0   \n",
       "994           0                  0            1         0                   0   \n",
       "995           0                  0            1         0                   0   \n",
       "996           0                  0            3         0                   0   \n",
       "997           0                  0            1         0                   0   \n",
       "\n",
       "     fr_epoxide  fr_ester  fr_ether  fr_furan  fr_guanido  fr_halogen  \\\n",
       "0             0         0         0         0           0           0   \n",
       "1             0         0         0         0           0           0   \n",
       "2             0         0         0         0           0           0   \n",
       "3             0         0         0         0           0           0   \n",
       "4             0         0         0         0           0           0   \n",
       "..          ...       ...       ...       ...         ...         ...   \n",
       "993           0         3         2         0           0           0   \n",
       "994           0         3         2         0           0           0   \n",
       "995           0         3         2         0           0           0   \n",
       "996           0         4         4         0           0           0   \n",
       "997           0         1         1         0           0           0   \n",
       "\n",
       "     fr_hdrzine  fr_hdrzone  fr_imidazole  fr_imide  fr_isocyan  \\\n",
       "0             0           0             0         0           0   \n",
       "1             0           0             0         0           0   \n",
       "2             0           0             0         0           0   \n",
       "3             0           0             0         0           0   \n",
       "4             0           0             0         0           0   \n",
       "..          ...         ...           ...       ...         ...   \n",
       "993           0           0             0         0           0   \n",
       "994           0           0             0         0           0   \n",
       "995           0           0             0         0           0   \n",
       "996           0           0             0         0           0   \n",
       "997           0           0             0         0           0   \n",
       "\n",
       "     fr_isothiocyan  fr_ketone  fr_ketone_Topliss  fr_lactam  fr_lactone  \\\n",
       "0                 0          0                  0          0           0   \n",
       "1                 0          0                  0          0           0   \n",
       "2                 0          0                  0          0           0   \n",
       "3                 0          0                  0          0           0   \n",
       "4                 0          0                  0          0           0   \n",
       "..              ...        ...                ...        ...         ...   \n",
       "993               0          0                  0          0           2   \n",
       "994               0          0                  0          0           2   \n",
       "995               0          0                  0          0           2   \n",
       "996               0          1                  1          0           0   \n",
       "997               0          2                  2          0           0   \n",
       "\n",
       "     fr_methoxy  fr_morpholine  fr_nitrile  fr_nitro  fr_nitro_arom  \\\n",
       "0             0              0           0         0              0   \n",
       "1             0              0           0         0              0   \n",
       "2             0              0           0         0              0   \n",
       "3             0              0           0         0              0   \n",
       "4             0              0           0         0              0   \n",
       "..          ...            ...         ...       ...            ...   \n",
       "993           1              0           0         0              0   \n",
       "994           1              0           0         0              0   \n",
       "995           1              0           0         0              0   \n",
       "996           4              0           0         0              0   \n",
       "997           1              0           0         0              0   \n",
       "\n",
       "     fr_nitro_arom_nonortho  fr_nitroso  fr_oxazole  fr_oxime  \\\n",
       "0                         0           0           0         0   \n",
       "1                         0           0           0         0   \n",
       "2                         0           0           0         0   \n",
       "3                         0           0           0         0   \n",
       "4                         0           0           0         0   \n",
       "..                      ...         ...         ...       ...   \n",
       "993                       0           0           0         0   \n",
       "994                       0           0           0         0   \n",
       "995                       0           0           0         0   \n",
       "996                       0           0           0         0   \n",
       "997                       0           0           0         0   \n",
       "\n",
       "     fr_para_hydroxylation  fr_phenol  fr_phenol_noOrthoHbond  fr_phos_acid  \\\n",
       "0                        0          0                       0             0   \n",
       "1                        0          0                       0             0   \n",
       "2                        0          0                       0             0   \n",
       "3                        0          0                       0             0   \n",
       "4                        0          0                       0             0   \n",
       "..                     ...        ...                     ...           ...   \n",
       "993                      0          0                       0             0   \n",
       "994                      0          0                       0             0   \n",
       "995                      0          0                       0             0   \n",
       "996                      0          0                       0             0   \n",
       "997                      0          0                       0             0   \n",
       "\n",
       "     fr_phos_ester  fr_piperdine  fr_piperzine  fr_priamide  fr_prisulfonamd  \\\n",
       "0                0             0             0            0                0   \n",
       "1                0             0             0            0                0   \n",
       "2                0             0             0            0                0   \n",
       "3                0             0             0            0                0   \n",
       "4                0             0             0            0                0   \n",
       "..             ...           ...           ...          ...              ...   \n",
       "993              0             0             0            0                0   \n",
       "994              0             0             0            0                0   \n",
       "995              0             0             0            0                0   \n",
       "996              0             0             0            0                0   \n",
       "997              0             0             0            0                0   \n",
       "\n",
       "     fr_pyridine  fr_quatN  fr_sulfide  fr_sulfonamd  fr_sulfone  \\\n",
       "0              0         0           0             0           0   \n",
       "1              0         0           0             0           0   \n",
       "2              0         2           0             0           0   \n",
       "3              0         0           0             0           0   \n",
       "4              0         0           0             0           0   \n",
       "..           ...       ...         ...           ...         ...   \n",
       "993            0         0           0             0           0   \n",
       "994            0         0           0             0           0   \n",
       "995            0         0           1             0           0   \n",
       "996            0         0           0             0           0   \n",
       "997            0         0           0             0           0   \n",
       "\n",
       "     fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  \\\n",
       "0                    0             0            0            0             0   \n",
       "1                    0             0            0            0             0   \n",
       "2                    0             0            0            0             0   \n",
       "3                    0             0            0            0             0   \n",
       "4                    0             0            0            0             0   \n",
       "..                 ...           ...          ...          ...           ...   \n",
       "993                  0             0            0            0             0   \n",
       "994                  0             0            0            0             0   \n",
       "995                  0             0            0            0             0   \n",
       "996                  0             0            0            0             0   \n",
       "997                  0             0            0            0             0   \n",
       "\n",
       "     fr_unbrch_alkane  fr_urea  \n",
       "0                   3        0  \n",
       "1                   3        0  \n",
       "2                   3        0  \n",
       "3                   4        0  \n",
       "4                   0        0  \n",
       "..                ...      ...  \n",
       "993                 0        0  \n",
       "994                 0        0  \n",
       "995                 0        0  \n",
       "996                 0        0  \n",
       "997                 0        0  \n",
       "\n",
       "[998 rows x 214 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a946941faca937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:13.720652Z",
     "start_time": "2025-05-28T16:57:13.716638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем немнформативный признак\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22040ef150ffc8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.281372Z",
     "start_time": "2025-05-28T16:57:14.276214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем пропуски\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0bb2972cbaa906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.915613Z",
     "start_time": "2025-05-28T16:57:14.885015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем дубликаты\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7089b7fa5f494e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:16.413719Z",
     "start_time": "2025-05-28T16:57:16.409041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(966, 213)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4529ddf3083cdcc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:00:51.840374Z",
     "start_time": "2025-05-28T17:00:51.836087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вычисляем медиану столбца\n",
    "median_value_ic50 = df['IC50, mM'].median()\n",
    "median_value_cc50 = df['CC50, mM'].median()\n",
    "median_value_si = df['SI'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "400a82221f828fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:21.052804Z",
     "start_time": "2025-05-28T17:02:21.047421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем целевые признаки\n",
    "df['CC50, mM'] = (df['CC50, mM'] > median_value_cc50).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502ea21feffa4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:05:44.334975Z",
     "start_time": "2025-05-28T17:05:44.331153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем данные и целевую переменную\n",
    "X = df.drop(columns = ['IC50, mM', 'CC50, mM', 'SI'])\n",
    "y = df['CC50, mM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae82771-5320-4ecd-a016-d937810c06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество выбросов по признакам:\n",
      "IC50, mM              0\n",
      "CC50, mM              0\n",
      "SI                    0\n",
      "MaxAbsEStateIndex    60\n",
      "MaxEStateIndex       60\n",
      "                     ..\n",
      "fr_thiazole          52\n",
      "fr_thiocyan           0\n",
      "fr_thiophene         68\n",
      "fr_unbrch_alkane     49\n",
      "fr_urea               7\n",
      "Length: 213, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers(df, alpha=0.05, method='iqr', normality_test='shapiro', add_sum_column=False):\n",
    "    \"\"\"\n",
    "    Обнаружение выбросов в DataFrame с использованием различных статистических методов.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Входной DataFrame с данными для анализа\n",
    "    alpha : float, по умолчанию 0.05\n",
    "        Уровень значимости для тестов на нормальность\n",
    "    method : str, по умолчанию 'iqr'\n",
    "        Метод обнаружения выбросов для ненормальных данных:\n",
    "        - 'iqr' - метод межквартильного размаха\n",
    "        - 'zscore' - модифицированный Z-score\n",
    "    normality_test : str, по умолчанию 'shapiro'\n",
    "        Тест на нормальность распределения:\n",
    "        - 'shapiro' - тест Шапиро-Уилка\n",
    "        - 'normaltest' - тест на нормальность D'Agostino-Pearson\n",
    "        - 'anderson' - тест Андерсона-Дарлинга\n",
    "    add_sum_column : bool, по умолчанию False\n",
    "        Если True, добавляет столбец с общим количеством выбросов для каждой строки\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        DataFrame с булевыми значениями, где True указывает на выброс\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем DataFrame для хранения результатов (по умолчанию все значения False)\n",
    "    outliers = pd.DataFrame(False, index=df.index, columns=df.columns)\n",
    "    \n",
    "    # Анализируем каждый столбец отдельно\n",
    "    for col in df.columns:\n",
    "        # Удаляем пропущенные значения для текущего столбца\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Если в столбце меньше 3 значений, пропускаем его\n",
    "        if len(data) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Проверяем нормальность распределения\n",
    "        normal = False  # Флаг нормальности распределения\n",
    "        \n",
    "        try:\n",
    "            # Выбираем тест на нормальность в зависимости от параметра normality_test\n",
    "            if normality_test == 'shapiro':\n",
    "                # Тест Шапиро-Уилка (подходит для небольших выборок < 5000)\n",
    "                _, p = stats.shapiro(data)\n",
    "                normal = p > alpha  # Если p-value > alpha, распределение считается нормальным\n",
    "                \n",
    "            elif normality_test == 'normaltest':\n",
    "                # Тест D'Agostino-Pearson (работает для выборок > 20)\n",
    "                _, p = stats.normaltest(data)\n",
    "                normal = p > alpha\n",
    "                \n",
    "            elif normality_test == 'anderson':\n",
    "                # Тест Андерсона-Дарлинга (более строгий)\n",
    "                result = stats.anderson(data)\n",
    "                # Сравниваем статистику с критическим значением для выбранного alpha\n",
    "                normal = result.statistic < result.critical_values[np.where(result.significance_level == int(alpha*100))[0][0]]\n",
    "        except:\n",
    "            # В случае ошибки в тесте считаем распределение ненормальным\n",
    "            pass\n",
    "        \n",
    "        # Если распределение нормальное, используем стандартный Z-score\n",
    "        if normal:\n",
    "            z = np.abs(stats.zscore(data))  # Вычисляем Z-оценки\n",
    "            outliers.loc[data.index, col] = z > 3  # Выбросы > 3 стандартных отклонений\n",
    "            \n",
    "        # Для ненормальных распределений используем выбранный метод\n",
    "        else:\n",
    "            if method == 'iqr':\n",
    "                # Метод межквартильного размаха (IQR)\n",
    "                q1 = data.quantile(0.25)  # Первый квартиль (25-й перцентиль)\n",
    "                q3 = data.quantile(0.75)  # Третий квартиль (75-й перцентиль)\n",
    "                iqr = q3 - q1  # Межквартильный размах\n",
    "                \n",
    "                # Границы для выбросов\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                # Отмечаем выбросы\n",
    "                outliers.loc[data.index, col] = (data < lower_bound) | (data > upper_bound)\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                # Модифицированный Z-score (более устойчивый к выбросам)\n",
    "                median = data.median()  # Медиана вместо среднего\n",
    "                mad = stats.median_abs_deviation(data, scale='normal')  # Медианное абсолютное отклонение\n",
    "                modified_z = np.abs(0.6745 * (data - median) / mad)  # Модифицированный Z-score\n",
    "                \n",
    "                # Выбросы при modified_z > 3.5\n",
    "                outliers.loc[data.index, col] = modified_z > 3.5\n",
    "    \n",
    "    # Добавляем столбец с суммой выбросов по строкам, если нужно\n",
    "    if add_sum_column:\n",
    "        outliers['outliers_sum'] = outliers.sum(axis=1)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Находим выбросы\n",
    "outliers = detect_outliers(df)\n",
    "\n",
    "# Выводим количество выбросов по каждому признаку\n",
    "print(\"\\nКоличество выбросов по признакам:\")\n",
    "print(outliers.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a0ea30522688a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:06:25.366761Z",
     "start_time": "2025-05-28T17:06:15.792777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Обучение Logistic Regression...\n",
      " Logistic Regression\n",
      "Accuracy: 0.6959 | Precision: 0.6964 | Recall: 0.6959 | F1: 0.6957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69        97\n",
      "           1       0.69      0.72      0.70        97\n",
      "\n",
      "    accuracy                           0.70       194\n",
      "   macro avg       0.70      0.70      0.70       194\n",
      "weighted avg       0.70      0.70      0.70       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение SVM...\n",
      " SVM\n",
      "Accuracy: 0.7165 | Precision: 0.7171 | Recall: 0.7165 | F1: 0.7163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.69      0.71        97\n",
      "           1       0.71      0.74      0.72        97\n",
      "\n",
      "    accuracy                           0.72       194\n",
      "   macro avg       0.72      0.72      0.72       194\n",
      "weighted avg       0.72      0.72      0.72       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение KNN...\n",
      " KNN\n",
      "Accuracy: 0.7268 | Precision: 0.7274 | Recall: 0.7268 | F1: 0.7266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72        97\n",
      "           1       0.72      0.75      0.73        97\n",
      "\n",
      "    accuracy                           0.73       194\n",
      "   macro avg       0.73      0.73      0.73       194\n",
      "weighted avg       0.73      0.73      0.73       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение Random Forest...\n",
      " Random Forest\n",
      "Accuracy: 0.7526 | Precision: 0.7535 | Recall: 0.7526 | F1: 0.7523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.78      0.76        97\n",
      "           1       0.77      0.72      0.74        97\n",
      "\n",
      "    accuracy                           0.75       194\n",
      "   macro avg       0.75      0.75      0.75       194\n",
      "weighted avg       0.75      0.75      0.75       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение XGBoost...\n",
      " XGBoost\n",
      "Accuracy: 0.7423 | Precision: 0.7439 | Recall: 0.7423 | F1: 0.7418\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75        97\n",
      "           1       0.76      0.70      0.73        97\n",
      "\n",
      "    accuracy                           0.74       194\n",
      "   macro avg       0.74      0.74      0.74       194\n",
      "weighted avg       0.74      0.74      0.74       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение CatBoost...\n",
      " CatBoost\n",
      "Accuracy: 0.7577 | Precision: 0.7580 | Recall: 0.7577 | F1: 0.7577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76        97\n",
      "           1       0.77      0.74      0.75        97\n",
      "\n",
      "    accuracy                           0.76       194\n",
      "   macro avg       0.76      0.76      0.76       194\n",
      "weighted avg       0.76      0.76      0.76       194\n",
      "\n",
      "──────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAMVCAYAAACm0EewAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC1pklEQVR4nOzdeXhTVf7H8c9Nuhfasm8ttCxCZVPLIiAim8KwiKjg4AgIOCI4KIwLyKCIC+o4iBuKI4Kg/mRQQHFwqaiAAg5oAZFFZLECZdeyFNomub8/StOmTWnapjcF3y8fnod+c3PvOenJmcNnTm4M0zRNAQAAAAAAABayBboBAAAAAAAA+OMhlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAoBQ2b96s22+/XQkJCQoLC1OlSpV0xRVX6JlnntHx48cD3Tz8AZ06dUpRUVFav369MjIy9O9//1vt27cPdLMuGH379lV8fHygmwEAwB9KUKAbAADAhebf//63xowZo6ZNm+r+++/XpZdequzsbG3YsEGvvvqq1q5dqyVLlgS6mfiDqVSpksaNG6crr7xSLpdLlSpV0ttvvx3oZgEAABTJME3TDHQjAAC4UKxdu1adO3dWz549tXTpUoWGhno8npWVpU8++UT9+/cPUAvxR3f06FEdOnRI8fHxioyMDHRzLhh9+/bVli1btHfv3kA3BQCAPww+vgcAQAk8+eSTMgxDr732WqFASpJCQkI8Aqn4+Hj17dtXS5YsUatWrRQWFqaGDRvqhRde8Hje2bNn9fe//12XXXaZoqOjVbVqVXXo0EEffPBBoWsYhuH+Y7fbVbduXQ0bNkyHDh1yH7N3714ZhqFnn3220PNbtGiha665xqN24sQJ3XfffUpISFBISIjq1aune++9V6dPny507bvvvrvQOQt+9Cn3+vPmzfM4buTIkTIMQ8OHD/eoHzx4UHfeeadiY2MVEhKihIQEPfroo3I4HIWuVVB8fLwMw9DYsWMLPda1a1cZhqG+fft61FNTU/WXv/xFNWvWVGhoqBITE/Wvf/1LLper0DnmzZvn8Zrn/vH2Ua8NGzaof//+qlq1qsLCwnT55ZfrP//5j9d2X3PNNV7PW/A1+/zzz9W9e3dFRUUpIiJCnTp10ooVKzyOmTp1qgzDkCRVr15dzZs3V1ZWlmrUqCHDMPTVV1+d5xX0fH6uZcuWKTQ0VOPHjy9V25OTk3X99dcrNjZWYWFhaty4se68804dPXq00Pm2b9+uP//5z6pVq5ZCQ0NVv359DR06VJmZme5j9u/fr7/+9a+Ki4tTSEiI6tatq5tuusk97r/66iuvfe3Ro4cMw9DUqVM96i+++KLq1q2rmJgYPfzww+76/Pnz3fUJEybI6XS6HyvJNXx9TY8cOaIxY8bo0ksvVaVKlVSzZk1169ZNq1ev9nhuSd/T11xzTaHa6tWr3b+r/Ly9Po899pgMwyh0DgAA/ImP7wEA4COn06kvvvhCSUlJiouL8/l5Gzdu1L333qupU6eqdu3aevvtt3XPPfcoKytL9913nyQpMzNTx48f13333ad69eopKytLn3/+uQYOHKi5c+dq6NChHuccOXKkRo0aJYfDofXr12vSpEk6cuSIli9fXuJ+ZWRkqEuXLtq3b58eeughtWrVSj/++KMefvhh/fDDD/r8888L/SO2NL799lvNnTtXdrvdo37w4EG1a9dONptNDz/8sBo1aqS1a9fq8ccf1969ezV37txiz121alXNnz9f06dPV1RUlCTpxx9/1DfffOP+OdeRI0fUsWNHZWVl6bHHHlN8fLw++ugj3Xfffdq1a5dmzZrl9Rpz585Vs2bNJEn33Xef9u3b5/H4l19+qV69eql9+/Z69dVXFR0drXfffVeDBw9WRkZGoSBOki6//HL39dLS0jRw4ECPx9966y0NHTpU119/vd58800FBwdr9uzZuu666/Tpp5+qe/fuRb4mkydP1m+//Xb+F64IH330kW666SaNGTNGzz33nNdjimv7rl271KFDB40aNUrR0dHau3evZsyYoauuuko//PCDgoODJUmbNm3SVVddperVq2vatGlq0qSJ0tLS9OGHHyorK0uhoaHav3+/2rZtq+zsbPcYPXbsmD799FP99ttvqlWrltc2/uc///EayC1dulTjxo3TiBEjNHjwYM2fP19fffWVnE6n5s2bp7lz57rHYOXKlfXoo48W+VoVdQ1fX9Pce9A98sgjql27tk6dOqUlS5bommuu0YoVK/wWCjmdTo0dO1Z2u90jaPPml19+0fTp0wu9VwEA8DsTAAD45ODBg6Yk85ZbbvH5OQ0aNDANwzA3btzoUe/Zs6cZFRVlnj592uvzHA6HmZ2dbY4cOdK8/PLLPR6TZD7yyCMetQEDBpg1a9Z0/7xnzx5TkvnPf/6z0LmbN29udunSxf3z9OnTTZvNZq5fv97juPfee8+UZC5fvtzj2mPHji10zj59+pgNGjQodP25c+eapmmaTqfTTEpKMvv37282aNDAHDZsmPvYO++806xUqZL5yy+/eJzz2WefNSWZP/74Y6Hr5degQQOzT58+5qWXXmo+//zz7vro0aPNQYMGuR/PNXHiRFOS+e2333qc56677jINwzB37NjhUX/11VdNSeb3339fZH9N0zSbNWtmXn755WZ2drZHvW/fvmadOnVMp9PpUe/QoYPZvXt3988FX7PTp0+bVatWNfv16+fxPKfTabZu3dps166du/bII4+Y+Zd133//vWmz2cxx48aZkswvv/yy4MvmIf/zly1bZoaEhJj33ntvkccX1/aCXC6XmZ2dbf7yyy+mJPODDz5wP9atWzczJibGPHz4cJHXGzFihBkcHGxu3bq1yGO+/PJLj76eOnXKjI2Ndb8G+d8zSUlJZocOHTza16ZNG7Nq1armqVOn3PUxY8aYUVFR5smTJ0t8jZK+prly3/vdu3c3b7jhBne9JO9p0zTNLl26eNRmzpxpRkZGmiNGjDAL/hOgYNsHDBhgXn755Wbnzp0LnRcAAH/i43sAAJSz5s2bq3Xr1h61IUOG6MSJE/r+++/dtUWLFqlTp06qVKmSgoKCFBwcrDlz5mjbtm2FzulyueRwOJSZmanVq1fr66+/9rprJve4/H8K+uijj9SiRQtddtllHsddd911Xj+qZJpmoXOaxdyicvbs2dq6datmzpzp9fpdu3ZV3bp1Pc7Zu3dvSdLKlSvPe+5cd999t15++WWZpqn09HQtWLDA60f6vvjiC1166aVq166dR3348OEyTVNffPGFR/3UqVOSpIiIiCKv/fPPP2v79u269dZbJcmjH3/605+UlpamHTt2eDznzJkzCgsLK/Kca9as0fHjxzVs2DCP87lcLvXq1Uvr168v9PFKKef3M2bMGPXs2VM33HBDkef35r///a9uvPFGXXbZZUXukPKl7ZJ0+PBhjR49WnFxce7x3KBBA0lyj+mMjAytXLlSgwYNUo0aNYo818cff6yuXbsqMTHR575MmzZN2dnZmjZtmkfd6XRq06ZN6tq1q7tmGIZq1aqlypUre9yHq1u3bjpx4oR++umnEl0jP19e01dffVVXXHGFwsLC3K/VihUrvL73S+PQoUN65JFHNGXKlGJ3eX7yySf64IMP9PLLL8tm458KAIDyxf/SAADgo+rVqysiIkJ79uwp0fNq165dZO3YsWOSpMWLF2vQoEGqV6+e3nrrLa1du1br16/XiBEjdPbs2ULPf+yxxxQcHKywsDBdffXVaty4sdfA58EHH1RwcLDHnx9//NHjmEOHDmnz5s2FjqtcubJM0yx0D6BZs2YVOvZ8Hxs8evSo/vGPf2jixIlKSEgo9PihQ4e0bNmyQuds3ry5+/m+GDp0qA4dOqTPPvtMc+fOVaNGjXT11VcXOu7YsWOqU6dOoXrdunXdj+e3f/9+j8e9yb2v0X333VeoH2PGjPHaj6NHj6p69erFnvOmm24qdM6nn35apmm6P/qV39y5c/X999/rxRdfLPLcRRk4cKA6deqk//3vf1q2bFmRxxXXdpfLpWuvvVaLFy/WAw88oBUrVuh///uf1q1bJykn1JKk3377TU6nU7Gxsedt15EjR4o9Jr8dO3boueee0zPPPKPo6OhC53I4HKpcuXKx58n96GdaWlqJrpFfca/pjBkzdNddd6l9+/Z6//33tW7dOq1fv169evVyv05ldf/996t27dpe7w+WX2ZmpsaNG6fhw4erQ4cOfrk2AADnwz2lAADwkd1uV/fu3fXxxx9r3759Pv8j+eDBg0XWqlWrJinn3kEJCQlauHChx/2b8t/oOb877rhDf/3rX2Wapg4cOKAnn3xSHTp00MaNGz3+sX3PPffoL3/5i8dzb7nlFo+fq1evrvDwcL3xxhter1UwfBg0aJDuv/9+j9r48eP166+/en3+pEmTFBMTowceeKDI87dq1UpPPPGE18fPFwblFxkZqeHDh+uFF17Qzp073ffrKqhatWpeQ4YDBw6425Pfpk2b1KBBg/OGGLnPmTRpUqF7K+Vq2rSp++8ZGRnav3+/GjduXOw5X3zxRV155ZVejyl4L6Xff/9dEydO1P33368mTZq4AzVf5d7vaMiQIRoxYoR++OGHQqGqL23fsmWLNm3apHnz5mnYsGHu+s8//+xxXNWqVWW32wvdn6ugGjVqFHtMfn/729/Uvn37Qvdik3JeV7vd7lPYmXuMt2D5fNfIr7jX9K233tI111yjV155xeN5J0+eLLZ9vvj666/11ltv6dNPP1VISMh5j3322Wd15MgRPf300365NgAAxSGUAgCgBCZNmqTly5frjjvu0AcffFDoH3nZ2dn65JNP1K9fP3ftxx9/1KZNmzw+wvfOO++ocuXKuuKKKyTlfHwoJCTEI5A6ePCg12/fk3KCmjZt2rh/Nk1TN9xwg9auXatrr73WXY+NjfU4TlKhj1317dtXTz75pKpVq+Z1J1NBNWrUKHTO6Ohor6HU//73P82ZM0fLli0r8uNeffv21fLly9WoUSNVqVKl2Oufz9ixY9W0aVNFR0cXCuNyde/eXdOnT9f333/vfv2lnG9dMwzD42Ndx48f19dff62//vWv571u06ZN1aRJE23atElPPvlkse388MMPZZqm151cuTp16qSYmBht3brV6zceevOPf/xD4eHheuihh3w6vqDcj5e98soratWqlYYNG6ZPPvnEY1z60vbc4wt+Q+Xs2bM9fg4PD1eXLl20aNEiPfHEE0Xuvurdu7cWLFigHTt2eIR73rz33nv64osv9N1333l9PCgoSC1bttSXX37prpmmqcOHD+vkyZM6ffq0+yN8K1asUGRkpC655JISXSO/4l5TwzAKvU6bN2/W2rVrS/SFCt44nU7dfffduvHGG9WzZ8/zHpuamqqFCxfqmWeeOe9HKQEA8CdCKQAASqBDhw565ZVXNGbMGCUlJemuu+5S8+bNlZ2drZSUFL322mtq0aKFRyhVt25d9e/fX1OnTlWdOnX01ltvKTk5WU8//bT7PkV9+/bV4sWLNWbMGN1000369ddf9dhjj6lOnTrauXNnoXbs27dP69atc++Umj59ukJDQ0t0z51c9957r95//31dffXVGj9+vFq1aiWXy6XU1FR99tln+vvf/6727duX6vV67bXX1K9fP/Xp06fIY6ZNm6bk5GR17NhR48aNU9OmTXX27Fnt3btXy5cv16uvvurzrrQmTZpo9erVioyMLPIeUOPHj9f8+fPVp08fTZs2TQ0aNNB///tfzZo1S3fddZc7gNiyZYseeOABZWVlqUOHDu6Pnkk5O5IyMzO1bt069y6m2bNnq3fv3rruuus0fPhw1atXT8ePH9e2bdv0/fffa9GiRUpPT9crr7yiJ598UldddZU6d+5cZF8qVaqkF198UcOGDdPx48d10003qWbNmjpy5Ig2bdqkI0eOFNpd8+qrr2rRokXnvf+VL6Kjo7VgwQJ17dpVM2fO1Pjx40vU9mbNmqlRo0aaOHGiTNNU1apVtWzZMiUnJxc6Nvcb+dq3b6+JEyeqcePGOnTokD788EPNnj1blStX1rRp0/Txxx/r6quv1kMPPaSWLVvq999/1yeffKIJEya4vxUx9zUYO3Zsofu45Tdp0iQNHjxYd9xxhwYNGqT58+dr27Ztcjgc6t+/vx588EGtW7dO8+bN04MPPlhol5wv1/DlNZVy3vuPPfaYHnnkEXXp0kU7duzQtGnTlJCQ4PUecEeOHNH27ds9allZWcrIyND27ds9Xou1a9cqLCzsvB/FzDV//ny1atVKo0eP9rlPAACUWSDurg4AwIVu48aN5rBhw8z69eubISEhZmRkpHn55ZebDz/8sMe3iOV+89t7771nNm/e3AwJCTHj4+PNGTNmFDrnU089ZcbHx5uhoaFmYmKi+e9//7vQt6qZZs43ZeX+MQzDrFatmtmtWzfziy++cB9T0m/qOnXqlPmPf/zDbNq0qRkSEmJGR0ebLVu2NMePH28ePHjQ49ol+fa9sLAwc/fu3R7HFvz2PdM0zSNHjpjjxo0zExISzODgYLNq1apmUlKSOXnyZI9vQ/Om4Lfr+fL4L7/8Yg4ZMsSsVq2aGRwcbDZt2tT85z//6fENeV26dPF4rYv6k9+mTZvMQYMGmTVr1jSDg4PN2rVrm926dTNfffVV0zRN85tvvjETEhLMv//97+aJEyc8nlvUN9itXLnS7NOnj1m1alUzODjYrFevntmnTx9z0aJF7mNyx8l1113n8dyC3xZXFG/jzDRzvqkwNDTU3LhxY4nbvnXrVrNnz55m5cqVzSpVqpg333yzmZqa6vXbI7du3WrefPPNZrVq1cyQkBCzfv365vDhw82zZ8+6j/n111/NESNGmLVr1zaDg4PNunXrmoMGDTIPHTrk0deaNWuav//+u8f5vV1zxowZZu3atc2oqCjz4Ycfdo/h+fPnm3Xq1DGjoqLMcePGmVlZWYVeT1+u4ctrapqmmZmZad53331mvXr1zLCwMPOKK64wly5dag4bNszre8rX8Zg7fqdPn+5x/aLmFMMwzDVr1njUC36DHwAA/maYZjFflwMAAEotPj5eLVq00EcffRTopqCErrnmGl1zzTWaOnWq18f37t2rhISEYr95EBeGvn37asuWLdq7d2+gm1IqX331lbp27cp4BABcUPj2PQAAAC8uvfTS835sMDQ0tNQfawT8LSIiotj7bQEAUNFwTykAAAAvZs2add7H69Sp43GfKSCQ2rVrV+heUwAAVHR8fA8AAAAAAACW4+N7AAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACwXFOgGWM3lcunAgQOqXLmyDMMIdHMAAAAAAAAuKqZp6uTJk6pbt65stqL3Q/3hQqkDBw4oLi4u0M0AAAAAAAC4qP3666+KjY0t8vE/XChVuXJlSTkvTFRUVIBbAwAAAAAAcHE5ceKE4uLi3BlMUf5woVTuR/aioqIIpQAAAAAAAMpJcbdN4kbnAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcgENpVatWqV+/fqpbt26MgxDS5cuLfY5K1euVFJSksLCwtSwYUO9+uqr5d9QAAAAAAAA+FVAQ6nTp0+rdevWeumll3w6fs+ePfrTn/6kzp07KyUlRQ899JDGjRun999/v5xbCgAAAAAAAH8KCuTFe/furd69e/t8/Kuvvqr69etr5syZkqTExERt2LBBzz77rG688Uavz8nMzFRmZqb75xMnTkiSHA6HHA6HJMlms8lms8nlcsnlcrmPza07nU6Zplls3W63yzAM93nz1yXJ6XT6VA8KCpJpmh51wzBkt9sLtbGoOn2iT/SJPtEn+kSf6BN9ok/0iT7RJ/pEn+hTIPrkq4CGUiW1du1aXXvttR616667TnPmzFF2draCg4MLPWf69Ol69NFHC9VTUlIUGRkpSapRo4YaNWqkPXv26MiRI+5jYmNjFRsbq59++knp6enuesOGDVWzZk1t2bJFZ86ccdebNWummJgYpaSkePwCW7VqpZCQEG3YsMGjDW3atFFWVpY2b97srtntdrVt21bp6enavn27ux4eHq7WrVvr6NGj2r17t7seHR2txMREHThwQPv27XPX6RN9ok/0iT7RJ/pEn+gTfaJP9Ik+0Sf6RJ8C0acaNWrIF4aZP1oLIMMwtGTJEg0YMKDIYy655BINHz5cDz30kLu2Zs0aderUSQcOHFCdOnUKPcfbTqm4uDgdO3ZMUVFRki7eZJI+0Sf6RJ/oE32iT/SJPtEn+kSf6BN9ok/0yeo+nTp1StHR0UpPT3dnL95ccKHU7bffrkmTJrlr33zzja666iqlpaWpdu3axV7nxIkTPr0wAAAAAAAAKDlfs5eA3ui8pGrXrq2DBw961A4fPqygoCBVq1YtQK0CAAAAAABASV1QoVSHDh2UnJzsUfvss8/Upk0br/eTAgAAAAAAQMUU0FDq1KlT2rhxozZu3ChJ2rNnjzZu3KjU1FRJ0qRJkzR06FD38aNHj9Yvv/yiCRMmaNu2bXrjjTc0Z84c3XfffYFoPgAAAAAAAEopoN++t2HDBnXt2tX984QJEyRJw4YN07x585SWluYOqCQpISFBy5cv1/jx4/Xyyy+rbt26euGFF3TjjTda3nYAAAAAAACUXoW50blVuNE5AAAAAABA+bkob3QOAAAAAACAiwOhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsFzAQ6lZs2YpISFBYWFhSkpK0urVq897/Ntvv63WrVsrIiJCderU0e23365jx45Z1FoAAAAAAAD4Q0BDqYULF+ree+/V5MmTlZKSos6dO6t3795KTU31evzXX3+toUOHauTIkfrxxx+1aNEirV+/XqNGjbK45QAAAAAAACiLgIZSM2bM0MiRIzVq1CglJiZq5syZiouL0yuvvOL1+HXr1ik+Pl7jxo1TQkKCrrrqKt15553asGGDxS0HAAAAAABAWQQF6sJZWVn67rvvNHHiRI/6tddeqzVr1nh9TseOHTV58mQtX75cvXv31uHDh/Xee++pT58+RV4nMzNTmZmZ7p9PnDghSXI4HHI4HJIkm80mm80ml8sll8vlPja37nQ6ZZpmsXW73S7DMNznzV+XJKfT6VM9KChIpml61A3DkN1uL9TGour0iT7RJ/pEn+gTfaJP9Ik+0Sf6RJ/oE32iT4Hok68CFkodPXpUTqdTtWrV8qjXqlVLBw8e9Pqcjh076u2339bgwYN19uxZORwO9e/fXy+++GKR15k+fboeffTRQvWUlBRFRkZKkmrUqKFGjRppz549OnLkiPuY2NhYxcbG6qefflJ6erq73rBhQ9WsWVNbtmzRmTNn3PVmzZopJiZGKSkpHr/AVq1aKSQkpNCOrjZt2igrK0ubN2921+x2u9q2bav09HRt377dXQ8PD1fr1q119OhR7d69212Pjo5WYmKiDhw4oH379rnr9Ik+0Sf6RJ/oE32iT/SJPtEn+kSf6BN9ok+B6FONGjXkC8PMH61Z6MCBA6pXr57WrFmjDh06uOtPPPGEFixY4PEC5Nq6dat69Oih8ePH67rrrlNaWpruv/9+tW3bVnPmzPF6HW87peLi4nTs2DFFRUVJuniTSfpEn+gTfaJP9Ik+0Sf6RJ/oE32iT/SJPtEnq/t06tQpRUdHKz093Z29eBOwUCorK0sRERFatGiRbrjhBnf9nnvu0caNG7Vy5cpCz7ntttt09uxZLVq0yF37+uuv1blzZx04cEB16tQp9ronTpzw6YUBAAAAAABAyfmavQTsRuchISFKSkpScnKyRz05OVkdO3b0+pyMjIxCn03MTfcClK0BAAAAAACgFAL67XsTJkzQ66+/rjfeeEPbtm3T+PHjlZqaqtGjR0uSJk2apKFDh7qP79evnxYvXqxXXnlFu3fv1jfffKNx48apXbt2qlu3bqC6AQAAAAAAgBIK2I3OJWnw4ME6duyYpk2bprS0NLVo0ULLly9XgwYNJElpaWlKTU11Hz98+HCdPHlSL730kv7+978rJiZG3bp109NPPx2oLgAAAAAAAKAUAnZPqUDhnlIAAAAAAADlp8LfUwoAAAAAAAB/XIRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACwXFOgG4CIyNTrQLcgzNT3QLQAAAAAAAOdBKHWBi5/430A3wW1vWKBbkKflmy0D3QS3/0x3BLoJHhK3bwt0EwAAAAAA4ON7AAAAAAAAsB47pQAETMXa6Tck0E1wa5lQP9BNcKtIO/3Y5QcAAABcXNgpBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALMeNzgEAuJBNjQ50C/JMTQ90CwAAAHABIZQCAKCEKtY3Rwa6BXlavtky0E1w45sjAQAAKj4+vgcAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsFxQoBsAAAAAa8RP/G+gm+C2N2xIoJvg1jKhfqCb4Paf6Y5AN8Etcfu2QDcBAHCRY6cUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALMe37wEAAACo8Pj2SO/49kjv+PZI4MLATikAAAAAAABYjlAKAAAAAAAAliOUAgAAAAAAgOUIpQAAAAAAAGA5QikAAAAAAABYjlAKAAAAAAAAliOUAgAAAAAAgOUIpQAAAAAAAGA5QikAAAAAAABYjlAKAAAAAAAAlgsKdAMAAAAAALhYxU/8b6Cb4LY3bEigm+DWMqF+oJvg9p/pjkA3wS1x+7ZAN8FS7JQCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWC3goNWvWLCUkJCgsLExJSUlavXr1eY/PzMzU5MmT1aBBA4WGhqpRo0Z64403LGotAAAAAAAA/CEokBdfuHCh7r33Xs2aNUudOnXS7Nmz1bt3b23dulX169f3+pxBgwbp0KFDmjNnjho3bqzDhw/L4XBY3HIAAAAAAACURUBDqRkzZmjkyJEaNWqUJGnmzJn69NNP9corr2j69OmFjv/kk0+0cuVK7d69W1WrVpUkxcfHn/camZmZyszMdP984sQJSZLD4XCHWTabTTabTS6XSy6Xy31sbt3pdMo0zWLrdrtdhmEUCsnsdrskyel0+lQPCgqSaZoedcMwZLfbC7UxyDDlMA3ZDFN2I+8cLlNymobshilbvrrTlFymoSDDlJG/7pJcKlx3uCRThoJtef3Mq0vB+fbaOYwQ2c0sSYacRrBnn8wsmQXqhkzZzWy5ZJPLCPJSt8tl2N11m5yymU65DLtcylc3nbLJKacRLFM5jQ9WsJxyyiWXghQkQ3mdcsghU6aC5dnGourZypYhQ0EF3i7e6qZMOeSQTTbZz7XRFWxIpimbwyHTZpNpz2u74XLJcDpl2u0ybXkvpuF0ynC55AoKUv5fSJF1h0OGacoV7Nl2w+GQTFNmvrrD4fDL2CuqXpL3U7DN9MvYk6Rsl2RICipUN2TI9KibpnLeNzJlP1d3GCF+GXs5dYdschWq281sGTLlMEI82mg3syWZcp6r547Bso6989Vdcskpp+yyy5Zv02zB940rOKf9/hh7kmRkZ0uGITPIs0+27GyZBesF3je5c6s/xt756r7M5fnHZVnHnke9FHN5/vFU1rGXVy/dXJ5//izr2CuuXtxcXmhMlmHsFVcvbi6vSOsIf449f6wj/DX2yrqOkOS3sVfWdYTkv7FX1nVE7vgsjzVsaeZySeWyhpVKPpdLKpc1bE69ZHN5Tgv9v4Y9X72oudy0ucptDSuVbC53Op3ltoY9X93bXB5sM8ttDetR92Eudxgh5baGzav7NpcHK7jc1rDF1QvO2e41bzmsYYurF5zLC+YUFWkdUZL3k68CFkplZWXpu+++08SJEz3q1157rdasWeP1OR9++KHatGmjZ555RgsWLFBkZKT69++vxx57TOHh4V6fM336dD366KOF6ikpKYqMjJQk1ahRQ40aNdKePXt05MgR9zGxsbGKjY3VTz/9pPT0dHe9YcOGqlmzprZs2aIzZ864682aNVNMTIxSUlI8foGtWrVSSEiINmzY4NGGNm3aKCsrS5s3b3bX7Ha72rZtq/T0dG3fvt1dDw8PV+vWrXX06FHt3r3bXe8Z69LHv9p1eTVTV1TLG5A70g2tOmioUy1TTaPz6t8fM/TdUUM9Y12Kjchry6qDhnakG7oh3qWYfHPKx/ts2ndaurWRy2MCfW+PTacc0vAmeQNvg32s2ux5WVlBlbU5bmhen1xZarv3ZaWH19f2OgPz+pR1XK33vamjlS/V7ho93fXojF+UeHCxDlRpp31VrnTXa5zcokZHkrWnejcdqdzCXY/9bZ1if1urn2r1U3pEA0nS4Ihwrctcp58dP6t3eG9F26Ldx684u0JpzjQNjBio4HwT5bKMZcowMzQ4cnD+X5MWnl6oCCNC/SL6uWvZZrYWZixUbXttdQ/r7q6nu9K17MwyNQxqqCtDc9qeNsRU6IEDqp6crJOtWupk68vcx0fs3Kkqa9bo9/btldGkibteedNGRW3cpONduyqzbl13PWbNGkXu3KkjffvIER3jrldLTlbYgQM6OOhmmUF5far5wVLZT2cobcgQd+3khg1+GXvR0dFKTEzUgQMHtG/fvrzfUwneT8ObuPwy9iRp3k6bKgVJNyXk1bNd0ryddtWLlHrH5tV/z5IW7bGrSbSpq2vnvD822Mf6ZexJUsMjyap5cou21BuiMyFV3fVmaYsVc+YXpTS4Q05bXmdb/TpfIY6T2pAwVlLO+JXKPvYkKc2ZphVnV6hFcAu1Cmnlrv+c/bPWZa1T25C2ahzc2F3fnLVZm7M3q0tYF9Wx11HakJzXxx9jT5LqvPOOnJEROnz9AHfNcGSr7tvvKLNOHR3rmTcXBKX/rlpLP1BGo0b6vWNHnTw3h/pj7Ellm8vzj7+yjj1J2pehUs/lGyqPzetTGcdertLO5YMj+7vrZR17uUo7l6cNGeTRp7KMvVylncsr0jrCn2OvrOsIpxHit7FX1nWEtN9vY6+s6whpid/GXlnXEQlnzpTbGrY0c7mkclnDSiWfy5WmclnDSiWfy6P0ebmsYaWSz+UnW6WU2xpWKtlc7tiypdzWsFLJ5vLhTVzltoaVSjaXb7CPLbc1bC5f5/LBEeHltobN5etcnrvmLY81bC5f5/LcNe+FkEec7/1Uo0YN+cIw80drFjpw4IDq1aunb775Rh3z/aKefPJJvfnmm9qxY0eh5/Tq1UtfffWVevTooYcfflhHjx7VmDFj1K1btyLvK+Vtp1RcXJyOHTumqKgoSRf2TqlmUz6pMDultoXeXmF2SrWLj6swO6UWPOuoUDulmqZ8X2F2SiU+/EmF2Sm1LfT2CrNTql18XE7bK8BOqQXPntudVAF2SjVN+T7nHBVgp1Tiw5+464HeKbU97Pa8PgV4p1SbhIS8PgV4p9S7zxoe9UDulLrkxy0VZh3RcOJHFWan1O6wWyvMTqnWCfUqzE6pd6afrTA7pS7dvCnntaggO6UaPvRxhdkp9XPorRVmp9QV8XUqzE6pt57OrDA7pZptTKkwO6USH/6kwuyU2hZ6e4XZKdUuPq7C7JRyr3krwE6p3DXvhZBHnO/9dOrUKUVHRys9Pd2dvXgT0I/vSTmdyM80zUK1XC6XS4Zh6O2331Z0dE7KOWPGDN100016+eWXve6WCg0NVWhoaKF6UFCQggoOqHO/3ILs+QeSD/WC5y1N3TAMr/WCbXSYOa+VyzTk8hIvOk1DTi91h2nkzIo+1rNd3n8n2fmC/iAz69zfzHx/z2MUUbfJJZvXutO9pd6jfm4SLShnUjzXLuX93SHv9xzLf0xxdVNmiequc/9Jki077/qGK2dLc0GG0ynD6aWvRdwvrch6tvc+Gfnq+cdVWcZeaev53zf5x1VZxl4us8i64bXukqHcX0f+sVmWsedL3dv7IH89/5gqy9jzpe48919Bue+b/ONXKtvYy2u86bVuFFU/977xdc62Yi4vOC7LMvY86qWYy72Np9KOPU8ln8tLMiaLG3u+1ouay72OyVKOPZ/rRczlFWkd4c+xV9Z1hCH/jT1/rCP8Nfb8sY7w19gr6zoid01eHmvY0tbLYw2bq6RzeXmsYX2pF3wfmOc67u81rC/1gnN57vgsjzVsXuN9m8tz59jyWMP6Us//Psg/Dv29hvWo+zCX5x8//l7Deip+Ls8/Bv29hvW1ntuGgmtef65hfa6fm8sLzqEVaR1R0veNLwL27XvVq1eX3W7XwYMHPeqHDx9WrVq1vD6nTp06qlevnjuQkqTExESZpumxVQwAAAAAAAAVW5l2Sq1fv16LFi1SamqqsrI8U9DFixef97khISFKSkpScnKybrjhBnc9OTlZ119/vdfndOrUSYsWLdKpU6dUqVIlSdJPP/0km83m/pw5AAAAAAAAKr5S75R699131alTJ23dulVLlixRdna2tm7dqi+++MJjJ9P5TJgwQa+//rreeOMNbdu2TePHj1dqaqpGjx4tSZo0aZKGDs27OdqQIUNUrVo13X777dq6datWrVql+++/XyNGjCjyRucAAAAAAACoeEq9U+rJJ5/Uc889p7Fjx6py5cp6/vnnlZCQoDvvvFN16tQp/gSSBg8erGPHjmnatGlKS0tTixYttHz5cjVokPMNAGlpaUpNTXUfX6lSJSUnJ+tvf/ub2rRpo2rVqmnQoEF6/PHHS9sNAAAAAAAABECpQ6ldu3apT58+knJuJn769GkZhqHx48erW7duevTRR306z5gxYzRmzBivj82bN69QrVmzZkpOTi5tswEAAAAAAFABlPrje1WrVtXJkyclSfXq1dOWLVskSb///rsyMjL80zoAAAAAAABclEq9U6pz585KTk5Wy5YtNWjQIN1zzz364osvlJycrO7du/uzjQAAAAAAALjIlDqUeumll3T27FlJOTckDw4O1tdff62BAwdqypQpfmsgAAAAAAAALj6lDqWqVq3q/rvNZtMDDzygBx54wC+NAgAAAAAAwMWt1PeU+uWXX7zWs7OzNXHixFI3CAAAAAAAABe/UodSV111lXbs2OFR27Bhgy677DJ99NFHZW4YAAAAAAAALl6lDqVGjBihzp07KyUlRdnZ2Zo0aZI6d+6s/v376/vvv/dnGwEAAAAAAHCRKfU9pR599FHFxMSoa9euqlevngzD0KpVq9S2bVt/tg8AAAAAAAAXoVKHUpI0fvx4RUVFafTo0Vq4cCGBFAAAAAAAAHxS6lDqhRdecP/96quv1pAhQzRp0iRVqVJFkjRu3Liytw4AAAAAAAAXpVKHUs8995zHz3Xq1NG8efMkSYZhEEoBAAAAAACgSKUOpfbs2ePPdgAAAAAAAOAPpNTfvgcAAAAAAACUVql3Sk2YMOG8j8+YMaO0pwYAAAAAAMBFrtShVEpKivvvX3/9tZKSkhQeHi4p555SAAAAAAAAQFFKHUp9+eWX7r9XrlxZ77zzjho2bOiXRgEAAAAAAODixj2lAAAAAAAAYDlCKQAAAAAAAFiu1B/f+/DDD91/d7lcWrFihbZs2eKu9e/fv2wtAwAAAAAAwEWr1KHUgAEDPH6+88473X83DENOp7PUjQIAAAAAAMDFrdShlMvl8mc7AAAAAAAA8Afil3tKnT171h+nAQAAAAAAwB9EqUMpp9Opxx57TPXq1VOlSpW0e/duSdKUKVM0Z84cvzUQAAAAAAAAF59Sh1JPPPGE5s2bp2eeeUYhISHuesuWLfX666/7pXEAAAAAAAC4OJU6lJo/f75ee+013XrrrbLb7e56q1attH37dr80DgAAAAAAABenUodS+/fvV+PGjQvVXS6XsrOzy9QoAAAAAAAAXNxKHUo1b95cq1evLlRftGiRLr/88jI1CgAAAAAAABe3oNI+8ZFHHtFtt92m/fv3y+VyafHixdqxY4fmz5+vjz76yJ9tBAAAAAAAwEWm1Dul+vXrp4ULF2r58uUyDEMPP/ywtm3bpmXLlqlnz57+bCMAAAAAAAAuMqXeKSVJ1113na677jp/tQUAAAAAAAB/EGUKpbxxOp264447JEnBwcGaPXu2vy8BAAAAAACAC1ypQ6mBAwd6rbtcLi1btkyLFy+W3W4vdcMAAAAAAABw8Sp1KBUdHe217nQ6JUnXX399aU8NAAAAAACAi1ypQ6m5c+d6rZ89e1Zvv/12qRsEAAAAAACAi1+pv32vKIZh+PuUAAAAAAAAuMj4PZQCAAAAAAAAilPqj++98MILXusOh6PUjQEAAAAAAMAfQ6lDqeeee67Ix+rXr1/a0wIAAAAAAOAPoNSh1J49eyRJR44ckc1mU7Vq1fzWKAAAAAAAAFzcSnVPqd9//11jx45V9erVVbt2bdWsWVPVq1fX3XffrfT0dH+3EQAAAAAAABeZEu+UOn78uDp06KD9+/fr1ltvVWJiokzT1LZt2zRv3jytWLFCa9asUZUqVcqjvQAAAAAAALgIlDiUmjZtmkJCQrRr1y7VqlWr0GPXXnutpk2bdt57TgEAAAAAAOCPrcQf31u6dKmeffbZQoGUJNWuXVvPPPOMlixZ4pfGAQAAAAAA4OJU4lAqLS1NzZs3L/LxFi1a6ODBg2VqFAAAAAAAAC5uJQ6lqlevrr179xb5+J49e/gmPgAAAAAAAJxXiUOpXr16afLkycrKyir0WGZmpqZMmaJevXr5pXEAAAAAAAC4OJX4RuePPvqo2rRpoyZNmmjs2LFq1qyZJGnr1q2aNWuWMjMztWDBAr83FAAAAAAAABePEodSsbGxWrt2rcaMGaNJkybJNE1JkmEY6tmzp1566SXFxcX5vaEAAAAAAAC4eJQ4lJKkhIQEffzxx/rtt9+0c+dOSVLjxo1VtWpVvzYOAAAAAAAAF6dShVK5qlSponbt2vmrLQAAAAAAAPiDKPGNzgEAAAAAAICyIpQCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYDlCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAAAAAAAAYLmAh1KzZs1SQkKCwsLClJSUpNWrV/v0vG+++UZBQUG67LLLyreBAAAAAAAA8LuAhlILFy7Uvffeq8mTJyslJUWdO3dW7969lZqaet7npaena+jQoerevbtFLQUAAAAAAIA/BTSUmjFjhkaOHKlRo0YpMTFRM2fOVFxcnF555ZXzPu/OO+/UkCFD1KFDB4taCgAAAAAAAH8KCtSFs7Ky9N1332nixIke9WuvvVZr1qwp8nlz587Vrl279NZbb+nxxx8v9jqZmZnKzMx0/3zixAlJksPhkMPhkCTZbDbZbDa5XC65XC73sbl1p9Mp0zSLrdvtdhmG4T5v/rokOZ1On+pBQUEyTdOjbhiG7HZ7oTYGGaYcpiGbYcpu5J3DZUpO05DdMGXLV3eakss0FGSYMvLXXZJLhesOl2TKULAtr595dSk4X6zpMEJkN7MkGXIawZ59MrNkFqgbMmU3s+WSTS4jyEvdLpdhd9dtcspmOuUy7HIpX910yiannEawTOU0PljBcsopl1wKUpAM5XXKIYdMmQqWZxuLqmcrW4YMBRV4u3irmzLlkEM22WQ/10ZXsCGZpmwOh0ybTaY9r+2GyyXD6ZRpt8u05b2YhtMpw+WSKyhI+X8hRdYdDhmmKVewZ9sNh0MyTZn56g6Hwy9jr6h6Sd5PwTbTL2NPkrJdkiEpqFDdkCHTo26aynnfyJT9XN1hhPhl7OXUHbLJVahuN7NlyJTDCPFoo93MlmTKea6eOwbLOvbOV3fJJaecsssuW77/f6Lg+8YVnNN+f4w9STKysyXDkBnk2SdbdrbMgvUC75vcudUfY+98dV/m8vzjsqxjz6Neirk8/3gq69jLq5duLs8/f5Z17BVXL24uLzQmyzD2iqsXN5dXpHWEP8eeP9YR/hp7ZV1HSPLb2CvrOkLy39gr6zoid3yWxxq2NHO5pHJZw0oln8sllcsaNqdesrk8p4X+X8Oer17UXG7aXOW2hpVKNpc7nc5yW8Oer+5tLg+2meW2hvWo+zCXO4yQclvD5tV9m8uDFVxua9ji6gXnbPeatxzWsMXVC87lBXOKirSOKMn7yVcBC6WOHj0qp9OpWrVqedRr1aqlgwcPen3Ozp07NXHiRK1evVpBQb41ffr06Xr00UcL1VNSUhQZGSlJqlGjhho1aqQ9e/boyJEj7mNiY2MVGxurn376Senp6e56w4YNVbNmTW3ZskVnzpxx15s1a6aYmBilpKR4/AJbtWqlkJAQbdiwwaMNbdq0UVZWljZv3uyu2e12tW3bVunp6dq+fbu7Hh4ertatW+vo0aPavXu3u94z1qWPf7Xr8mqmrqiWNyB3pBtaddBQp1qmmkbn1b8/Zui7o4Z6xroUG5HXllUHDe1IN3RDvEsx+eaUj/fZtO+0dGsjl8cE+t4em045pOFN8gbeBvtYtdnzsrKCKmtz3NC8Prmy1Hbvy0oPr6/tdQbm9SnruFrve1NHK1+q3TV6uuvRGb8o8eBiHajSTvuqXOmu1zi5RY2OJGtP9W46UrmFux772zrF/rZWP9Xqp/SIBpKkwRHhWpe5Tj87flbv8N6KtkW7j19xdoXSnGkaGDFQwfkmymUZy5RhZmhw5OD8vyYtPL1QEUaE+kX0c9eyzWwtzFio2vba6h6W9zHSdFe6lp1ZpoZBDXVlaE7b04aYCj1wQNWTk3WyVUudbH2Z+/iInTtVZc0a/d6+vTKaNHHXK2/aqKiNm3S8a1dl1q3rrsesWaPInTt1pG8fOaJj3PVqyckKO3BABwfdLDMor081P1gq++kMpQ0Z4q6d3LDBL2MvOjpaiYmJOnDggPbt25f3eyrB+2l4E5dfxp4kzdtpU6Ug6aaEvHq2S5q30656kVLv2Lz671nSoj12NYk2dXXtnPfHBvtYv4w9SWp4JFk1T27RlnpDdCakqrveLG2xYs78opQGd8hpy+tsq1/nK8RxUhsSxkrKGb9S2ceeJKU507Ti7Aq1CG6hViGt3PWfs3/Wuqx1ahvSVo2DG7vrm7M2a3P2ZnUJ66I69jpKG5Lz+vhj7ElSnXfekTMyQoevH+CuGY5s1X37HWXWqaNjPfPmgqD031Vr6QfKaNRIv3fsqJPn5lB/jD2pbHN5/vFX1rEnSfsyVOq5fEPlsXl9KuPYy1XauXxwZH93vaxjL1dp5/K0IYM8+lSWsZertHN5RVpH+HPslXUd4TRC/Db2yrqOkPb7beyVdR0hLfHb2CvrOiLhzJlyW8OWZi6XVC5rWKnkc7nSVC5rWKnkc3mUPi+XNaxU8rn8ZKuUclvDSiWbyx1btpTbGlYq2Vw+vImr3NawUsnm8g32seW2hs3l61w+OCK83NawuXydy3PXvOWxhs3l61yeu+a9EPKI872fatSoIV8YZv5ozUIHDhxQvXr1tGbNGo+P4T3xxBNasGCBxwsg5aR3V155pUaOHKnRo0dLkqZOnaqlS5dq48aNRV7H206puLg4HTt2TFFRUZIu7J1SzaZ8UmF2Sm0Lvb3C7JRqFx9XYXZKLXjWUaF2SjVN+b7C7JRKfPiTCrNTalvo7RVmp1S7+LictleAnVILnj23O6kC7JRqmvJ9zjkqwE6pxIc/cdcDvVNqe9jteX0K8E6pNgkJeX0K8E6pd581POqB3Cl1yY9bKsw6ouHEjyrMTqndYbdWmJ1SrRPqVZidUu9MP1thdkpdunlTzmtRQXZKNXzo4wqzU+rn0FsrzE6pK+LrVJidUm89nVlhdko125hSYXZKJT78SYXZKbUt9PYKs1OqXXxchdkp5V7zVoCdUrlr3gshjzjf++nUqVOKjo5Wenq6O3vxJmA7papXry673V5oV9Thw4cL7Z6SpJMnT2rDhg1KSUnR3XffLUlyuVwyTVNBQUH67LPP1K1bt0LPCw0NVWhoaKF6UFBQod1Wub/cguz5B5IP9aJ2cZWkbhiG13rBNjrMnDeWyzTk8hIvOk1DTi91h2nkzIo+1rNdRuGicibRXEFm1rm/mfn+nscoom6TSzavdad7S71H/dwkWlDOpHiuXcr7u0OOQscWPKa4uimzRHXXuf8kyZadd33DlbOluSDD6ZTh9NJXh/e2F1nP9t4nI189/7gqy9grbT3/+yb/uCrL2MtlFlk3vNZdMpT768g/Nssy9nype3sf5K/nH1NlGXu+1J3n/iso932Tf/xKZRt7eY03vdaNourn3je+ztlWzOUFx2VZxp5HvRRzubfxVNqx56nkc3lJxmRxY8/XelFzudcxWcqx53O9iLm8Iq0j/Dn2yrqOMOS/seePdYS/xp4/1hH+GntlXUcY58KD8ljDlrZeHmvYXCWdy8tjDetLveD7wDzXcX+vYX2pF5zLc8dneaxh8xrv21yeO8eWxxrWl3r+90H+cejvNaxH3Ye5PP/48fca1lPxc3n+MejvNayv9dw2FFzz+nMN63P93FxecA6tSOuIkr5vfBGwG52HhIQoKSlJycnJHvXk5GR1zLfFLVdUVJR++OEHbdy40f1n9OjRatq0qTZu3Kj27dtb1XQAAAAAAACUUcB2SknShAkTdNttt6lNmzbq0KGDXnvtNaWmpro/njdp0iTt379f8+fPl81mU4sWLTyeX7NmTYWFhRWqAwAAAAAAoGILaCg1ePBgHTt2TNOmTVNaWppatGih5cuXq0GDnJutpaWlKTU1NZBNBAAAAAAAQDkIaCglSWPGjNGYMWO8PjZv3rzzPnfq1KmaOnWq/xsFAAAAAACAchWwe0oBAAAAAADgj4tQCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYLeCg1a9YsJSQkKCwsTElJSVq9enWRxy5evFg9e/ZUjRo1FBUVpQ4dOujTTz+1sLUAAAAAAADwh4CGUgsXLtS9996ryZMnKyUlRZ07d1bv3r2Vmprq9fhVq1apZ8+eWr58ub777jt17dpV/fr1U0pKisUtBwAAAAAAQFkEBfLiM2bM0MiRIzVq1ChJ0syZM/Xpp5/qlVde0fTp0wsdP3PmTI+fn3zySX3wwQdatmyZLr/8cq/XyMzMVGZmpvvnEydOSJIcDoccDockyWazyWazyeVyyeVyuY/NrTudTpmmWWzdbrfLMAz3efPXJcnpdPpUDwoKkmmaHnXDMGS32wu1Mcgw5TAN2QxTdiPvHC5TcpqG7IYpW76605RcpqEgw5SRv+6SXCpcd7gkU4aCbXn9zKtLwfliTYcRIruZJcmQ0wj27JOZJbNA3ZApu5ktl2xyGUFe6na5DLu7bpNTNtMpl2GXS/nqplM2OeU0gmUqp/HBCpZTTrnkUpCCZCivUw45ZMpUsDzbWFQ9W9kyZCiowNvFW92UKYccsskm+7k2uoINyTRlczhk2mwy7XltN1wuGU6nTLtdpi3vxTScThkul1xBQcr/Cymy7nDIME25gj3bbjgckmnKzFd3OBx+GXtF1Uvyfgq2mX4Ze5KU7ZIMSUGF6oYMmR5101TO+0am7OfqDiPEL2Mvp+6QTa5CdbuZLUOmHEaIRxvtZrYkU85z9dwxWNaxd766Sy455ZRddtny/f8TBd83ruCc9vtj7EmSkZ0tGYbMIM8+2bKzZRasF3jf5M6t/hh756v7MpfnH5dlHXse9VLM5fnHU1nHXl69dHN5/vmzrGOvuHpxc3mhMVmGsVdcvbi5vCKtI/w59vyxjvDX2CvrOkKS38ZeWdcRkv/GXlnXEbnjszzWsKWZyyWVyxpWKvlcLqlc1rA59ZLN5Tkt9P8a9nz1ouZy0+YqtzWsVLK53Ol0ltsa9nx1b3N5sM0stzWsR92HudxhhJTbGjav7ttcHqzgclvDFlcvOGe717zlsIYtrl5wLi+YU1SkdURJ3k++ClgolZWVpe+++04TJ070qF977bVas2aNT+dwuVw6efKkqlatWuQx06dP16OPPlqonpKSosjISElSjRo11KhRI+3Zs0dHjhxxHxMbG6vY2Fj99NNPSk9Pd9cbNmyomjVrasuWLTpz5oy73qxZM8XExCglJcXjF9iqVSuFhIRow4YNHm1o06aNsrKytHnzZnfNbrerbdu2Sk9P1/bt29318PBwtW7dWkePHtXu3bvd9Z6xLn38q12XVzN1RbW8Abkj3dCqg4Y61TLVNDqv/v0xQ98dNdQz1qXYiLy2rDpoaEe6oRviXYrJN6d8vM+mfaelWxu5PCbQ9/bYdMohDW+SN/A22MeqzZ6XlRVUWZvjhub1yZWltntfVnp4fW2vMzCvT1nH1Xrfmzpa+VLtrtHTXY/O+EWJBxfrQJV22lflSne9xsktanQkWXuqd9ORyi3c9djf1in2t7X6qVY/pUc0kCQNjgjXusx1+tnxs3qH91a0Ldp9/IqzK5TmTNPAiIEKzjdRLstYpgwzQ4MjB+f/NWnh6YWKMCLUL6Kfu5ZtZmthxkLVttdW97Du7nq6K13LzixTw6CGujI0p+1pQ0yFHjig6snJOtmqpU62vsx9fMTOnaqyZo1+b99eGU2auOuVN21U1MZNOt61qzLr1nXXY9asUeTOnTrSt48c0THuerXkZIUdOKCDg26WGZTXp5ofLJX9dIbShgxx105u2OCXsRcdHa3ExEQdOHBA+/bty/s9leD9NLyJyy9jT5Lm7bSpUpB0U0JePdslzdtpV71IqXdsXv33LGnRHruaRJu6unbO+2ODfaxfxp4kNTySrJont2hLvSE6E5I3PzVLW6yYM78opcEdctryOtvq1/kKcZzUhoSxknLGr1T2sSdJac40rTi7Qi2CW6hVSCt3/efsn7Uua53ahrRV4+DG7vrmrM3anL1ZXcK6qI69jtKG5Lw+/hh7klTnnXfkjIzQ4esHuGuGI1t1335HmXXq6FjPvLkgKP131Vr6gTIaNdLvHTvq5Lk51B9jTyrbXJ5//JV17EnSvgyVei7fUHlsXp/KOPZylXYuHxzZ310v69jLVdq5PG3III8+lWXs5SrtXF6R1hH+HHtlXUc4jRC/jb2yriOk/X4be2VdR0hL/Db2yrqOSDhzptzWsKWZyyWVyxpWKvlcrjSVyxpWKvlcHqXPy2UNK5V8Lj/ZKqXc1rBSyeZyx5Yt5baGlUo2lw9v4iq3NaxUsrl8g31sua1hc/k6lw+OCC+3NWwuX+fy3DVveaxhc/k6l+eueS+EPOJ876caNWrIF4aZP1qz0IEDB1SvXj1988036pjvF/Xkk0/qzTff1I4dO4o9xz//+U899dRT2rZtm2rWrOn1GG87peLi4nTs2DFFRUVJurB3SjWb8kmF2Sm1LfT2CrNTql18XIXZKbXgWUeF2inVNOX7CrNTKvHhTyrMTqltobdXmJ1S7eLjctpeAXZKLXj23O6kCrBTqmnK9znnqAA7pRIf/sRdD/ROqe1ht+f1KcA7pdokJOT1KcA7pd591vCoB3Kn1CU/bqkw64iGEz+qMDuldofdWmF2SrVOqFdhdkq9M/1shdkpdenmTTmvRQXZKdXwoY8rzE6pn0NvrTA7pa6Ir1Nhdkq99XRmhdkp1WxjSoXZKZX48CcVZqfUttDbK8xOqXbxcRVmp5R7zVsBdkrlrnkvhDzifO+nU6dOKTo6Wunp6e7sxZuAfnxPyulEfqZpFqp583//93+aOnWqPvjggyIDKUkKDQ1VaGhooXpQUJCCCg6oc7/cguz5B5IP9YLnLU3dMAyv9YJtdJg5r5XLNOTyEi86TUNOL3WHaeTMij7Ws13efyfZ+YL+IDPr3N/MfH/PYxRRt8klm9e6072l3qN+bhItKGdSPNcu5f3dIUehYwseU1zdlFmiuuvcf5Jky867vuHK2dJckOF0ynB66avDe9uLrGd775ORr55/XJVl7JW2nv99k39clWXs5TKLrBte6y4Zyv115B+bZRl7vtS9vQ/y1/OPqbKMPV/qznP/FZT7vsk/fqWyjb28xpte60ZR9XPvG1/nbCvm8oLjsixjz6Neirnc23gq7djzVPK5vCRjsrix52u9qLnc65gs5djzuV7EXF6R1hH+HHtlXUcY8t/Y88c6wl9jzx/rCH+NvbKuI3LX5OWxhi1tvTzWsLlKOpeXxxrWl3rB94F5ruP+XsP6Ui84l+eOz/JYw+Y13re5PHeOLY81rC/1/O+D/OPQ32tYj7oPc3n+8ePvNayn4ufy/GPQ32tYX+u5bSi45vXnGtbn+rm5vOAcWpHWESV93/giYKFU9erVZbfbdfDgQY/64cOHVatWrfM+d+HChRo5cqQWLVqkHj16lGczAQAAAAAAUA4C9u17ISEhSkpKUnJyskc9OTnZ4+N8Bf3f//2fhg8frnfeeUd9+vQp72YCAAAAAACgHAT043sTJkzQbbfdpjZt2qhDhw567bXXlJqaqtGjR0uSJk2apP3792v+/PmScgKpoUOH6vnnn9eVV17p3mUVHh6u6OjoIq8DAAAAAACAiiWgodTgwYN17NgxTZs2TWlpaWrRooWWL1+uBg1yvgEgLS1Nqamp7uNnz54th8OhsWPHauzYvDv8Dxs2TPPmzbO6+QAAAAAAACilgN/ofMyYMRozZozXxwoGTV999VX5NwgAAAAAAADlLmD3lAIAAAAAAMAfF6EUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwXMC/fQ8AAAAAAPwx2Q27qgdXly2Ae2ZcdRwBu3ZBZ8+eDXQTfBIcHCy73V7m8xBKAQAAAAAAy1UJqqJ74u9RdEi0DBkBa4fzHwG7dCF79uwJdBN8FhMTo9q1a8swSv+7I5QCAAAAAACWMmRoYK2BqhdVT5HVIhXATEpxR8zAXbyAsISEQDehWKZpKiMjQ4cPH5Yk1alTp9TnIpQCAAAAAACWqmSvpMSoREXERMgWEtjbXYfaKlAoFRYW6Cb4JDw8XJJ0+PBh1axZs9Qf5eNG5wAAAAAAwFIR9ggFGUEy7AHcIoUyiYiIkCRlZ2eX+hyEUgAAAAAAwFLuG5uTSV2wynIvqVyEUgAAAAAAALAcoRQAAAAAAAAsx43OAQAAAABAhdDn2b2WXu+/98WX6nnrNm5Uj2HD1K1DB3346qv+bdQfCDulAAAAAAAASuDNJUt015AhWvv99/o1LS1g7SjLTcYrAkIpAAAAAAAAH53OyNDiTz/VHYMGqXeXLlqwdKnH4x99+aU6DR6sKklJiuvcWbfce6/7scysLE2eMUNNevRQzBVXqGWfPpq3eLEkacHSpYqJifE419KlSz1uKD516lRddtlleuONN9SwYUOFhobKNE198sknuuqqqxQTE6Nq1aqpb9++2rVrl8e59u3bp1tuuUVVq1ZVZGSk2rRpo2+//VZ79+6VzWbThg0bPI5/8cUX1aBBA5mmWfYXrQiEUgAAAAAAAD5679NP1SQ+XpckJOiWvn214IMP3MHNx6tW6c/jx6vX1Vdr7aJF+u/rr+uK5s3dzx310ENa9PHHenbSJKV88IFemDJFlSIiSnT9n3/+Wf/5z3/0/vvva+PGjZKk06dPa8KECVq/fr1WrFghm82mG264QS6XS5J06tQpdenSRQcOHNCHH36oTZs26YEHHpDL5VJ8fLx69OihuXPnelxn7ty5Gj58uF++Za8o3FMKAAAAAADAR28uXqw/9+0rSbq2UyedzsjQl+vWqVuHDnrmtdd0c69emjJ2rPv4Vk2bSpJ27t2r9z/9VB+99pq6deggSUqIiyvx9bOysrRgwQLVqFHDXbvxxhs9jpkzZ45q1qyprVu3qkWLFnrnnXd05MgRrV+/XlWrVpUkNW7c2H38qFGjNHr0aM2YMUOhoaHatGmTNm7cqMXndnGVF3ZKAQAAAAAA+OCnPXu0YcsW3dSrlyQpKChIN153neYvWSJJ2rxjh65p397rczdv3y673a7ObdqUqQ0NGjTwCKQkadeuXRoyZIgaNmyoqKgoJSQkSJJSU1MlSRs3btTll1/uDqQKGjBggIKCgrTkXD/eeOMNde3aVfHx8WVqa3HYKQUAAAAAAOCDN5cskcPhUOMePdw10zQVHBSk39LTFR4aWuRzw8LCzntum81W6P5N3m5kHhkZWajWr18/xcXF6d///rfq1q0rl8ulFi1aKCsrS5IUHh5+3muHhITotttu09y5czVw4EC98847mjlz5nmf4w/slAIAAAAAACiGw+HQ2x9+qKfuu0/rFi1y//n2vfdUv25dvfvf/6rFJZfoq2+/9fr8Fk2ayOVyaXWBG4rnql6lik6ePKnTp0+7a7n3jDqfY8eOadu2bfrHP/6h7t27KzExUb/99pvHMa1atdLGjRt1/PjxIs8zatQoff7555o1a5ays7M1cODAYq9dVoRSAAAAAAAAxVi+cqV+P3FCwwYOVPMmTTz+DOjZU28uWaKH7rpL//n4Yz328svavnu3tvz0k2a88YYkqUG9erq1f3+Nfvhhfbhihfbu26dV69fr/U8+kSS1bdVKEREReuihh/Tzzz/rnXfe0bx584ptV5UqVVStWjW99tpr+vnnn/XFF19owoQJHsf8+c9/Vu3atTVgwAB988032r17t95//32tXbvWfUxiYqKuvPJKPfjgg/rzn/9c7O4qf+DjewAAAAAAoEL4733xgW5Ckd5cskRdr7xS0ZUrF3psQI8e+ue//63KkZF661//0lOzZ+tfc+YoqlIldUpKch/3wpQpeuT553XvE0/o+O+/K65OHd0/apQkqWp0tN566y3df//9eu2119SjRw9NnTpVf/3rX8/bLpvNpnfffVfjxo1TixYt1LRpU73wwgu65ppr3MeEhITos88+09///nf96U9/ksPh0KWXXqqXX37Z41wjR47UmjVrNGLEiDK8Ur4jlAIAAAAAACjG+y+9VORjl196qTJ++MH99wH57jmVX1hoqJ5+4AE9/cADXh8fMGCABgwY4FG744473H+fOnWqpk6dWuh5PXr00NatWz1qBe9P1aBBA7333ntF9kGS0tLS1KJFC7Vt2/a8x/kLH98DAAAAAAD4Azt16pTWr1+vF198UePGjbPsuoRSAAAAAAAAf2B33323rrrqKnXp0sWyj+5JfHwPAAAAAADgD23evHk+3VTd39gpBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAAAVVHx8vGbOnOn3YyuCoEA3AAAAAAAAQJKav9TR0uv9ePeaEh3/18mT9daHH0qSgoKCFFurlq7v0UP/GDNGkRER5dFErV+/XpGRkX4/tiIglAIAAAAAAPBRz06dNPvxx+VwOPTNd99pzNSpOn3mjF6YMsXjuOzsbAUHB5f5ejVq1CiXYysCPr4HAAAAAADgo9CQENWuXl2xtWtrcJ8+Gtynj5Z98YUenzVL7W+6SW8uWaJLe/VSTFKSTNNU+smTGjt1qhp06aJaV16p3iNHavOOHR7n/OjLL9Vp8GCFhYWpevXqGjhwoPuxgh/Jmzp1qurXr6/Q0FDVrVtX48aNK/LY1NRUXX/99apUqZKioqI0aNAgHTp0yONcl112mRYsWKD4+HhFR0frlltu0cmTJ/3/wnlBKAUAAAAAAFBK4aGhcjgckqTdqal6/9NP9c5zz2ndokWSpIFjx+rQsWNaPGuWvlm4UJclJqrPqFE6np4uSfp41Sr9efx49br6aqWkpGjFihVq06aN12u99957eu655zR79mzt3LlTS5cuVcuWLb0ea5qmBgwYoOPHj2vlypVKTk7Wrl27NHjwYI/jdu3apaVLl+qjjz7SRx99pJUrV+qpp57y18tzXnx8DwAAAAAAoBTW//CD/rN8ua5p316SlJWdrTlPPqkaVatKkr769lv9uHOnflm5UqEhIZKk6ffdp2VffKEln32mkTffrGdee0039+qlKWPHKjwxUZLUunVrr9dLTU1V7dq11aNHDwUHB6t+/fpq166d12M///xzbd68WXv27FFcXJwkacGCBWrevLnWr1+vtm3bSpJcLpfmzZunypUrS5Juu+02rVixQk888YSfXqWisVMKAAAAAADARx+vWqUa7dqpSlKSuv7lL+qUlKR/TZokSapft647kJKklK1bdSojQ7FXXaUa7dq5/+zdv197fv1VkrR5xw53qFWcm2++WWfOnFHDhg11xx13aMmSJe5dWgVt27ZNcXFx7kBKki699FLFxMRo27Zt7lp8fLw7kJKkOnXq6PDhw76/IGXATikAAAAAAAAfdWnbVs9PmaLgoCDVqVHD42bmEeHhHse6XC7Vrl5dn86dW+g80eeCoPDQUJ+vHRcXpx07dig5OVmff/65xowZo3/+859auXJloZuqm6YpwzAKnaNgveDzDMOQy+XyuU1lwU4pAAAAAAAAH0WEh6tR/fqqX7dusd+ud1liog4dO6Ygu12N6tf3+FO9ShVJUotLLtFX337r8/XDw8PVv39/vfDCC/rqq6+0du1a/fDDD4WOu/TSS5Wamqpfz+3IkqStW7cqPT1diec+Jhho7JQCAAAAAAAoB906dFD71q016J579Pj48bokPl5pR47ok1Wr1K97dyU1b66H7rpLfxo1Sglxcbpt3Dg5HA59/PHHeuCBBwqdb968eXI6nWrfvr0iIiK0YMEChYeHq0GDBoWO7dGjh1q1aqVbb71VM2fOlMPh0JgxY9SlS5cib6RuNUIpAAAAAABQIfx495pAN8GvDMPQklmzNPWFFzT64Yd19Phx1apeXVclJalWtWqSpKvbttVb//qXnpo9W/964w1FRUXp6quv9nq+mJgYPfXUU5owYYKcTqdatmypZcuWqdq5cxW89tKlS/W3v/1NV199tWw2m3r16qUXX3yxXPtcEoRSAAAAAAAAPnjtPN9I948xY/SPMWMK1StHRupfkya5b4buzYAePTSgRw+Ft2hR6LG9e/fmHTdggAYMGFDkefIfK0n169fXBx98UOTxU6dO1dSpUz1q9957r+69994in+NP3FMKAAAAAAAAliOUAgAAAAAAgOUIpQAAAAAAAGA5QikAAAAAAABYjlAKAAAAAAAAliOUAgAAAAAAgOUIpQAAAAAAAGA5QikAAAAAAABYjlAKAAAAAAAAliOUAgAAAAAAuEDEx8dr5syZ7p8Nw9DSpUsD1p6yCAp0AwAAAAAAACTplv/eYun13u3zbomO/+vkyXrrww8lSXa7XXVq1FCvq6/Wo+PGqUp0dHk08aJGKAUAAAAAAOCjnp06afbjj8vhcGj77t0aPWWK0k+e1JvPPBPopl1w+PgeAAAAAACAj0JDQlS7enXF1q6tHh076qZevbRizRr34/OXLNHl/furSlKSLuvXT7Pf9dyNte/gQQ29/37V69RJ1du1U6fBg/W/zZslSbt27dL111+vWrVqqVKlSmrbtq0+//xzS/tnJXZKAQAAAAAAlMKeX39V8jffKCgoJ15547339PisWZrx0EO6rFkzbdy+XXdPnarI8HD95frrdSojQ9fdfrvq1qypRS++qFrVq2vj1q0yXS5J0qlTp/SnP/1Jjz/+uMLCwvTmm2+qX79+2rFjh+rXrx/IrpYLQikAAAAAAAAffbxqlWq0ayeny6WzmZmSpKfvv1+S9NTs2Xrqvvs0oEcPSVJ8bKy279qlOYsW6S/XX6+F//2vjv72m1a/+66qnrsHVaN8YVPr1q3VunVr98+PP/64lixZog8//FB33323VV20DKEUAAAAAACAj7q0bavnp0xRxpkzmrd4sXb+8ovuGjJER44f176DB3XXI49o7NSp7uMdTqeiK1WSJG3esUOtmzVzB1IFnT59Wo8++qg++ugjHThwQA6HQ2fOnFFqaqoVXbMcoRQAAAAAAICPIsLD3bub/jVpknqNGKEnXnlFo//8Z0nSy488oratWnk8x27LuaV3eGjoec99//3369NPP9Wzzz6rxo0bKzw8XDfddJOysrLKoSeBx43OAQAAAAAASumhu+7S82++KafLpbo1a2rPvn1qVL++x5/42FhJUotLLtHmHTt0PD3d67lWr16t4cOH64YbblDLli1Vu3Zt7d2718LeWItQCgAAAAAAoJSubttWiY0a6Z///rcmjxmjZ+fM0ctvvaWde/dqy08/af6SJXrhzTclSYP+9CfVql5dg8eN09qUFO359VctTU7Wtxs3SpIaN26sxYsXa+PGjdq0aZOGDBki17mboF+M+PgeAAAAAACoEN7t826gm1Aq44YO1Z1TpmjL8uWaNXWqnps3T5NnzFBkeLiaN2misbfdJkkKCQ7WstmzNfHZZ3XDmDFyOJ1q1rChnps8WZL03HPPacSIEerYsaOqV6+uBx98UCdOnAhk18oVoRQAAAAAAIAPXnviCa/1wX36aHCfPoX+7k39unX1zowZXh+Lj4/XF1984VEbO3asx88FP85nmmZxza6w+PgeAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALEcoBQAAAAAAAMsRSgEAAAAAAMByhFIAAAAAAACwHKEUAAAAAAAALBcU6AYAAAAAAABIku2qmyy9nuvr9yy9HjyxUwoAAAAAAMAHf508WREtWxb6sys1VZL09YYNuvHuu9WwWzdFtGypD1esKPacTqdT/3z9dV3Wr5/Cw8NVtWpVXXnllZo7d255dyfg2CkFAAAAAADgo56dOmn244971GpUqSJJOn3mjFpecoluGzBAQ8aP9+l8j8+apbnvvacZDz2kjgMH6sSJE9qwYYN+++03v7c9V1ZWlkJCQsrt/L5ipxQAAAAAAICPQkNCVLt6dY8/drtdknRd586aOm6cBvTo4fP5lq9cqTtuuUUDr7tOCQkJat26tUaOHKkJEya4j3G5XHr66afVuHFjhYaGqn79+nriiSfcj//www/q1q2bwsPDVa1aNf31r3/VqVOn3I8PHz5cAwYM0PTp01W3bl1dcsklkqT9+/dr8ODBqlKliqpVq6brr79ee/fuLeMr5DtCKQAAAAAAgACpVb26Vn77rY4cP17kMZMmTdLTTz+tKVOmaOvWrXrnnXdUq1YtSVJGRoZ69eqlKlWqaP369Vq0aJE+//xz3X333R7nWLFihbZt26bk5GR99NFHysjIUNeuXVWpUiWtWrVKX3/9tSpVqqRevXopKyurXPuci4/vAQAAAAAA+OjjVatUo10798/XXnWV3p4xo9Tne/r++3XrhAlK6NpVzZs3V8eOHXX99derd+/ekqSTJ0/q+eef10svvaRhw4ZJkho1aqSrrrpKkvT222/rzJkzmj9/viIjIyVJL730kvr166enn37aHV5FRkbq9ddfd39s74033pDNZtPrr78uwzAkSXPnzlVMTIy++uorXXvttaXuk68IpQAAAAAAAHzUpW1bPT9livvniPDwMp0vsVEjbViyRN9v3aoNaWlatWqV+vXrp+HDh+v111/Xtm3blJmZqe7du3t9/rZt29S6dWt3ICVJnTp1ksvl0o4dO9yhVMuWLT3uI/Xdd9/p559/VuXKlT3Od/bsWe3atatMffIVoRQAAAAAAICPIsLD1ah+fb+e02azqU2LFup8yy0aP3683nrrLd12222aPHmywosJvUzTdO90Kih/PX9oJeXcpyopKUlvv/12oefVqFGjFL0oOe4pBQAAAAAAUIFceumlkqTTp0+rSZMmCg8P14oVK4o8duPGjTp9+rS79s0338hms7lvaO7NFVdcoZ07d6pmzZpq3Lixx5/o6Gj/dqgIhFIAAAAAAAB+cCojQ5u2b9em7dslSb/s369N27fr17S0Ip8zZMIEvTh/vv63ebN++eUXffXVVxo7dqwuueQSNWvWTGFhYXrwwQf1wAMPaP78+dq1a5fWrVunOXPmSJJuvfVWhYWFadiwYdqyZYu+/PJL/e1vf9Ntt93m/uieN7feequqV6+u66+/XqtXr9aePXu0cuVK3XPPPdq3b59/X5gi8PE9AAAAAABQIbi+fi/QTSiT73/8Ub1GjHD//OA//ylJ+kv//nrtiSe8PqdHx45a9PHHenbOHKWfOqXatWurW7dumjp1qoKCcmKbKVOmKCgoSA8//LAOHDigOnXqaPTo0ZKkiIgIffrpp7rnnnvUtm1bRURE6MYbb9SMYm6+HhERoVWrVunBBx/UwIEDdfLkSdWrV0/du3dXVFSUP16OYhFKAQAAAAAA+KCoYCnX1W3bKuOHH0p0zhE33aQRN90kSQpv0cLrMTabTZMnT9bkyZO9Pt6yZUt98cUXRV5j3rx5Xuu1a9fWm2++WaL2+hMf3wMAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAJZyyZXzFzOw7UDpmWbZf3mEUgAAAAAAwFIZzgw5TIdMJ6nUhSojI0OSFBwcXOpzBPmrMQAAAAAAAL445TylbSe2KTo8WpH2SMkIXFsyXRUnGDPOng10E4plmqYyMjJ0+PBhxcTEyG63l/pchFIAAAAAAMBSpky9f+h9xYXHKfpstIwAplLmiYBdupCy7DqyWkxMjGrXrl2mcxBKAQAAAAAAy/3u+F2P7XpM1YKrya7S77Ypq+decwTs2gUlfLw80E3wSXBwcJl2SOUilAIAAAAAAAHhNJ06nHU4oG2wpVWcUCosLCzQTbBUwG90PmvWLCUkJCgsLExJSUlavXr1eY9fuXKlkpKSFBYWpoYNG+rVV1+1qKUAAAAAAADwl4CGUgsXLtS9996ryZMnKyUlRZ07d1bv3r2Vmprq9fg9e/boT3/6kzp37qyUlBQ99NBDGjdunN5//32LWw4AAAAAAICyCGgoNWPGDI0cOVKjRo1SYmKiZs6cqbi4OL3yyitej3/11VdVv359zZw5U4mJiRo1apRGjBihZ5991uKWAwAAAAAAoCwCdk+prKwsfffdd5o4caJH/dprr9WaNWu8Pmft2rW69tprPWrXXXed5syZo+zsbK93qc/MzFRmZqb75/T0dEnS8ePH5XDkfG7UZrPJZrPJ5XLJ5XK5j82tO51OmaZZbN1ut8swDPd589clyel0+lQPCgqSaZoedcMwZLfbC7cx67QcpiGbYcqe78sKXKbkNA3ZDVO2fHWnKblMQ0GGKSN/3SW5VLjucEmmDAXbPL8iM6cuBeeLNY8bQbIrW5IhZ4GhFaRsmQXqhkzZ5ZBLNrny3dQur26XK19uapNLNjmLrDsVJPPcNzbYzuRUXHIpSEEe3+TgkEOmTAXLc7wUVc9WtgwZCirQJ291U6Yccsgmm/tGfSdsNsk0ZXM4ZNpsMvPdDM5wuWQ4nTLtdpm2vD4ZTqcMl0uuoCDl/4UUWXc4ZJimXAXeA4bDIZmmzHz148eP+2XsFTkmS/B+smef9svYk6RsV863yAYVqhsyZHrUTVM57xuZsp+rHzeC/DL2cupO2eQqVLfLIUOmHAXGmF0OSaac5+q2MznXKOvYO1/dJZeccsouu2z5+lTwfXPi3Lj0x9iTJCM7WzIMmUGefbJlZ8ssWC/wvjl+/HjOOfww9s5X92Uut2efdtfLOvY86qWYy48bea9ZWcdeXr10c3nu2JXKPvaKqxc3l5+wef5CyjL2iqsXN5f/9ttvFWYdoczTfht7ZV1HpBv+G3tlXUc4zzj9NvbKuo445XT6beyVdR2Ru24ujzVsaeZyV2ZGuaxhpZLP5ScMs1zWsDn1ks3lrjOuclnDnq9e1Fx+0jTLbQ0rlWwu/+2338ptDXu+ure53J59utzWsB51H+by40ZQua1h8+q+zeW2M7ZyW8MWVy84Z7vXvOWwhi2uXnAuz13zXgh5xPneT6dOnTrXbc95uKCAhVJHjx6V0+lUrVq1POq1atXSwYMHvT7n4MGDXo93OBw6evSo6tSpU+g506dP16OPPlqonpCQUIbWw5tqgW6Ah+OBboDblYFuQEHVKtZvqqKoWK8K49crxq5XFetV2RzoBrhVqLFbtWqgW1AhxQS6AR5+D3QD3NoFugH5xcQEugUVVnSgG+Ch4qwb2ge6Afkx93pVsdYNFWfsVqh1w0W25j158qSio4ueNQP+7XtG/v9LQzkpWsFaccd7q+eaNGmSJkyY4P7Z5XLp+PHjqlat2nmvgwvXiRMnFBcXp19//VVRUVGBbg5QIoxfXKgYu7hQMXZxIWP84kLF2L34maapkydPqm7duuc9LmChVPXq1WW32wvtijp8+HCh3VC5ateu7fX4oKAgVSsiTQwNDVVoaKhHLYb/1+cPISoqigkOFyzGLy5UjF1cqBi7uJAxfnGhYuxe3M63QypXwG50HhISoqSkJCUnJ3vUk5OT1bFjR6/P6dChQ6HjP/vsM7Vp08br/aQAAAAAAABQMQX02/cmTJig119/XW+88Ya2bdum8ePHKzU1VaNHj5aU89G7oUOHuo8fPXq0fvnlF02YMEHbtm3TG2+8oTlz5ui+++4LVBcAAAAAAABQCgG9p9TgwYN17NgxTZs2TWlpaWrRooWWL1+uBg0aSJLS0tKUmprqPj4hIUHLly/X+PHj9fLLL6tu3bp64YUXdOONNwaqC6iAQkND9cgjjxT62CZwIWD84kLF2MWFirGLCxnjFxcqxi5yGWZx388HAAAAAAAA+FlAP74HAAAAAACAPyZCKQAAAAAAAFiOUAoAAAAAAACWI5QCAAAAAACA5QilAMACfKcEAAAAAHgilMIFh3/c40LhcrncfzcMQ5J06NAhORyOQDUJKDGn0xnoJgBlwroBF6r86wjgQsB8i9IglMIFI3eSO3XqVIBbAvjGZrNp7969uv/++yVJ77//vgYPHqzDhw8HuGVA8U6ePClJstvt2rBhgzIzMwPcIsB3+/fv18qVKyXl/J8C/EMJF5JffvlFe/fulc1mI5jCBcPlcrn/T1gp799ujGEUh1AKFd7PP/+sL7/8UoZh6L333tPAgQOVnp4e6GYBxXK5XFq+fLkWL16svn376uabb9bIkSNVt27dQDcNOK99+/Zp+PDh+uyzz/T++++rXbt2+v777wPdLMAnWVlZGj58uKZMmaIVK1ZIIpjChSM1NVUJCQnq0qWLfvrpJ4IpXDBstpxo4YUXXtDw4cN1zz33aMOGDYxhFItQChXejBkz1L17dz3yyCMaNGiQhg4dqujo6EA3CyiWzWbT6NGj1bVrVy1fvlzdu3fXbbfdJomPRKFiy8jI0PHjx/Xggw/q1ltv1ZtvvqkOHTqwqMQFISQkRE899ZQcDodmzpypzz//XBLBFC4MP/30k6pWraqoqCgNGDBAW7Zs4R/1qNDyj80pU6boscceU0ZGhr777jv17NlTn3/+OWMY50UohQpv1qxZat++vZ566indf//97n/UAxVZ/n/41K1bV7feequOHj2qMWPGSMr5SBT3lkJFZJqmLrnkEo0cOVI//PCDGjZsqGrVqkkSi0pUeC6XS6ZpKikpSbNmzdKhQ4f0/PPPE0zhgtGyZUvFxcWpefPm6tixowYNGqSt/9/efUZFda9vH/8OHRU0xopdMdHYewyx916xJGJBxdiwizW2GBv2xBpUROyCvR67xl6IRtTYRUUsqAhIm3le+DBHTnKS/HMShoTrs5YruvfMeE/Wdtj7mvt37ytX9PkraVZyh9S9e/cwGAxs376d9evXExgYSNu2bWnYsKGCKflVCqUkzUo+aTSZTCQmJlK8eHEWL17Mvn37LFyZyK8zmUwYDAZOnjzJ2bNnGTFiBN999x0eHh4cO3bMHEzZ2NgAcPPmTQVUkiYkH7tJSUkULFiQRYsWUbhwYWbPns2GDRsABVOSNt2+fZvTp0/z7Nkz80yTsmXLsnDhQh4/fszs2bPN5w8KpiQtSg5Uc+bMyciRI7l58ybVqlWjaNGiuLu7K5iSNC0oKIiCBQuyYcMGsmTJAkDBggWZOHEinp6eNG7cmP3792NlZaXPX/kZhVKSJiVfGF2+fJl79+5x5swZLl68SIsWLWjbti179+5N8finT59aqFKRlJKP3aCgIJo0aUJwcDCRkZHY29vj6elJt27dOHbsGF988QVGo5Fx48bRq1cvYmNjLV26pHPJx+7evXvx9vamRIkS9OjRA19fX6ytrVm8eDGbNm0C3gZTO3bs0PBzSRMePXpEkSJF+Pjjj2nVqhUdO3Zk/fr13L59mwoVKrBu3TqePHnCggUL2L17N6BgStKOe/fumQOn5EC1ZMmS5MiRgzx58vDVV1+RL1++FMGURgBIWlOgQAE+++wzbt26xbNnz4C35xW5c+dmwoQJeHp6Uq9ePc6ePZtiGLoIgMGkn8iSxiRfGAUHBzNkyBB69+5Nu3btKFCgAEajka5du7Jt2zbWrl1LnTp1mDlzJocOHWLz5s3Y2dnpg04sbt++fbRq1Yr58+fj7u5OpkyZzPuio6NZuXIl06ZNw2AwEBMTw7Zt26hcubIFKxZ5a9OmTfTo0YPu3bvTrl0783F55coVBg8eTFJSEg0bNiQqKoqJEydy9+5d8uXLZ+GqJb179eoVHTt2ZNeuXYwYMYKzZ8/y7Nkzrl69SuPGjWncuDEODg7MmDGD4sWL06lTJxo1amTpskW4e/cuRYsWBWDChAm4uLjQpUsXAHx8fDhw4ABnzpzh9OnTTJgwgbCwMFatWkWpUqUsWbakc0aj0bxk710//vgjPj4+nDx5kn/961+ULVvWfF0XFhZGYGAgQ4YMMa8UEEmmUErSpF27duHu7s7UqVPp3Lkzzs7OKfZ36dKFgIAAqlevzpkzZzh69Cjly5e3ULUi/2YymRg8eDCvX79m6dKlREdHExoair+/Pzlz5qRhw4ZUrFiRK1eucP78edzc3ChUqJClyxbhwoUL1K9fn8mTJ+Pl5WXe/vz5c7Jmzcrt27cZM2YM165dIyYmhlWrVulzVywqKioKJycnAF6+fEm7du148OABmzZtwsXFhe3btxMSEsLy5cspWbIkBw8eBKBVq1YEBASQIUMGS5Yvwv79+/H29ubGjRv06dOHU6dOYW9vj7e3N4ULF2by5Mn06tWLOnXqcPz4cUaOHElCQgKHDx/G1tZWX8RKqns3kNq9ezcvXrwgMTGRFi1a4OTkxI0bNxg6dCinTp1i165dKYKpZImJiQqmJAWFUpKmmEwmYmNjcXd3p3Tp0kyZMoXo6GgePnzI9u3bsbGxoX///gAsW7aMqKgoGjdubP6WScSSTCYTJpMJd3d3IiIimDdvHrNnz+bRo0c8ffoUg8FAkSJFWLFiBRkzZrR0uSIpBAYGsmjRIo4ePUpkZCS7d+9m1apVhISE0K9fP0aMGMGLFy948+YNNjY2ZMuWzdIlSzr29OlTSpYsydSpU+natSuA+ZzgwYMHbNmyxdxNEhkZye3bt9mxYwfnzp1jypQpFC9e3ILVS3p3/fp11q9fz5gxY9i5cyfjx4/H0dGR4OBgfH19uXz5MqdPn+bVq1d069aNb7/9FoBTp07h4uKiDlWxuKFDhxIQEEDu3Lm5du0a5cuXZ/DgwbRp04br168zYsQITp8+TXBwMJUqVbJ0uZLGKZSSNMnd3Z333nuPgQMHsmDBAq5evcrNmzeJi4ujTp06BAQEAPwseRdJbb90DP744480bNiQ2NhY6tSpQ4cOHWjVqhXLly9n/vz5HDlyJMWSPhFLeff4PXDgAHXr1mXUqFEcOnSIrFmzkidPHvLly8eYMWM4d+4c5cqVs3DFIm8lJiYyaNAg/Pz8WLZsGR06dADeBlPNmzfn9u3bbNu27WfLnOLi4rC3t7dEySLA206TWbNm4evry5kzZ8iZMyd79uxh8ODBlClTho0bNwJv7z4dGBiIl5eXeUmfSFqwatUqhg4dyq5duyhatChv3ryhS5cuREVFMWbMGOrXr88PP/zAgAEDcHJyYuvWrZYuWdI4DTqXNOmjjz4iJCSE0qVL8/jxY7p3705ISAg9evTg9evX5juPKJASS0q+oD906BAjR46kQ4cOLF++HFdXV65cucLBgwdZt24dLVu2BN7O5cmePbuG64rFJR+D8fHxwNuLpNq1a+Pr68v27dspX74848ePZ8GCBYwcOZKKFSvy5s0bS5YsYmYymbCxsWHWrFkMGDAADw8P1q5dC2C+ACpUqBDNmzfn8uXLKZ6rQEoszcrKilq1avHmzRv27duHnZ0d9erVY/bs2YSEhFC/fn0A+vTpw9atWxVIiUUtWrTIPLg82Y0bNyhZsiRlypTB0dGRbNmysWLFChITE5k7dy4ApUuXxs/Pj82bN1ugavm7UaeUWFTyRf2lS5d49OgR0dHRNGnSBDs7O65du8b9+/epW7eu+XE9e/YkKiqKgIAAbG1tLV2+CMHBwXh6etK0aVNy587N7Nmzad++PXPmzDEvbzpx4gRbtmxh4cKFHDlyhDJlyli4aknPkj9Pd+/eTWBgII8ePaJ06dJ069aNUqVKpZjTAzBq1CjWr1/PsWPHyJUrlwUrl/Tu5cuXWFlZpTg+4+PjGTNmDLNnz2blypV07NgReNsx1bp1a06fPs2JEyf46KOPLFW2yC/y9vZm37597N+/HxcXF+Lj49m3bx9DhgwhT5487N+/H9D8HbEcPz8/9u3bR2BgINbW1sDbc4ihQ4fy/fffc+LECeDfHahHjhyhYcOGnDt3LsUS6f82GF0kmY4OsSiDwcDGjRupVasWw4cPp02bNlSvXp3Fixfz4YcfUrduXQDu37/P8OHD2bRpE2PHjlUgJWnCnTt3GDVqFFOnTiUgIIDp06djb29P3rx5zYHUnTt3WLhwIXv37uXo0aMKpMTiDAYDW7dupWXLluTIkQMXFxeuXLmCm5sbhw4dMl/w7927F09PT5YuXcqGDRsUSIlF3bx5k4oVK5rPEYKDgwGws7Nj+vTpDB06FA8PD1avXg287ZjatGkTNWrUwM7OzpKli5gld/oDNG7cmPj4eC5cuAC8PZbr16/PzJkziYiIoEqVKgAKpMRiunfvbg6kDh48yIMHDzAYDLRr145Tp04xe/Zs4N8dqHFxcRQpUoQsWbKkeB0FUvKbTCIWdP78edP7779v+u6770zPnj0zPXjwwOTh4WGqXr26aeHChSaTyWTau3evqX379qaSJUuaLly4YNmCJd0zGo3m3//000+mSpUqmX+fJ08eU8+ePc37L126ZDKZTKYbN26YHj16lLqFivwXL1++NFWvXt00ceJE87Z79+6ZevbsacqcObMpJCTEFBMTY1qyZImpffv2psuXL1uwWhGT6fnz56YZM2aYMmbMaDIYDKZGjRqZcubMaapYsaKpffv2pkOHDplCQ0NNU6ZMMdna2pq2bNlifu67n9kilvDw4UPT2bNnf3FfrVq1TNWrV0+xLT4+3rRp0yZTpUqVTHfv3k2NEkV+JjEx0fz7Q4cOmQoWLGgaPny46eHDhyaTyWSaOnWqyc7OzjRp0iTTjRs3TDdu3DA1btzYVLt2bVNSUpKlypa/KcWWYlE//vgjuXLlol27drz33nu4uLgwbdo0XFxcWLt2LSaTiXr16uHh4WG+raiIJRkMBoKDg9m7dy9xcXHcv3+fw4cP07BhQxo3bszChQsBOHfuHF9++SWhoaEUKVJEXSaSZsTFxXHz5k3y5s1r3pY3b15GjRpFxYoVCQ4OxtHRkXbt2uHn50eJEiUsWK2kd1evXqVz587UrFmTUaNG4ebmRtGiRQkJCaFr1668ePECT09PatSoQUhICA4ODrRs2ZLdu3cDmj0plvXq1SuqVauGu7s7n3/+OZcuXeLVq1fm/SNGjODevXvs2LEDeNtJZWtrS7NmzTh48CD58+e3VOmSjhmNRvNyPYAaNWrw+eefc+DAAebPn8+zZ88YMmQIs2fPxtfXl+rVq9OwYUOePXvG7t27sbKyStEVKPJb1A8qFmH6/zNNrKysiIuLIyYmBicnJxITE8mdOzdfffUVRYsW5V//+hf16tWjSZMmli5ZBIDz58/Tvn17Zs+eTY0aNahWrRp169alZcuWLFmyxPy4oKAgwsPDyZo1qwWrFfm35M/d7NmzU7ZsWY4fP467uzuZMmXCYDBQsGBBMmTIwA8//ABA5syZLVyxCJw8eZKIiAgqVqxI7ty5SUpKMt+GfMSIEfTt25dLly5x9+5dVq9eTdGiRblw4QIFCxa0dOmSzt25c4eLFy8yfPhwDAYDM2fOpGXLlri6ujJ27FjKlClDjRo1yJIlCzt27KBJkyZYWVlhMpmwtbXVqAqxiHfnPy1btgwnJyfc3d356quvsLGxYcuWLQAMGjSIPn360LRpU27duoWNjQ1Vq1bF2tpac9Dk/0xHi1hE8jeX5cqV4969eyxYsIAJEyaYP8Csra0pUaLEz9Yki1hSaGgoe/bsYfTo0fTt2xeAdu3aERYWRkREBMePHyc6Opq9e/eydOlSjh49Ss6cOS1ctaRnyUGU0WjEZDKZv/msUaMGK1euZO3atXz22WdkyJABAGdnZ9577z2SkpKwsrJSl4lY3KNHj0hMTMRoNJInTx68vLwA8Pf358WLF0ydOpVSpUpRqlQpGjZsiI2NDREREeTIkcPClUt6dunSJVq3bs1HH33EoEGDqFmzJp6enixatIg9e/ZQs2ZN6tatS5cuXRg0aBB9+/alZ8+elCtXTp+7YjEmk8kcSPn4+LBu3Tq6d+9OeHg4uXLlYvz48RiNRrZu3QpAv379yJ8/f4qOvqSkJAVS8n+mI0ZSRfKF0eXLl7l16xb29vaUKFGC4sWLs3jxYry8vDAajXTt2hVnZ2e+++47Xr16RZ48eSxduggAd+/epU+fPvz444/06dPHvL1t27aYTCbWrFlD7dq1+eCDD8iSJQtHjhyhdOnSFqxY0rvkz909e/YQEBDAgwcPKFeuHD179mTYsGHcuXOHuXPnsn//fipVqsTVq1fZunUrJ0+eTNG2L5La3rx5g4ODA/D2zmNZsmQxLwfJmTOnOZhas2YN1tbWTJ48Gfj3EGkFUmJJV69epUaNGvTq1Yv+/fvj4uICvP3CtW/fvvTt25dNmzaxd+9eevToQY4cOYiOjmbfvn2UKVNGQ6HFYpID0VmzZrFs2TL27NlD+fLlgX93UE2cOBE7Ozs2b97My5cvmTRpUopVATp/kD/CYDKZTJYuQtKHjRs30q9fP/MyvRcvXrB27VoaNGjAypUr6dOnD++//z4ODg5ER0ezdetW8wehSFowc+ZMlixZQsaMGdm9e/fPLnyuXr1Kzpw5sbKy0tInSRO2bt2Ku7s7Hh4eODs7ExwcTN68eRk5ciSNGzdm3rx5HD16lNDQUAoVKsTkyZMVpopFPXjwgEGDBtGzZ0/q1avH+PHjCQ0NZd26dSQlJZmX/j98+BA/Pz/WrVtH3bp1mTNnjqVLFyE2NpbOnTuTM2dOvvnmG/P2hIQEwsPDiY6OplixYgDExMQQERHBjBkzCAkJwc/Pjw8//NBSpYsAEB0djaenJ25ubnh7e3Pjxg0uXrzIwoULyZMnD19++SWurq4MGjSIly9f4ufnp+4++Z8plJJUcf78eWrXro2vry/Nmzfn+fPnzJ07F39/f7Zt20adOnW4ceMGP/30E0lJSZQpU4Z8+fJZumxJx5K7TP7TwoULWbp0KaVLl2bq1KnkypUrxfp7kbTAZDIRGRlJkyZNaNmyJT4+PgA8fvyYHj168OLFC/z9/SlcuDAAUVFR2NnZmW/rLGIpt27dolOnTmTJkoWvvvqKTZs2cf/+fVauXPmLjx88eDDnzp1j48aNZM+ePZWrFUkpISGB2rVr0759e/r16wfAnj172L17N8uWLeP999+nYMGC7N+/33yOkZCQQEJCgnkZtUhq+qVz2ObNm3Pv3j2+/PJLFixYgNFo5IMPPmD79u1UqFDBPFcq+Vz5v50zi/xeCqUkVWzcuBFfX18OHDhg/qGblJTEF198wfbt2zl//jy5c+e2cJUibyX/cD169Ch79+4lMTGRYsWK0aVLFwC++eYbVq9ezYcffsjUqVPJmTOngilJc2JiYqhSpQr9+/fHy8uLhIQEbG1tefz4MeXLl8fT05NJkyZZukyRn7lx4wb9+vUjY8aM3L17F6PRSMmSJTEYDFhbWxMXF4fBYMDGxobo6Gi++eYbze+TNOHVq1dUqVKFatWqMXjwYIKDg/H396dkyZJUr16dTJkyMWXKFJo3b87MmTN17iAW9e7xt2bNGhwdHWnZsiUnT55kzJgxhISE0K9fPxo0aMDHH3/M8uXLWb9+PevXr8fJyQn471/iivxfaKaUpIrXr18TEhJCYmIi8DaQsra2plevXuzZs4e7d+8qlJI0IfmHa1BQEB4eHlSvXp03b94wY8YMdu/ezYIFC+jXrx9JSUkEBQXRt29fFixYoBkmYlFRUVG8ePGC7Nmzp5jFYzQa+emnn4C3cx4SEhLImTMn9erV49q1a5YsWeS/cnV1Ze7cuQwaNIhr165hb29PlSpVuH37NlZWVmTMmJHExEQSEhKYNm2aAilJM5ydnfn2229p0KABe/fu5fnz58yYMYM6derg6upKQkIC69at49mzZwAKpMRi3h1qPnz4cDZu3EifPn14/vw5lStX5l//+hcPHz40z0QDWL16Nfny5TMHUoACKflTKJSSP9XNmzdZtWoVr169omrVqrRt2xZ4e6enEiVKMHHiREaNGmUeiJc9e3bs7OyIi4uzZNmSjiV/S5QcRhkMBu7du8fQoUOZPn26+S57p06donHjxvTv359Vq1YxYMAAYmNjOXToEElJSRZ+F5Ke/fjjj/Tu3ZsnT55gZWXFnDlzqFevHs7OzowaNYrOnTtTvHhxPD09zSegkZGRKe6WI5LWfPjhh8ybN4+BAwcSHx9Pnz59KFWqlKXLEvlNtWvX5tatW0RERFCgQAGyZctm3mdtbU3mzJnJly8fyYtVdFEvlpB83Pn6+rJ8+XJ27NhB5cqVUzzGxcWFmJgYDh48yPz583n8+DE7d+4E1CElfy7F8/KnCQkJoVq1ahw9epTTp0/j4eHBxo0bAShYsCCNGzfmxIkTTJo0iYiICJ48ecLSpUvN65RFUltyIHXp0iX8/PyIj48H3t75yWAw4ObmBrzt7KtSpQrbtm1j3bp1rF+/HoARI0awZs0adfmJxYSEhFC1alVKly7N7NmzyZ07N97e3uaLnVatWjFq1Ch69OhBv379mDZtGv379+fAgQP07t3bwtWL/DpXV1dmzZqFlZUVw4YN4+jRoyn2awKFpFX58uWjQoUKKQKp+Ph4xo0bx/Hjx+ncubP5izARS3n9+jWHDx9m/PjxVK5cmVu3brF582aaNWuGl5cXjx494sqVK+zYsYOMGTNy/vx5bG1tSUxM1LErfyp1Ssmf4ocffqBq1aoMHDiQSZMm8eTJE7p3705YWJj5wn/8+PE4ODgQFBTEvHnzKFOmDOHh4Wzfvl0X9ZLqko/LkJAQypUrx7hx47CzswPA0dGRsLAwrl+/TtmyZc23Ii9fvjylS5fm3r175td57733LPUWJJ27dOkSn3zyCcOGDWP8+PHA2y8AevXqxdmzZ3FwcCB//vxMmjSJEiVKMGvWLM6fP4+zszPHjx/no48+suwbEPkdPvjgA+bPn8/gwYMZPnw4c+bMoUqVKoA6TOTvY9WqVZw5c4Z169axa9cuihYtaumSJB36z+6mTJkyYWVlxfr168mZMyffffcdcXFxFChQgB07dhAdHU1gYCA5cuQgX758GAwGEhMTsbFRhCB/Lg06l//ZrVu3qFChAm3btmXp0qXm7U2bNiU+Pp7Xr19TqlQp+vTpQ5kyZXj69ClHjhwhc+bMfPDBB7rLnqS65EDq4sWLfPLJJwwaNIjJkyeneEzPnj0JCQlh2rRp1KpVy7z9008/pXXr1gwePDi1yxYxe/XqFXXr1iU8PDxFSDp8+HDmz59Prly5iImJwdXVlZUrV1KkSBFiYmJwdHQkNjZWd3mSv52rV68yduxYZs6cqaWn8rdy7do1vvjiC9577z0mT55M8eLFLV2SpEPvDjV/9/e7du1i5syZnD59moEDB9KoUSOqVq3KnDlzOHjwIEFBQVhbWwNasid/HYVS8j8LDAxkyJAhfP755/Tu3RtXV1emTJnCxIkT+eKLL8iQIQPffPMNpUuXZsuWLeZ5UiKWdP36dUqUKMGkSZMYMWKE+QdtYGAg9erV486dO0yfPp1bt27h7e1NgQIF2LVrF9999x2nT5/G1dXV0m9B0rFXr14RGBjI5MmTadq0KYsWLWLmzJlMmjSJRYsW4ebmxq5du8x3eZo+fTo2NjZYW1vrpFL+tuLj480drSJ/JxEREdjb25M5c2ZLlyLp0Lsh1KJFi/j++++Jj4+nXLly+Pj4ABAWFkbevHnNz0kezr948WKL1Czpi3rv5A9LvrD5/PPPiY6OZsGCBdjY2JCYmEhAQABbtmyhfv36ANSvX59atWrx/fff07RpUwtXLuldQkIC3333HdbW1hQpUgR4uwxkypQpTJs2jQMHDlC5cmUGDx7MunXr6Nu3LwUKFMDW1pb9+/crkBKLc3Z25rPPPsPBwQEfHx9OnjzJw4cP2bJlCzVq1ADAy8uLVatWcfv2bezt7c3PVSAlf1cKpOTvSnfoFUtKDqR8fHzw9/fniy++wNHRkdGjR3Px4kXWrFlD3rx5iY6O5tSpU0ybNo0nT56wZ88eQB1S8tdTKCV/WPKH07179/Dy8sJoNDJv3jzu3LnDt99+S/369TEajQBkzpyZokWL6hsiSRNsbW3x8PAgNjaWsWPHkiFDBu7cuYOvry9r166lfPnyAHzyySd88sknjBo1CpPJhL29vWZIicWEhYVx+PBhQkND8fHxIXPmzLRr1w6DwcCkSZMoW7asOZCKi4vD3t6ePHnykD17dhITE7G2ttZJpYiISDp06tQpNm/ezKZNm3Bzc2PLli04ODhQvXp182POnTvH6tWryZAhA+fOnTM3G2iGlPzVdITJ/9nt27cZNmwYGzduZMuWLfj4+LBz506++OILbG1tmTNnDhcvXuT69evmu+pt2rQJGxsbdZhImlGqVCl69+5NUlISvXr1Ijw8nBMnTlCpUqWfrbvPmTOnhauV9O7y5ct07dqVsmXLkitXLpycnADImDEjLVq0AN7eDdLLy4slS5Zgb2/P2LFj2bdvH8eOHdMJpYiISDry7rksQGRkJA4ODri5ubF582Y8PDyYOXMmvXr1IioqiuPHj9OwYUNcXFwoXLgwVlZWCqQk1egok/+za9eucfLkSSpVqsS5c+cIDAykcOHCAHTv3p03b97g5+dHYmIio0ePxt/fH19fX06cOKG77Ema8tFHH9GvXz/g7aDHmzdvUqlSJfPd9qysrFL8QBexhCtXrlC9enW8vLzo27ev+eYQq1evpmLFinzwwQe0atUKeBtMOTo64uLigq+vL8ePH6dYsWKWLF9ERERSWfL56/z583F1dcXJyYk8efKwcOFChg8fjq+vL7169QLg4sWLrFy5kg8//NDcQGA0GhVISarRoHP5Q7788ku++uorSpUqRUhICPDv5SIACxYswN/fn6ioKG7fvs2xY8eoUKGCJUsW+a+uXLnCN998w4EDBxg9ejQeHh6A1tCL5UVGRtKiRQuKFSvGkiVLzNunTp3KqFGjyJo1K8eOHaNYsWK8fPmSLVu20KdPH2JiYjhz5ow+d0VERNKR/xxq/uWXX7J//37s7Oxo2rQpN2/eZMqUKeYB57GxsbRp04YsWbIQGBio816xCLUAyP9J8oyowoULM3ToUJKSkqhXrx4A9vb2xMbGAtCnTx+6dOmCtbU1p0+f1oWRpGnJHVO1a9dm+vTpLF26FNBAaLG8e/fu8fz5czp27GjetmnTJqZOncrKlStxc3OjRo0ahIaGkjlzZpo1a8bSpUv56aef9LkrIiKSziQHUmfOnOHhw4f4+vpSqlQpPvzwQxYvXoyNjQ2XLl1i8eLFbNq0iWbNmhEWFsbKlSsxGAyoX0UsQZ1S8oclJSWxc+dOhg0bRr58+di3b595X0hICGXKlCEqKso8+0QkrQsNDWXKlClcu3aNvXv34uzsrGBKLCIhIQFbW1vWrl2Ll5cXly9fJn/+/AAcO3aMzJkzU6pUKR4/fkyPHj3Yv38/t27dIleuXOrwExERSaeMRiM//PCD+aY93377Lb179zbv37t3r3n+b9GiRXFxcWHlypXY2tqSlJSEtbW1pUqXdEyhlPym5Aucc+fOcf78eaysrHBzc6NYsWLExsayf/9+hg0bhouLC2vWrGH+/PkEBwdz6NAhsmXLZunyJR1LPnavXLlCWFgYpUqVIlu2bNja2v7XC/dr166ROXNmcuXKZYGKReDGjRsEBAQwYcIEtm/fTvPmzTly5AiffvrpLz5+9erVzJgxg+3bt5MnT55UrlZEREQs6d0le8nnt2vXruWzzz6jffv2zJo1K8Vc3+joaGJjY7G3tzc3D2iouViSQin5VckfbEFBQfTv35/cuXOTIUMGQkNDCQ4O5tNPP+XNmzccPnyYAQMGEBUVhZWVFUFBQVSqVMnS5YsQFBREz549sbOzw8HBAW9vbzp16kT27NnVUSJp0tixY1m9ejU3b94kMjKSevXqYTQa2bx5M/nz5yc+Ph47OzvzSeigQYO4d+8e/v7+ZMqUydLli4iISCp591w2MDAQe3t7WrVqhbW1NStXrqRr166MHDmSIUOGkDVr1p8955f+LJLaNFNKfpXBYODw4cP06tWLcePGcfbsWWbOnMmzZ8+oV68eO3fuxMHBgbp16/L999+zbNkyTpw4oUBKLM5oNBIZGcn8+fOZNm0a586do3nz5gQEBDB37lyePHmitfOSpiQfi25ubtjb2/PmzRvee+89PDw8iIiIoEePHoSFhWFnZwe8HYI+cuRI/P39mThxogIpERGRdMRoNJrDpLt37zJs2DAWLFjA3r17SUpKonPnzvj5+TFlyhRmzZrF8+fPgZ/PTFUgJZamHj35mSdPnnD37l0AKlasyMGDB+nTpw9eXl48ePCAtm3b0rVrV5KSkmjdujW7d++mZs2aZM2alQYNGli4eknvkr/tiY+Px8nJiSJFitC0aVNy5crF3LlzGTt2LDt27ABgwIAB6piSNCP5GCxUqBB37tzh6NGj1KtXjwEDBvDixQv8/PwoWbIknp6eRERE8OrVK86dO8f+/fspUaKEhasXERGR1JS8ZG/YsGFERESQM2dOzp49i4+PD0ajkYYNG9KtWzcAevbsyatXr5g8ebLm/Uqao1BKUrhy5QpeXl44OTnh6OhIUFAQzZo1Iz4+ntevX9O2bVsaNmzI4sWLOX78OAEBAdSuXZu9e/dSt25dS5cvgsFgYOvWrfj6+hITE0NiYmKKoY2TJk0C3g56jI6OZvTo0Zp9JhZ1584dDh48SM2aNXF0dKRgwYIULVrUfDdTgHHjxlG5cmU2b97MkSNHcHR0pHbt2syaNQtXV1cLVi8iIiKWsmTJEvz8/Ni/fz/Zs2fHaDTStGlTJkyYgMFgoEGDBnTr1o2YmBhWr16trmpJkxRKidmPP/7Ip59+Sp8+fejVq5d5YG7ybcXPnTtHUlISAwcOBCBLliy4u7tToEABDdcVi0vudrp48SLu7u4MHDiQ69evc+rUKby9vZk9e7Z5ePmkSZOIjo7m/PnzWr4nFhUfH0///v25cOECVlZWxMbGUr9+fS5dusTy5cv56KOPsLKyonDhwjRq1IhGjRqZ78ynDj8REZH07dq1a3z88ceUK1fOPGvywIEDVK1aldGjR2M0GmnUqBF9+/ald+/e5tEVOn+QtESDzgWA58+f06JFC8qVK8e8efPM29+9m8Pu3btp3LgxP/zwAyVLlmTs2LGcP3+eDRs2kCFDBkuVLmJ24cIFTp8+zfPnzxk5ciQAc+fOZePGjRQtWpSpU6eSI0cO8+OfPHlC9uzZLVWuCABRUVE4OTlx4cIFrl69SlhYGCtWrCA0NJR8+fKRkJBAiRIlyJ07N5UrV6Zq1apUqFBBJ5UiIiLpVFJSEtbW1vTt25eLFy9y/PhxAGJjY3F0dGTLli20adOGOnXqMHr0aKpXr57iuk4kLdFRKQCEh4fz6NEj2rRpg9FoNG9/9/aiderUoWXLlpQuXZrKlSszZ84cvv76awVSkiY8evSIwYMHM2TIEGJiYszbBwwYQJs2bbh27RpjxowhPDzcvE+BlKQFya305cqVo2PHjgwbNoyuXbvSsWNHtmzZQkBAAB9//DFPnz4lMDAQZ2dnQINJRURE0ot3r88A82iKTp06cfLkSXx9fQFwdHQE3l67dezYkbCwMKZOnQqgQErSLHVKCQCrV6+mS5cuxMfHYzAYfjFJj4mJ4eDBgyQkJHD79m2aNm1K0aJFLVSxSEpGo5GVK1fy7bffEhMTw/Hjx8mSJYt5//z581m0aBG1a9dm7ty5+sEsadrGjRvp2bMnly5dIm/evObt0dHRZMyY0YKViYiISGp697ps7dq1XL9+ndjYWFq0aMHHH3/MzJkzGTVqFGPGjKFr166YTCb69OlD3bp1qVmzJuXLl+fIkSN8+umnFn4nIr9MM6UEgIIFC2JjY0NQUBBt2rT5xQv2FStWsHnzZvbu3WuBCkVS+s+lS1ZWVnTu3JlMmTIxbdo0PvvsMwICAnj//fcB6N+/P7a2tjRs2FCBlKRpJpOJkiVLkilTJt68eQP8u01fnakiIiLpy7t32duwYQMVKlQgU6ZMfPLJJ6xbt45u3brh5OTEsGHDWLx4MSaTiezZs9O7d29++uknChUqlGJ8hUhao1BKAChQoADOzs6sXLmSihUrUqBAASDlhf/NmzcpX7685piIxSUfg4cOHWLHjh1ERkZSuXJlunTpQtu2bTGZTMyePRsPDw9WrVpF1qxZAfjiiy8sXLnIbzMYDBQrVoyMGTNy6NAhXF1dzW36+uwVERFJfzZv3szq1avZvHkzlSpVYufOnQQEBJCQkEDWrFnx8vKiYcOGXL58GVtbW2rXro21tTWrVq3CyckpxeoBkbRG7QICQJ48eVi4cCF79uxh7NixXLlyBXh7ARQTE8OoUaPYtGkTnp6euigSizMYDAQFBdG4cWOuXbvG48eP6devH506deLatWu4u7vj7e1NTEwMzZo14/nz55YuWeR3S15V7+joyO3bty1cjYiIiFhK8jnBw4cPqVevHpUqVWLjxo20b9+eRYsW8dlnn/Hy5Utu375N/vz5ady4MfXq1eP69ev06NGDJUuW4O/vr04pSdPUKSVmLVu2ZO7cufTr14/Tp0/zySef4ODgwIMHDzh58iS7d+/mgw8+sHSZkg4lr6VP7pB68OABI0eOZMaMGfTt2xeAc+fO0bp1a7788kvWrl2Lu7s7sbGxrF+/nujoaHO3lEhalxz8e3l5Ua1aNQtXIyIiIqkpISGBhIQEMmTIYD4nePXqFc+fP2fDhg10796d6dOn4+XlBcC2bds4evQoM2bMwNnZmYSEBB4+fIiDgwNHjhyhZMmSlnw7Ir9Jg87lZ06fPs2MGTO4efMmGTNmxM3Nje7du2uouViEn58fdnZ2tG/fHjs7OwDu379PzZo1WbZsGTVq1CAxMREbGxvOnj1L1apVWb58OZ06dcJoNPL69Wvz3cpE/k60VFpERCR9SV6md+PGDRo0aMCoUaNwcnJiz549DB8+nOvXrzN58mQGDx4MvL0BSocOHShQoADz5883nzckJSWRmJiIvb29Jd+OyO+iTin5mcqVK7Nu3ToNgxaLM5lMrFixghcvXuDo6Ejz5s2xs7PDZDIRERHB/fv3zY9NSkqiYsWKVK1alR9//BF4OxhSgZT8XSmQEhERST+WLFmCj48PHh4evPfee/j6+hIdHc28efNo0KABO3bs4OnTp0RHRxMSEsLr16/56quvCA8PJzg4GIPBYP5Cy9ra2jyPUiStUyglv+jdiyF9Wy+WkHzcHThwgLZt2/L111+TlJRE8+bNyZ8/P15eXowcOZI8efJQq1Yt8/MMBoOCKBERERH52/juu+/w9vZmzZo1tGrVivj4eB4+fIi/vz/9+/enaNGizJs3D5PJxLZt2xg3bhyVK1cmc+bMnD59GhsbG/OdekX+brR8T0TSrPj4eOzs7Hj27BktW7bEZDLh7e1NmzZtuHPnDuPGjePAgQOMHz+eHDlycOLECZYsWcKpU6c0/0xERERE0rwrV65QqlQpunXrxnfffWfeXrVqVS5dusThw4dJTEykSpUqACQmJnLhwgVy5cpFnjx5sLKyMo+yEPk7UiglImlScqfU2rVrCQ4OJjw8nDNnzpA9e3Zmz55N69atuX37NkuWLGHp0qXkypULR0dHli5dStmyZS1dvoiIiIjIb7p79y7ffPMNy5YtY+7cuXTq1Ik2bdrw/fff4+bmhq2tLXv27KFcuXKULVuWFi1aULlyZRwcHIB/3xBI5O9KoZSIpFmnTp2iTp06fPPNN1StWpWMGTPSsWNHIiIimDJlCi1atMDa2prw8HDs7e2xsrIic+bMli5bREREROR3e/jwIfPmzWPBggXkz58fR0dH1qxZg6urKwkJCdy/f58lS5awc+dOcuTIwb59+zReRf4xFEqJSJq1YsUKpk2bxsmTJ81hk9FopFq1aoSFheHr60uTJk3IkCGDhSsVEREREfnjHj58yKJFi5g1axajR49m5MiRAMTFxaW4i546o+SfRgtPRSTNSV66Fx8fz5s3b8w/iGNiYsiQIQPLli2jfPnyjB8/Hmtra1q3bm3hikVERERE/jgXFxd69uxJYmIiU6ZMIUeOHHTv3h17e3uSkpKwsrLCYDBgZWWlYEr+UXQki0ia8G7TZnI7ctOmTYmMjMTHxwfA3BEVHR1N9erVKVKkCOXKlUv9YkVERERE/o9+a5FSvnz56NevH/369WPw4MEsW7YMAGtr6xTL9RRIyT+JOqVExOKSO6NOnTrFyZMnKVy4MB999BFFihThm2++oVevXhiNRsaPH09SUhKbN28me/bsLF68GEdHR0uXLyIiIiLyq97tboqNjcXR0dF8DvwuFxcX+vXrh8FgoEePHuTIkYOmTZtaomSRVKGZUiKSJmzevJlOnTpRqFAhnj9/TsWKFRkzZgyVKlVi9erV9O/fH0dHR+zs7Hj16hV79+6lfPnyli5bRERERORXvRtITZ8+nR9++IE5c+aQLVu2//qc+/fvs3PnTrp3746NjXpJ5J9LoZSIWNzDhw8ZN24cH3/8Md27dyc4OJjly5cTGRmJr68vVapUISIigoMHD2Jra0v58uUpWLCgpcsWEREREfndfHx8CAgIYNSoUTRs2BBXV9ff9bzExEQFU/KPpVBKRCzq/PnzTJgwgdevX7NkyRKKFCkCwL59+5g/fz6RkZFMnjyZ6tWrW7hSEREREZHf790OqQMHDtClSxcCAwN1XivyDk1IExGLunz5Mvfu3eP8+fNERUWZt9erV4/+/fuTI0cO+vbty8mTJy1YpYiIiIjI7zNixAgg5UDyO3fukC1bNqpUqWLe9p/9IUajMXUKFElDFEqJiEV17tyZ0aNHU7hwYUaOHMnly5fN++rVq4enpyelS5cmV65cFqxSREREROS3HT58mB9++IHExMQU262trYmMjOTRo0cpticlJREYGMjjx491Vz1Jl3TUi0iqSf42KDIyksjISHNnVNu2bRk4cCBxcXF8+eWXXLlyxfycJk2asHTpUs2QEhEREZE0r2rVquzYsQMbGxs2bNhg3l6gQAHi4uJYu3Ytz549A8BgMJCYmMiSJUtYsWKFhSoWsSzNlBKRVJF8y9tt27Yxd+5cfvrpJ6pVq0adOnXo1q0bACtXrmTFihVky5aNMWPGULp0aQtXLSIiIiLy+yQlJWFtbQ3A9evXKVeuHLVq1WL79u0AjBs3jtmzZ9O7d28+/fRTnJ2dmTx5Mk+fPuX06dMaZi7pkjqlRCRVGAwGtm/fTvv27albty5z5szBxsaGcePGMXfuXODtUj5PT09u3LiBr68v8fHxFq5aREREROS3PX361BxIHThwgA8++ICVK1dy/fp1mjVrBsCECRMYN24c33//Pe7u7gwaNAiTycSpU6ewsbEhKSnJkm9BxCLUKSUiqeLWrVu0a9eO7t2707t3b16+fEnx4sXJlSsXL1++xNvbmwEDBgCwdu1aqlatSoECBSxctYiIiIjIr9uxYwd+fn7MnDmTuXPnMm/ePJ4/f469vT27du1i6NChlChRgm3btgEQERHBy5cvsbW1pUCBAuZlfOqUkvRIoZSI/KnevfXtu6Kiopg4cSL9+/fH2tqaWrVqUbduXYYOHUq3bt0IDQ1l0KBBjBw50gJVi4iIiIj8MSdOnMDd3R1nZ2ceP37M4cOHKVmyJABv3rxh586dDB06lFKlSrFly5afPf+/nT+LpAc68kXkT5P8AzUiIoIzZ85w6NAh8z4nJycmTpxI/vz5mTdvHmXLlmXKlCkULlyYcuXK4eTkxI4dO3j69OnPbo8rIiIiIpLWmEwmjEYjVatWpUmTJly/fp1KlSqZl/EBODg40KRJE3x9fbly5QrVq1f/2esokJL0TEe/iPwpkgOpS5cu0aBBAzp06EDbtm1p2LCh+TGOjo4AXL58GXt7ezJnzgy8HQrZt29ftm3bRrZs2TAYDBZ5DyIiIiIiv4fRaMRgMJgDpfr16+Pv78/NmzcZP348Z8+eNT/W3t6exo0bM3HiRN5//32MRqOlyhZJc7R8T0T+Z8mBVEhICG5ubvTt2xd3d3cOHz7MsGHD8PHxYcqUKSQlJWEwGJg4cSI7duygWbNmPHv2jNWrV3PmzBkKFixo6bciIiIiIvKr3l1uN3/+fF68eMGgQYPIlCkTx48fp3PnzlSsWBEfHx/Kly8PwJYtW2jRosUvvoZIeqZ/BSLyP7OysuLGjRt8/PHHDBo0iGnTplGxYkW6dOlC1qxZefDgAQDW1tZYWVnRvHlzypUrx9q1azl58iT79u1TICUiIiIiaZ7JZDKHScOGDWPq1Klkz56diIgIANzc3FixYgXnz5/nq6++YsWKFTRr1gxPT88UHVIKpETe0nh/EfmfGY1Gli1bhpOTE++//755u5+fH8+fP+fq1auMHz8eg8FAr169KF++PEuWLCE6OpqEhASyZMliueJFRERERH7DmzdvcHBwMI+ZWL58OatWrWLr1q1UqlQJeBtYRUVFUa1aNQIDAxk6dCjffvstzs7OhIeHY2Vlhclk0qgKkXdo+Z6I/CkePnzI9OnTOXnyJF26dCEqKopp06YxdOhQypQpw549ezh16hRhYWFkzJiR4cOH0717d0uXLSIiIiLyqzp27EiHDh1o0aKFOVQaOHAgkZGR+Pv7c+XKFY4ePcqSJUt4+fIlU6dOpW3btkRERBAfH4+LiwtWVlYkJiZiY6O+EJF36V+EiPwpXFxcGDFiBJMnT2bu3LncvHmTPXv2ULt2bQAaN24MQFBQEKdOnaJKlSqWLFdERERE5HcpVKgQjRo1AiAhIQE7Ozvy5cvHmjVrGDp0KAcOHKBQoUI0a9aM8PBwunfvTq1atciRI4f5NYxGowIpkV+gfxUi8qfJlSsXY8aMwcrKikOHDnHhwgVzKBUXF4e9vT2tW7emVatWalsWERERkTQteRj5119/DcDChQsxmUx4enrSunVrXrx4wdatW/H09KR+/foUL16cw4cPExoa+rM77GmGlMgv0/I9EfnThYeHM3nyZM6cOUOrVq3w8fEBICkpCWtrawtXJyIiIiLy25KX6iX/t2nTpoSGhjJu3Dg6dOiAnZ0dr1+/JlOmTAAkJibSrFkzbGxs2Lp1q76EFfkdFNeKyJ8uV65cjB49mkqVKrFt2zbGjRsHoEBKRERERP4W3h1IHhYWBsD27dv55JNPmDx5MoGBgeZA6vXr1wQFBVG/fn0ePXpEUFAQBoPhZ91SIvJzCqVE5C+RHEwVLVqU77//nmfPnlm6JBERERGR32Q0Gs2B1OrVq+nXrx/Hjx8HICAggAoVKjBt2jQ2bNhATEwMz54949KlSxQtWpSzZ89ia2tLYmKiluyJ/A5avicif6nHjx8DkDNnTgtXIiIiIiLy65LnSAEcP36cxYsXs2PHDurWrcuQIUOoXLkyAJ999hkXL15kxIgRdOzYkfj4eDJkyIDBYNDICpH/A0W3IvKXypkzpwIpEREREflbSA6kBg8eTJcuXciePTuNGzdm165dzJo1y9wxtXr1aipWrIi3tzf79u0jY8aM5vlTCqREfj91SomIiIiIiIj8f8ePH6d169YEBwfzySefALBhwwYmTZrEhx9+yLBhw8wdUxMmTGDMmDEKokT+IBtLFyAiIiIiIiKSVtjY2GBlZYW9vb15m7u7O0lJSXz++edYW1vTv39/3NzczDf00ZI9kT9Gy/dEREREREQkXUpeOPSfC4gSExN58OABAAkJCQB06NCBYsWKcfnyZVauXGneD7rLtMgfpVBKRERERERE0p1377KXmJho3l6lShVatGhB165duXDhAra2tgA8ffqUihUr0rVrV9atW8e5c+csUrfIP4lmSomIiIiIiEi68u5d9ubNm8fhw4cxmUwULFiQWbNmER8fz2effcauXbsYOXIkzs7ObN26lYSEBA4fPkyFChWoXLkyCxcutPA7Efl7U6eUiIiIiIiIpCvJgdTIkSOZNGkSH3zwAVmzZmXjxo1UqlSJFy9esHHjRgYMGMCOHTvw8/MjQ4YM7NmzBwB7e3s+/PBDS74FkX8EdUqJiIiIiIhIunPlyhWaNm3KwoULadCgAQC3bt2iVatWZMiQgRMnTgDw4sULHBwccHBwAGDs2LEsW7aMw4cP4+rqarH6Rf4J1CklIiIiIiIi6c6LFy94+fIlxYsXB94OOy9cuDD+/v7cu3eP1atXA+Dk5ISDgwPXr1+nV69eLF26lO3btyuQEvkTKJQSERERERGRdKd48eI4OjoSFBQEYB56ni9fPhwdHXn16hXw7zvr5ciRA3d3d77//nvKlStnmaJF/mFsLF2AiIiIiIiIyF/t3eHmJpMJe3t7mjVrxrZt23BxcaFdu3YAZMiQgSxZspjvumcymTAYDGTJkoW6detarH6RfyLNlBIREREREZF/pP3793PixAnGjBkDpAymAEJDQxk1ahRhYWGULVuWChUqsH79ep4+fcqFCxfMXVIi8tdQKCUiIiIiIiL/OHFxcXh7e3PixAk8PDwYNmwY8O9gKrkD6qeffmLLli2sWrWKzJkzkzt3bgICArC1tSUpKUnBlMhfSKGUiIiIiIiI/CM9fPiQ6dOnc/LkSVq1aoWPjw/wNpgyGAzmOVKJiYnm8OndbTY2mngj8lfSoHMRERERERH5R3JxcWHEiBFUqlSJ4OBgpk2bBmDulAJ4/PgxHh4eBAYGmgMpk8mkQEokFahTSkRERERERP7RwsPDmTx5MmfOnKFly5aMGDECgEePHuHu7k5ERARXrlxRECWSyhRKiYiIiIiIyD/eu8FUmzZt8PT0xN3dncePH3Px4kXNkBKxAIVSIiIiIiIiki6Eh4fz9ddfc/r0aa5evYqLiwshISHY2tpqhpSIBSiUEhERERERkXQjPDwcHx8fnjx5wpYtWxRIiViQQikRERERERFJVyIjI8mcOTNWVlYKpEQsSKGUiIiIiIiIpEtGoxErK92UXsRSFEqJiIiIiIiIiEiqUyQsIiIiIiIiIiKpTqGUiIiIiIiIiIikOoVSIiIiIiIiIiKS6hRKiYiIiIiIiIhIqlMoJSIiIiIiIiIiqU6hlIiIiIiIiIiIpDqFUiIiIiIiIiIikuoUSomIiIiIiIiISKpTKCUiIiLyB3Xt2hWDwcAXX3zxs319+vTBYDDQtWvX1C9MRERE5G9AoZSIiIjI/yBfvnysXbuW2NhY87Y3b96wZs0a8ufPb8HKRERERNI2hVIiIiIi/4Py5cuTP39+goKCzNuCgoLIly8f5cqVM2+Li4vD29ubHDly4ODgwKeffsqZM2d+9no1a9bEYDCk+DVnzpwUj1m+fDnFixfHwcGBYsWKsWDBgv/T69y5cweDwcDFixfNjx8zZswv/l0iIiIifxWFUiIiIiL/o27durF8+XLzn5ctW4anp2eKxwwfPpxNmzbh7+/P+fPncXV1pUGDBjx//vxnr9ezZ08ePXrEo0ePyJs3b4p9S5cuZfTo0UyePJnQ0FC+/vprxo4di7+/f4rHmUymX32dd4WFhTF37lwcHR3/yNsXERER+UMUSomIiIj8jzw8PDh27Bh37tzh7t27HD9+nE6dOpn3R0dHs3DhQmbMmEGjRo346KOPWLp0KY6Ojvj5+aV4rbi4ODJnzkyuXLnIlSsX1tbWKfZPmjSJmTNn0rp1awoVKkTr1q0ZNGgQixcvTvG4hISEX32dd40ePZr27duTI0eOP+H/hoiIiMjvY2PpAkRERET+7rJly0aTJk3w9/fHZDLRpEkTsmXLZt5/8+ZNEhIScHNzM2+ztbWlcuXKhIaGpnitZ8+e4ezs/It/z5MnT7h//z7du3enZ8+e5u2JiYlkzpw5xWNfvXpFxowZf7P28+fPExwczLVr1/jXv/71u96viIiIyJ9BoZSIiIjIn8DT05N+/foB8O2336bYZzKZADAYDD/b/u62xMRE7t+/T8GCBX/x7zAajcDbJXxVqlRJse8/O6EePXqEi4vLb9Y9ZMgQhg4dSu7cuX/zsSIiIiJ/Ji3fExEREfkTNGzYkPj4eOLj42nQoEGKfa6urtjZ2XHs2DHztoSEBM6ePUvx4sXN206dOsWbN2/49NNPf/HvyJkzJ3ny5OHWrVu4urqm+FWoUCHz427evMnz589TDFr/JVu3buX69esMHTr0j7xlERERkf+JOqVERERE/gTW1tbmpXj/2bWUMWNGevfuzbBhw8iaNSv58+dn+vTpxMTE0L17dwDCw8MZO3YsH3/8MY6OjoSHhwOQlJREVFQUsbGxODo6Mn78eLy9vXF2dqZRo0bExcVx9uxZIiMjGTx4MGfPnsXb25tSpUpRsWLFX615+vTpzJ8/nwwZMvwF/0dEREREfp1CKREREZE/yX+bBQUwdepUjEYjHh4eREVFUbFiRfbs2cN7770HQIcOHTh8+DDAz5bSffnll+TLl4+uXbvSo0cPMmTIwIwZMxg+fDgZM2akVKlSDBw4EIBBgwaRN29eZs2a9bPlgv/J1dWVLl26/A/vWEREROSPM5iShxyIiIiIiMXUrFmT8ePHU7NmzZ/tGzhwIGXLlqVr166pXpeIiIjIX0UzpURERETSgKxZs2JnZ/eL+5ydnXF0dEzlikRERET+WuqUEhERERERERGRVKdOKRERERERERERSXUKpUREREREREREJNUplBIRERERERERkVSnUEpERERERERERFKdQikREREREREREUl1CqVERERERERERCTVKZQSEREREREREZFUp1BKRERERERERERS3f8DrthHdj8xTuMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def test_classification_models_with_scaling(X, y, test_size=0.2, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Тестирует классификационные модели со стандартизацией данных и возвращает результаты.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame или np.array\n",
    "        Матрица признаков\n",
    "    y : pd.Series или np.array\n",
    "        Целевая переменная\n",
    "    test_size : float, optional\n",
    "        Размер тестовой выборки (по умолчанию 0.2)\n",
    "    random_state : int, optional\n",
    "        Seed для воспроизводимости (по умолчанию 42)\n",
    "    verbose : bool, optional\n",
    "        Выводить ли прогресс (по умолчанию True)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Таблица с результатами метрик и временем обучения для каждой модели\n",
    "    \"\"\"\n",
    "\n",
    "    # Разделение данных\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Инициализация моделей с пайплайнами (StandardScaler + модель)\n",
    "    models = {\n",
    "        'Logistic Regression': make_pipeline(StandardScaler(),\n",
    "                                           LogisticRegression(random_state=random_state, max_iter=1000)),\n",
    "        'SVM': make_pipeline(StandardScaler(),\n",
    "                            SVC(random_state=random_state)),\n",
    "        'KNN': make_pipeline(StandardScaler(),\n",
    "                            KNeighborsClassifier()),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(random_state=random_state, n_jobs=-1, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(random_state=random_state, verbose=False)\n",
    "    }\n",
    "\n",
    "    # Словарь для хранения результатов\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Train Time (s)': [],\n",
    "        'Scaler Used': []\n",
    "    }\n",
    "\n",
    "    # Обучение и оценка моделей\n",
    "    for name, model in models.items():\n",
    "        if verbose:\n",
    "            print(f\"🔍 Обучение {name}...\")\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Обучение модели\n",
    "            model.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            # Предсказание\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Расчет метрик\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            # Определение использования StandardScaler\n",
    "            scaler_used = 'StandardScaler' if 'standardscaler' in str(model).lower() else 'No'\n",
    "\n",
    "            # Сохранение результатов\n",
    "            results['Model'].append(name)\n",
    "            results['Accuracy'].append(accuracy)\n",
    "            results['Precision'].append(precision)\n",
    "            results['Recall'].append(recall)\n",
    "            results['F1 Score'].append(f1)\n",
    "            results['Train Time (s)'].append(train_time)\n",
    "            results['Scaler Used'].append(scaler_used)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\" {name}\\nAccuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                print(\"─\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Ошибка в {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создание DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results).sort_values('F1 Score', ascending=False)\n",
    "\n",
    "    # Визуализация результатов\n",
    "    if verbose:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "        x = np.arange(len(results_df['Model']))\n",
    "        width = 0.2\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "        plt.title('Сравнение моделей классификации', pad=20)\n",
    "        plt.xlabel('Модели')\n",
    "        plt.ylabel('Оценка')\n",
    "        plt.xticks(x + width*1.5, results_df['Model'], rotation=45, ha='right')\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Запуск расчета\n",
    "results = test_classification_models_with_scaling(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294a5ea4c764740",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-28T17:09:08.860255Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Initial F1: 0.6957, Accuracy: 0.6959 with 210 features\n",
      "Best F1: 0.7108, Accuracy: 0.7113 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== SVM ===\n",
      "Initial F1: 0.7163, Accuracy: 0.7165 with 210 features\n",
      "Best F1: 0.7422, Accuracy: 0.7423 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== KNN ===\n",
      "Initial F1: 0.7266, Accuracy: 0.7268 with 210 features\n",
      "Best F1: 0.7679, Accuracy: 0.7680 with 203 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== RandomForest ===\n",
      "Initial F1: 0.7523, Accuracy: 0.7526 with 210 features\n",
      "Best F1: 0.7781, Accuracy: 0.7784 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== XGBoost ===\n",
      "Initial F1: 0.7418, Accuracy: 0.7423 with 210 features\n",
      "Best F1: 0.7833, Accuracy: 0.7835 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== CatBoost ===\n",
      "Initial F1: 0.7577, Accuracy: 0.7577 with 210 features\n",
      "Best F1: 0.7629, Accuracy: 0.7629 with 209 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации\n",
    "\n",
    "def evaluate_model(X, y, model, test_size=0.2, random_state=42):\n",
    "    \"\"\"Функция оценки модели классификации с добавлением StandardScaler\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Для моделей, чувствительных к масштабу, используем Pipeline со StandardScaler\n",
    "    if isinstance(model, (LogisticRegression, SVC, KNeighborsClassifier)):\n",
    "        model = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "def find_best_feature_subset(X, y, model, model_name):\n",
    "    \"\"\"Модифицированная версия для классификации с указанием имени модели\"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    current_features = X.columns.tolist()\n",
    "    initial_scores = evaluate_model(X[current_features], y, model)\n",
    "    best_f1 = initial_scores['f1']\n",
    "    best_accuracy = initial_scores['accuracy']\n",
    "    best_features = current_features.copy()\n",
    "    history = []\n",
    "\n",
    "    history.append({\n",
    "        'features': current_features.copy(),\n",
    "        'f1': best_f1,\n",
    "        'accuracy': best_accuracy,\n",
    "        'action': 'initial'\n",
    "    })\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Initial F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(current_features)} features\")\n",
    "\n",
    "    improved = True\n",
    "    while improved and len(current_features) > 1:\n",
    "        improved = False\n",
    "        worst_feature = None\n",
    "\n",
    "        for feature in current_features:\n",
    "            trial_features = [f for f in current_features if f != feature]\n",
    "            current_scores = evaluate_model(X[trial_features], y, model)\n",
    "\n",
    "            history.append({\n",
    "                'features': trial_features.copy(),\n",
    "                'f1': current_scores['f1'],\n",
    "                'accuracy': current_scores['accuracy'],\n",
    "                'action': f'removed {feature}'\n",
    "            })\n",
    "\n",
    "            if current_scores['f1'] > best_f1:\n",
    "                best_f1 = current_scores['f1']\n",
    "                best_accuracy = current_scores['accuracy']\n",
    "                best_features = trial_features.copy()\n",
    "                worst_feature = feature\n",
    "                improved = True\n",
    "\n",
    "        if improved:\n",
    "            current_features.remove(worst_feature)\n",
    "\n",
    "    print(f\"Best F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(best_features)} features\")\n",
    "    print(\"Optimal features:\", best_features)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_f1,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей по очереди\"\"\"\n",
    "    # Создаем модели с дефолтными параметрами\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = find_best_feature_subset(X, y, model, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создаем DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53e9f1a111a073db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: {'accuracy': 0.7474226804123711, 'f1': 0.7472547924809232, 'roc_auc': 0.7905728557763844}\n",
      "Selected features: 149 (Removed 61 outliers)\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: {'accuracy': 0.7422680412371134, 'f1': 0.7422406462585034, 'roc_auc': 0.791316824317143}\n",
      "Selected features: 182 (Removed 28 outliers)\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: {'accuracy': 0.7268041237113402, 'f1': 0.726738778005156, 'roc_auc': 0.7914231055372516}\n",
      "Selected features: 71 (Removed 139 outliers)\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: {'accuracy': 0.7783505154639175, 'f1': 0.7780615638385612, 'roc_auc': 0.8126793495589328}\n",
      "Selected features: 97 (Removed 113 outliers)\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: {'accuracy': 0.7731958762886598, 'f1': 0.7729787234042553, 'roc_auc': 0.8303220320969285}\n",
      "Selected features: 93 (Removed 117 outliers)\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: {'accuracy': 0.7577319587628866, 'f1': 0.7577255214560913, 'roc_auc': 0.8253268147518333}\n",
      "Selected features: 108 (Removed 102 outliers)\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации, через последовательное удаление признаков, упорядоченных по количеству возможных выбросов\n",
    "\n",
    "outliers_count = pd.DataFrame({\n",
    "        'feature': df.columns,\n",
    "        'outliers': outliers.sum(axis=0)\n",
    "    }).sort_values('outliers', ascending=False)\n",
    "all_features_outliers = outliers_count['feature'][outliers_count['outliers']>0].tolist()\n",
    "\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = get_best_features(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_best_features(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с удалением выбросов для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "\n",
    "    # Перебираем количество удаляемых признаков с выбросами\n",
    "    for n in range(1, len(all_features_outliers)+1):\n",
    "        \n",
    "        current_features = X.drop(columns = all_features_outliers[:n]).columns.tolist()\n",
    "\n",
    "        metrics = evaluate_metrics(X[current_features], y, model_name)\n",
    "\n",
    "        if metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = metrics\n",
    "            best_features = current_features.copy()\n",
    "            removed = all_features_outliers[:n]\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: {best_metrics}\")\n",
    "    print(f\"Selected features: {len(best_features)} (Removed {len(removed)} outliers)\")\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'outliers'\n",
    "    }\n",
    "   \n",
    "\n",
    "def evaluate_metrics(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        metrics['roc_auc'] = None\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_2 = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8277aa7f99158c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7526, F1=0.7525, ROC-AUC=0.8103943033266021\n",
      "Optimal features (8): ['NHOHCount', 'BCUT2D_MRLOW', 'MolLogP', 'BCUT2D_MWLOW', 'NumSaturatedHeterocycles']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7629, F1=0.7628, ROC-AUC=0.811403974917632\n",
      "Optimal features (17): ['NHOHCount', 'MolLogP', 'BCUT2D_MWLOW', 'BCUT2D_MRLOW', 'NumSaturatedHeterocycles']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7680, F1=0.7680, ROC-AUC=0.8360612179827824\n",
      "Optimal features (6): ['NHOHCount', 'BCUT2D_MWLOW', 'MolLogP', 'EState_VSA3', 'AvgIpc']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью SHAP\n",
    "\n",
    "def test_tree_models_sh(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с SHAP-анализом\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_shap_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_shap_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с помощью SHAP для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostRegressor(\n",
    "        iterations=100,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "   \n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': importance['feature'],\n",
    "        'importance': importance['mean_abs_shap']\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'][feat_importance['importance']>0].tolist()\n",
    "    \n",
    "    \n",
    "    # Находим оптимальное количество признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "\n",
    "    for n in range(1, min(20, len(all_features)+1)):  # Ограничиваем до 20 признаков для скорости\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'shap'\n",
    "    }\n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_significant_shap_features(shap_values, feature_names, threshold=0):\n",
    "    \"\"\"\n",
    "    Возвращает отсортированный по убыванию список признаков с SHAP-значимостью\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    shap_values : shap.Explanation или np.ndarray\n",
    "        SHAP значения для всех наблюдений\n",
    "    feature_names : list или pd.Index\n",
    "        Список названий признаков\n",
    "    threshold : float, optional\n",
    "        Порог значимости (по умолчанию 0)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame: DataFrame с колонками 'feature' и 'mean_abs_shap',\n",
    "                  отсортированный по убыванию важности\n",
    "    \"\"\"\n",
    "    if isinstance(shap_values, shap.Explanation):\n",
    "        shap_array = shap_values.values\n",
    "    else:\n",
    "        shap_array = shap_values\n",
    "\n",
    "    # Для многоклассовой классификации берем среднее по всем классам\n",
    "    if len(shap_array.shape) == 3:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=(0, 1))\n",
    "    else:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=0)\n",
    "\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': mean_abs_shap\n",
    "    })\n",
    "\n",
    "    significant_features = shap_importance[shap_importance['mean_abs_shap'] > threshold] \\\n",
    "        .sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    return significant_features.reset_index(drop=True)\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_3 = test_tree_models_sh(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13843390031221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7680, F1=0.7679, ROC-AUC=0.8037517270698267\n",
      "Optimal features (14): ['NHOHCount', 'PEOE_VSA7', 'BCUT2D_MWLOW', 'BCUT2D_MRLOW', 'NumSaturatedHeterocycles']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7526, F1=0.7519, ROC-AUC=0.8168243171431608\n",
      "Optimal features (20): ['NHOHCount', 'fr_hdrzone', 'NumSaturatedHeterocycles', 'fr_C_O', 'fr_amide']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7629, F1=0.7628, ROC-AUC=0.8201721755765757\n",
      "Optimal features (6): ['NHOHCount', 'NumSaturatedHeterocycles', 'BCUT2D_MWLOW', 'BCUT2D_MRLOW', 'PEOE_VSA6']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью features importance\n",
    "\n",
    "def test_tree_models_fs(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с отбором признаков\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_feature_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_feature_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков для классификации на основе важности признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Инициализация и обучение модели\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostClassifier(iterations=100, random_seed=42, verbose=False)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Получение важности признаков\n",
    "    if name == 'XGBoost':\n",
    "        importance = model.feature_importances_\n",
    "    else:\n",
    "        importance = model.feature_importances_\n",
    "    \n",
    "    # Сортировка признаков по важности\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'].tolist()\n",
    "    \n",
    "    # Поиск оптимального набора признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Ограничиваем количество проверяемых комбинаций для скорости\n",
    "    max_features_to_test = min(20, len(all_features))\n",
    "    \n",
    "    for n in range(1, max_features_to_test + 1):\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'feature_importance'\n",
    "    }\n",
    "    \n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_4  = test_tree_models_fs(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5860f10-8bb9-4002-8bf4-4c0889b7f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем все лучшие результаты\n",
    "results_df = pd.concat([results_col_combination,results_col_combination_2, results_col_combination_3, results_col_combination_4], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943b2ea9-ff60-444e-be72-a4bd2ff042ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.710849</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.742158</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.767887</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>203</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.778062</td>\n",
       "      <td>0.778351</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.783298</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.762861</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>209</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.740919</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>196</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.726797</td>\n",
       "      <td>0.726804</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>191</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>0.726804</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>210</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.757416</td>\n",
       "      <td>0.757732</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>206</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.772810</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>191</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.757674</td>\n",
       "      <td>0.757732</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>210</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.752472</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>[NHOHCount, BCUT2D_MRLOW, MolLogP, BCUT2D_MWLO...</td>\n",
       "      <td>8</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.762786</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>[NHOHCount, MolLogP, BCUT2D_MWLOW, BCUT2D_MRLO...</td>\n",
       "      <td>17</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.767986</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[NHOHCount, BCUT2D_MWLOW, MolLogP, EState_VSA3...</td>\n",
       "      <td>6</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.767887</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[NHOHCount, PEOE_VSA7, BCUT2D_MWLOW, BCUT2D_MR...</td>\n",
       "      <td>14</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.751918</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>[NHOHCount, fr_hdrzone, NumSaturatedHeterocycl...</td>\n",
       "      <td>20</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.762786</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>[NHOHCount, NumSaturatedHeterocycles, BCUT2D_M...</td>\n",
       "      <td>6</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model   best_f1  best_accuracy  \\\n",
       "0   LogisticRegression  0.710849       0.711340   \n",
       "1                  SVM  0.742158       0.742268   \n",
       "2                  KNN  0.767887       0.768041   \n",
       "3         RandomForest  0.778062       0.778351   \n",
       "4              XGBoost  0.783298       0.783505   \n",
       "5             CatBoost  0.762861       0.762887   \n",
       "6   LogisticRegression  0.740919       0.742268   \n",
       "7                  SVM  0.726797       0.726804   \n",
       "8                  KNN  0.726623       0.726804   \n",
       "9         RandomForest  0.757416       0.757732   \n",
       "10             XGBoost  0.772810       0.773196   \n",
       "11            CatBoost  0.757674       0.757732   \n",
       "12        RandomForest  0.752472       0.752577   \n",
       "13             XGBoost  0.762786       0.762887   \n",
       "14            CatBoost  0.767986       0.768041   \n",
       "15        RandomForest  0.767887       0.768041   \n",
       "16             XGBoost  0.751918       0.752577   \n",
       "17            CatBoost  0.762786       0.762887   \n",
       "\n",
       "                                        best_features  num_features  \\\n",
       "0   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "1   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "2   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           203   \n",
       "3   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "4   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "5   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           209   \n",
       "6   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           196   \n",
       "7   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           191   \n",
       "8   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           210   \n",
       "9   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           206   \n",
       "10  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           191   \n",
       "11  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           210   \n",
       "12  [NHOHCount, BCUT2D_MRLOW, MolLogP, BCUT2D_MWLO...             8   \n",
       "13  [NHOHCount, MolLogP, BCUT2D_MWLOW, BCUT2D_MRLO...            17   \n",
       "14  [NHOHCount, BCUT2D_MWLOW, MolLogP, EState_VSA3...             6   \n",
       "15  [NHOHCount, PEOE_VSA7, BCUT2D_MWLOW, BCUT2D_MR...            14   \n",
       "16  [NHOHCount, fr_hdrzone, NumSaturatedHeterocycl...            20   \n",
       "17  [NHOHCount, NumSaturatedHeterocycles, BCUT2D_M...             6   \n",
       "\n",
       "              selector  \n",
       "0    without selection  \n",
       "1    without selection  \n",
       "2    without selection  \n",
       "3    without selection  \n",
       "4    without selection  \n",
       "5    without selection  \n",
       "6             outliers  \n",
       "7             outliers  \n",
       "8             outliers  \n",
       "9             outliers  \n",
       "10            outliers  \n",
       "11            outliers  \n",
       "12                shap  \n",
       "13                shap  \n",
       "14                shap  \n",
       "15  feature_importance  \n",
       "16  feature_importance  \n",
       "17  feature_importance  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e05de657-40c5-4224-8a4d-a4d6ee638132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.7371, F1=0.7365, ROC-AUC=0.7908916994367096\n",
      "Optimal features (191): ['fr_priamide', 'PEOE_VSA8', 'SlogP_VSA10', 'AvgIpc', 'fr_phenol_noOrthoHbond']...\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7526, F1=0.7525, ROC-AUC=0.7900414496758423\n",
      "Optimal features (20): ['PEOE_VSA13', 'NHOHCount', 'Chi3n', 'fr_C_O', 'Chi2n']...\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.7680, F1=0.7679, ROC-AUC=0.8208630035072804\n",
      "Optimal features (203): ['fr_priamide', 'PEOE_VSA13', 'PEOE_VSA8', 'SlogP_VSA10', 'AvgIpc']...\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7629, F1=0.7628, ROC-AUC=0.8007227122967372\n",
      "Optimal features (6): ['EState_VSA3', 'PEOE_VSA7', 'BCUT2D_MWLOW', 'MolLogP', 'AvgIpc']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7680, F1=0.7677, ROC-AUC=0.8241577213306409\n",
      "Optimal features (196): ['fr_priamide', 'PEOE_VSA8', 'SlogP_VSA10', 'AvgIpc', 'fr_phenol_noOrthoHbond']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7680, F1=0.7680, ROC-AUC=0.8355298118822404\n",
      "Optimal features (6): ['EState_VSA3', 'PEOE_VSA7', 'BCUT2D_MWLOW', 'MolLogP', 'AvgIpc']...\n"
     ]
    }
   ],
   "source": [
    "# Определяем налучший результат работы моделей\n",
    "def test_all_classifiers(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = evaluate_classifier(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_classifier(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка классификатора с оптимальным набором признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    # Находим оптимальные метрики\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Получаем все наборы признаков из предыдущих результатов\n",
    "    feature_sets = [set(features) for features in results_df['best_features']]\n",
    "    \n",
    "    # Добавляем все признаки для сравнения\n",
    "    feature_sets.append(set(X.columns))\n",
    "    \n",
    "    # Удаляем дубликаты\n",
    "    unique_feature_sets = []\n",
    "    seen = set()\n",
    "    for fs in feature_sets:\n",
    "        frozen = frozenset(fs)\n",
    "        if frozen not in seen:\n",
    "            seen.add(frozen)\n",
    "            unique_feature_sets.append(list(fs))\n",
    "    \n",
    "    for current_features in unique_feature_sets:\n",
    "        current_metrics = get_classification_metrics(X[current_features], y, model_name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features)\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "def get_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_features_selection  = test_all_classifiers(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e76424a1-6c08-4999-b289-7a6dd18ad9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.736546</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[fr_priamide, PEOE_VSA8, SlogP_VSA10, AvgIpc, ...</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.752472</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>[PEOE_VSA13, NHOHCount, Chi3n, fr_C_O, Chi2n, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.767887</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[fr_priamide, PEOE_VSA13, PEOE_VSA8, SlogP_VSA...</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.762786</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>[EState_VSA3, PEOE_VSA7, BCUT2D_MWLOW, MolLogP...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.767739</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[fr_priamide, PEOE_VSA8, SlogP_VSA10, AvgIpc, ...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.767986</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[EState_VSA3, PEOE_VSA7, BCUT2D_MWLOW, MolLogP...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.736546       0.737113   \n",
       "1                 SVM  0.752472       0.752577   \n",
       "2                 KNN  0.767887       0.768041   \n",
       "3        RandomForest  0.762786       0.762887   \n",
       "4             XGBoost  0.767739       0.768041   \n",
       "5            CatBoost  0.767986       0.768041   \n",
       "\n",
       "                                       best_features  num_features  \n",
       "0  [fr_priamide, PEOE_VSA8, SlogP_VSA10, AvgIpc, ...           191  \n",
       "1  [PEOE_VSA13, NHOHCount, Chi3n, fr_C_O, Chi2n, ...            20  \n",
       "2  [fr_priamide, PEOE_VSA13, PEOE_VSA8, SlogP_VSA...           203  \n",
       "3  [EState_VSA3, PEOE_VSA7, BCUT2D_MWLOW, MolLogP...             6  \n",
       "4  [fr_priamide, PEOE_VSA8, SlogP_VSA10, AvgIpc, ...           196  \n",
       "5  [EState_VSA3, PEOE_VSA7, BCUT2D_MWLOW, MolLogP...             6  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_features_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58a92eb3-7eb3-464f-b484-e89cf9b9702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing LogisticRegression: 100%|██████████| 772/772 [00:23<00:00, 33.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.7216, F1=0.7214, ROC-AUC=0.781113827186736\n",
      "Удалено строк: 7 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing SVM: 100%|██████████| 772/772 [01:26<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7680, F1=0.7680, ROC-AUC=0.7861090445318313\n",
      "Удалено строк: 5 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing KNN: 100%|██████████| 772/772 [00:21<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.7784, F1=0.7781, ROC-AUC=0.8292060792857902\n",
      "Удалено строк: 6 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest: 100%|██████████| 772/772 [02:07<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.7835, F1=0.7831, ROC-AUC=0.8110319906472526\n",
      "Удалено строк: 4 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost: 100%|██████████| 772/772 [02:40<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7835, F1=0.7833, ROC-AUC=0.8208630035072803\n",
      "Удалено строк: 5 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost: 100%|██████████| 772/772 [14:56<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7990, F1=0.7988, ROC-AUC=0.8347858433414816\n",
      "Удалено строк: 8 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Определяем лучшую комбинацию строк для повышения качества работы моделей на всех данных\n",
    "def test_all_classifiers_optimized(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов с оптимизацией набора данных\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = optimize_classifier_data(X, y, name, model)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def optimize_classifier_data(X, y, name, model, verbose=True):\n",
    "    \"\"\"\n",
    "    Оптимизирует набор данных для классификатора путем последовательного удаления строк\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Признаки\n",
    "    y : pd.Series\n",
    "        Целевая переменная\n",
    "    name : str\n",
    "        Имя модели\n",
    "    model : sklearn classifier\n",
    "        Модель классификатора\n",
    "    verbose : bool\n",
    "        Выводить ли прогресс\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации\n",
    "    \"\"\"\n",
    "    # Используем оптимальные признаки для модели\n",
    "    models = {\n",
    "        'LogisticRegression': make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    if name == 'LogisticRegression':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][0]]\n",
    "        \n",
    "    elif name == 'SVM':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][1]]\n",
    "        \n",
    "    elif name == 'KNN':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][2]]\n",
    "    elif name == 'RandomForest':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][3]]\n",
    "    elif name == 'XGBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][4]]\n",
    "    elif name == 'CatBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][5]]\n",
    "    \n",
    "    \n",
    "    # Разделение данных со стратификацией\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Инициализация\n",
    "    X_opt = X_train.copy()\n",
    "    y_opt = y_train.copy()\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    removed_indices = []\n",
    "    \n",
    "    # Прогресс-бар для наглядности\n",
    "    iterator = tqdm(X_train.index, desc=f\"Optimizing {name}\") if verbose else X_train.index\n",
    "    \n",
    "    for idx in iterator:\n",
    "        # Временно удаляем строку\n",
    "        X_temp = X_opt.drop(index=idx)\n",
    "        y_temp = y_opt.drop(index=idx)\n",
    "        \n",
    "        # Обучаем и оцениваем\n",
    "        model = models[name]\n",
    "        model.fit(X_temp, y_temp)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Рассчитываем метрики\n",
    "        current_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y)) == 2:\n",
    "            current_metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Решение о сохранении/удалении строки\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            X_opt = X_temp\n",
    "            y_opt = y_temp\n",
    "            removed_indices.append(idx)\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Удалено строк: {len(removed_indices)} из {len(X_train)}\")\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'removed_indices': removed_indices,\n",
    "        'num_removed': len(removed_indices),\n",
    "        'indices': X_opt.index.tolist(),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "    \n",
    "# Запуск расчета\n",
    "results_indices_selection = test_all_classifiers_optimized(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d9c0bc-20b0-429e-93ef-d9e9d1fe0044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>removed_indices</th>\n",
       "      <th>num_removed</th>\n",
       "      <th>indices</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.721383</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>[344, 617, 642, 943, 152, 147, 430]</td>\n",
       "      <td>7</td>\n",
       "      <td>[26, 721, 302, 132, 440, 514, 777, 46, 331, 73...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.767986</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[344, 302, 605, 692, 540]</td>\n",
       "      <td>5</td>\n",
       "      <td>[26, 721, 617, 132, 440, 514, 777, 46, 331, 73...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.778062</td>\n",
       "      <td>0.778351</td>\n",
       "      <td>[344, 777, 922, 285, 373, 76]</td>\n",
       "      <td>6</td>\n",
       "      <td>[26, 721, 617, 302, 132, 440, 514, 46, 331, 73...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.783136</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>[344, 26, 617, 500]</td>\n",
       "      <td>4</td>\n",
       "      <td>[721, 302, 132, 440, 514, 777, 46, 331, 731, 9...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.783298</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>[344, 721, 181, 436, 656]</td>\n",
       "      <td>5</td>\n",
       "      <td>[26, 617, 302, 132, 440, 514, 777, 46, 331, 73...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>0.798969</td>\n",
       "      <td>[344, 26, 617, 302, 47, 604, 422, 830]</td>\n",
       "      <td>8</td>\n",
       "      <td>[721, 132, 440, 514, 777, 46, 331, 731, 947, 9...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.721383       0.721649   \n",
       "1                 SVM  0.767986       0.768041   \n",
       "2                 KNN  0.778062       0.778351   \n",
       "3        RandomForest  0.783136       0.783505   \n",
       "4             XGBoost  0.783298       0.783505   \n",
       "5            CatBoost  0.798835       0.798969   \n",
       "\n",
       "                          removed_indices  num_removed  \\\n",
       "0     [344, 617, 642, 943, 152, 147, 430]            7   \n",
       "1               [344, 302, 605, 692, 540]            5   \n",
       "2           [344, 777, 922, 285, 373, 76]            6   \n",
       "3                     [344, 26, 617, 500]            4   \n",
       "4               [344, 721, 181, 436, 656]            5   \n",
       "5  [344, 26, 617, 302, 47, 604, 422, 830]            8   \n",
       "\n",
       "                                             indices           selector  \n",
       "0  [26, 721, 302, 132, 440, 514, 777, 46, 331, 73...  without selection  \n",
       "1  [26, 721, 617, 132, 440, 514, 777, 46, 331, 73...  without selection  \n",
       "2  [26, 721, 617, 302, 132, 440, 514, 46, 331, 73...  without selection  \n",
       "3  [721, 302, 132, 440, 514, 777, 46, 331, 731, 9...  without selection  \n",
       "4  [26, 617, 302, 132, 440, 514, 777, 46, 331, 73...  without selection  \n",
       "5  [721, 132, 440, 514, 777, 46, 331, 731, 947, 9...  without selection  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_indices_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10259f61-a633-4394-88f7-24551659a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем налучшую комбинацию строк для повышения качества работы моделей\n",
    "def optimize_classifier(X_train, X_test, y_train, y_test, model_type='logistic', n_trials=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Оптимизирует гиперпараметры для классификаторов с улучшенной обработкой ошибок SVM\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Тип модели: 'logistic', 'svm' или 'knn'\n",
    "    n_trials : int\n",
    "        Количество испытаний для Optuna\n",
    "    random_state : int\n",
    "        Seed для воспроизводимости\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации (лучшие параметры, метрики, study)\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "    # Проверяем, является ли это первым trial (нулевым по индексу)\n",
    "        \n",
    "            # Обычная логика подбора параметров\n",
    "        try:\n",
    "            if model_type == 'logistic':\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['l2', 'l1']),\n",
    "                    'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),LogisticRegression(**params, random_state=random_state))\n",
    "                \n",
    "            elif model_type == 'svm':\n",
    "                kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'kernel': kernel,\n",
    "                    'gamma': trial.suggest_float('gamma', 1e-5, 10, log=True),\n",
    "                    'tol': trial.suggest_float('tol', 1e-5, 1e-1, log=True),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                if kernel == 'poly':\n",
    "                    params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "                \n",
    "                model = make_pipeline(StandardScaler(),SVC(**params, random_state=random_state, probability=True))\n",
    "                \n",
    "            elif model_type == 'knn':\n",
    "                params = {\n",
    "                    'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
    "                    'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "                    'p': trial.suggest_int('p', 1, 3)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),KNeighborsClassifier(**params))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if model_type == 'svm':\n",
    "                error_info = f\"SVM failed with params: {params}. Error: {str(e)}\"\n",
    "                trial.set_user_attr(\"svm_error\", error_info)\n",
    "            return float('-inf')\n",
    "        \n",
    "        # Общая часть для всех моделей (дефолтных и оптимизированных)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y_train)) == 2:\n",
    "            trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return f1\n",
    "        \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    # Анализ результатов\n",
    "    best_params = study.best_params\n",
    "    best_metrics = {\n",
    "        'f1': study.best_value,\n",
    "        'accuracy': study.best_trial.user_attrs['accuracy'],\n",
    "    }\n",
    "    \n",
    "    if 'roc_auc' in study.best_trial.user_attrs:\n",
    "        best_metrics['roc_auc'] = study.best_trial.user_attrs['roc_auc']\n",
    "    \n",
    "    # Добавляем информацию о неудачных trials для SVM\n",
    "    if model_type == 'svm':\n",
    "        failed_trials = [t for t in study.trials if 'svm_error' in t.user_attrs]\n",
    "        if failed_trials:\n",
    "            best_metrics['failed_trials_count'] = len(failed_trials)\n",
    "            best_metrics['last_error'] = failed_trials[-1].user_attrs['svm_error']\n",
    "    \n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'best_params': best_params,\n",
    "        'best_metrics': best_metrics,\n",
    "        'study': study\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44b5a03c-04b9-48de-81be-5b1fb9e7eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск подбора гиперпараметров\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train_logistic = X_train[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]].drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "y_train_logistic = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "X_test_logistic = X_test[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]]\n",
    "logistic_results = optimize_classifier(X_train_logistic, X_test_logistic, y_train_logistic, y_test, 'logistic', n_trials=200)\n",
    "\n",
    "X_train_svm = X_train[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]].drop(index=results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "y_train_svm = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "X_test_svm = X_test[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]]\n",
    "svm_results = optimize_classifier(X_train_svm, X_test_svm, y_train_svm, y_test, 'svm', n_trials=200)\n",
    "X_train_knn = X_train[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]].drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "y_train_knn = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "X_test_knn = X_test[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]]\n",
    "knn_results = optimize_classifier(X_train_knn, X_test_knn, y_train_knn, y_test, 'knn', n_trials=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e4f328-a7fb-40ed-b301-9daaee300865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best params: {'C': 0.08533309898270353, 'penalty': 'l1', 'solver': 'saga', 'max_iter': 887}\n",
      "LogisticRegression best f1: 0.7312\n",
      "LogisticRegression best accuracy: 0.7320\n",
      "SVM best params: {'kernel': 'rbf', 'C': 0.98974591386703, 'gamma': 0.04890538925194014, 'tol': 0.030504597091952933, 'max_iter': 642}\n",
      "SVM best f1: 0.7680\n",
      "SVM best accuracy: 0.7680\n",
      "KNN best params: {'n_neighbors': 5, 'weights': 'distance', 'p': 2}\n",
      "KNN best f1: 0.7783\n",
      "KNN best accuracy: 0.7784\n"
     ]
    }
   ],
   "source": [
    "# Вывод результатов подбора гиперпараметров для линейных моделей\n",
    "print(f\"LogisticRegression best params: {logistic_results['best_params']}\")\n",
    "print(f\"LogisticRegression best f1: {logistic_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"LogisticRegression best accuracy: {logistic_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"SVM best params: {svm_results['best_params']}\")\n",
    "print(f\"SVM best f1: {svm_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"SVM best accuracy: {svm_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"KNN best params: {knn_results['best_params']}\")\n",
    "print(f\"KNN best f1: {knn_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"KNN best accuracy: {knn_results['best_metrics']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f55b546d-50e7-4fd4-9f1a-daf3824d6ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost Classifier...\n",
      "\n",
      "Best trial:\n",
      "  F1-score: 0.809233\n",
      "  Accuracy: 0.809278\n",
      "  ROC-AUC: 0.836593\n",
      "\n",
      "Best params:\n",
      "  iterations: 275\n",
      "  depth: 10\n",
      "  learning_rate: 0.07980262131203382\n",
      "  l2_leaf_reg: 6.054202591280198\n",
      "  random_strength: 1.0794120527822506\n",
      "  bagging_temperature: 0.5338009842409196\n",
      "  border_count: 169\n",
      "  min_data_in_leaf: 22\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для CatBoost\n",
    "# Подготовка данных для CatBoost\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train_cb = X_train[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]].drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "y_train_cb = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "X_test_cb = X_test[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]]\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров CatBoostClassifier\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42,\n",
    "        'thread_count': 4\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(\n",
    "        X_train_cb, \n",
    "        y_train_cb,\n",
    "        eval_set=(X_test_cb, y_test),\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Предсказание и метрики\n",
    "    y_pred = model.predict(X_test_cb)\n",
    "    y_proba = model.predict_proba(X_test_cb)[:, 1] if len(y_train_cb.unique()) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing CatBoost Classifier...\")\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study.trials) > 0:\n",
    "    best_trial_cb = study.best_trial\n",
    "    \n",
    "    print(\"\\nBest trial:\")\n",
    "    print(f\"  F1-score: {best_trial_cb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_cb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_cb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_cb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_cb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_cb.params\n",
    "    best_params.update({\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42\n",
    "    })\n",
    "    \n",
    "    best_model = CatBoostClassifier(**best_params)\n",
    "    best_model.fit(\n",
    "        pd.concat([X_train_cb, X_test_cb]),\n",
    "        pd.concat([y_train_cb, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc0194b-d65f-4c27-98bf-4258b6634371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost Classifier...\n",
      "\n",
      "Best XGBoost Classifier:\n",
      "  F1-score: 0.773172\n",
      "  Accuracy: 0.773196\n",
      "  ROC-AUC: 0.819534\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 86\n",
      "  max_depth: 9\n",
      "  learning_rate: 0.026757750238765785\n",
      "  subsample: 0.9057716545094789\n",
      "  colsample_bytree: 0.9725239221264129\n",
      "  gamma: 0.0006617856652471141\n",
      "  reg_alpha: 0.2947373731815785\n",
      "  reg_lambda: 5.980259259401054\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для XGBoost\n",
    "# Подготовка данных для XGBoost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]].drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]]\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров XGBoost (классификация)\"\"\"\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'eval_metric': 'logloss',\n",
    "        'early_stopping_rounds': 20  # Перенесено сюда\n",
    "    }\n",
    "    \n",
    "    # Автоматическое определение типа задачи\n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        xgb_params['objective'] = 'multi:softmax'\n",
    "        xgb_params['num_class'] = len(np.unique(y_train))\n",
    "    else:\n",
    "        xgb_params['objective'] = 'binary:logistic'\n",
    "\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)],  # Оставлено здесь\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Остальной код без изменений\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_xgb = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing XGBoost Classifier...\")\n",
    "    study_xgb.optimize(\n",
    "        objective_xgb,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_xgb.trials) > 0:\n",
    "    best_trial_xgb = study_xgb.best_trial\n",
    "    \n",
    "    print(\"\\nBest XGBoost Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_xgb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_xgb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_xgb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_xgb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_xgb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_xgb.params\n",
    "    best_params.update({\n",
    "        'objective': 'multi:softmax' if len(np.unique(y_train)) > 2 else 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        best_params['num_class'] = len(np.unique(y_train))\n",
    "    \n",
    "    best_xgb = XGBClassifier(**best_params)\n",
    "    best_xgb.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4b9f77b-8173-459f-bd02-b586ef3f9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest Classifier...\n",
      "\n",
      "Best RandomForest Classifier:\n",
      "  F1-score: 0.778345\n",
      "  Accuracy: 0.778351\n",
      "  ROC-AUC: 0.814274\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 250\n",
      "  max_depth: 19\n",
      "  min_samples_split: 2\n",
      "  min_samples_leaf: 3\n",
      "  max_features: 0.504070022146092\n",
      "  bootstrap: True\n",
      "  class_weight: balanced\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для RandomForest\n",
    "# Подготовка данных для RandomForest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]].drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]]\n",
    "\n",
    "def objective_rf(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров RandomForest (классификация)\"\"\"\n",
    "    \n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'criterion': 'gini'  # Можно добавить выбор между 'gini' и 'entropy'\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный для многоклассовой)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_rf = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing RandomForest Classifier...\")\n",
    "    study_rf.optimize(\n",
    "        objective_rf,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_rf.trials) > 0:\n",
    "    best_trial_rf = study_rf.best_trial\n",
    "    \n",
    "    print(\"\\nBest RandomForest Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_rf.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_rf.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_rf.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_rf.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_rf.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_rf.params\n",
    "    best_params.update({\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    best_rf = RandomForestClassifier(**best_params)\n",
    "    best_rf.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b1c00b7-5c8d-4a50-983d-cda7ebcd19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем лучшие результаты\n",
    "best_results_total = []\n",
    "logistic = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['model'][0],\n",
    "        'best_f1':  logistic_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  logistic_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['indices'][0])\n",
    "    }\n",
    "best_results_total.append(logistic)\n",
    "svm = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'SVM']['model'][1],\n",
    "        'best_f1':  svm_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  svm_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'SVM']['indices'][1])\n",
    "    }\n",
    "best_results_total.append(svm)\n",
    "knn = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'KNN']['model'][2],\n",
    "        'best_f1':  knn_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  knn_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'KNN']['indices'][2])\n",
    "    }\n",
    "best_results_total.append(knn)\n",
    "rf = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'RandomForest']['model'][3],\n",
    "        'best_f1':  best_trial_rf.value,\n",
    "        'best_accuracy':  best_trial_rf.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['indices'][3])\n",
    "    }\n",
    "best_results_total.append(rf)\n",
    "\n",
    "xg = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'XGBoost']['model'][4],\n",
    "        'best_f1':  best_trial_xgb.value,\n",
    "        'best_accuracy':  best_trial_xgb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['indices'][4])\n",
    "    }\n",
    "best_results_total.append(xg)\n",
    "cb = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'CatBoost']['model'][5],\n",
    "        'best_f1':  best_trial_cb.value,\n",
    "        'best_accuracy':  best_trial_cb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['indices'][5])\n",
    "    }\n",
    "best_results_total.append(cb)\n",
    "\n",
    "best_results_total_df = pd.DataFrame(best_results_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b12ad85d-f5c6-490c-950b-c6e38a73a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>count_features</th>\n",
       "      <th>count_strings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.731245</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>191</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.767986</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>20</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.778345</td>\n",
       "      <td>0.778351</td>\n",
       "      <td>203</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.778345</td>\n",
       "      <td>0.778351</td>\n",
       "      <td>6</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.773172</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>196</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.809233</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>6</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  count_features  count_strings\n",
       "0  LogisticRegression  0.731245       0.731959             191            765\n",
       "1                 SVM  0.767986       0.768041              20            767\n",
       "2                 KNN  0.778345       0.778351             203            766\n",
       "3        RandomForest  0.778345       0.778351               6            768\n",
       "4             XGBoost  0.773172       0.773196             196            767\n",
       "5            CatBoost  0.809233       0.809278               6            764"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лучшие результаты\n",
    "best_results_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f059f47-0250-4ccd-9494-64142ba4112c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
