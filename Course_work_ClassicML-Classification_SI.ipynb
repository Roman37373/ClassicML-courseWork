{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.937578Z",
     "start_time": "2025-05-28T16:44:43.537577Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import shap\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from scipy import stats\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, \n",
    "                           r2_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, classification_report,\n",
    "                           roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Other ML libraries\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267e6757ca7a5130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:44:45.960272Z",
     "start_time": "2025-05-28T16:44:45.957343Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5b2279b2b7ab49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:11.783119Z",
     "start_time": "2025-05-28T16:57:11.742132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "df = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3d4d43a0b8f2e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:26.787706Z",
     "start_time": "2025-05-28T17:02:26.674195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>IC50, mM</th>\n",
       "      <th>CC50, mM</th>\n",
       "      <th>SI</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>NumValenceElectrons</th>\n",
       "      <th>NumRadicalElectrons</th>\n",
       "      <th>MaxPartialCharge</th>\n",
       "      <th>MinPartialCharge</th>\n",
       "      <th>MaxAbsPartialCharge</th>\n",
       "      <th>MinAbsPartialCharge</th>\n",
       "      <th>FpDensityMorgan1</th>\n",
       "      <th>FpDensityMorgan2</th>\n",
       "      <th>FpDensityMorgan3</th>\n",
       "      <th>BCUT2D_MWHI</th>\n",
       "      <th>BCUT2D_MWLOW</th>\n",
       "      <th>BCUT2D_CHGHI</th>\n",
       "      <th>BCUT2D_CHGLO</th>\n",
       "      <th>BCUT2D_LOGPHI</th>\n",
       "      <th>BCUT2D_LOGPLOW</th>\n",
       "      <th>BCUT2D_MRHI</th>\n",
       "      <th>BCUT2D_MRLOW</th>\n",
       "      <th>AvgIpc</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi0n</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi1n</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>Chi2n</th>\n",
       "      <th>Chi2v</th>\n",
       "      <th>Chi3n</th>\n",
       "      <th>Chi3v</th>\n",
       "      <th>Chi4n</th>\n",
       "      <th>Chi4v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Ipc</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>LabuteASA</th>\n",
       "      <th>PEOE_VSA1</th>\n",
       "      <th>PEOE_VSA10</th>\n",
       "      <th>PEOE_VSA11</th>\n",
       "      <th>PEOE_VSA12</th>\n",
       "      <th>PEOE_VSA13</th>\n",
       "      <th>PEOE_VSA14</th>\n",
       "      <th>PEOE_VSA2</th>\n",
       "      <th>PEOE_VSA3</th>\n",
       "      <th>PEOE_VSA4</th>\n",
       "      <th>PEOE_VSA5</th>\n",
       "      <th>PEOE_VSA6</th>\n",
       "      <th>PEOE_VSA7</th>\n",
       "      <th>PEOE_VSA8</th>\n",
       "      <th>PEOE_VSA9</th>\n",
       "      <th>SMR_VSA1</th>\n",
       "      <th>SMR_VSA10</th>\n",
       "      <th>SMR_VSA2</th>\n",
       "      <th>SMR_VSA3</th>\n",
       "      <th>SMR_VSA4</th>\n",
       "      <th>SMR_VSA5</th>\n",
       "      <th>SMR_VSA6</th>\n",
       "      <th>SMR_VSA7</th>\n",
       "      <th>SMR_VSA8</th>\n",
       "      <th>SMR_VSA9</th>\n",
       "      <th>SlogP_VSA1</th>\n",
       "      <th>SlogP_VSA10</th>\n",
       "      <th>SlogP_VSA11</th>\n",
       "      <th>SlogP_VSA12</th>\n",
       "      <th>SlogP_VSA2</th>\n",
       "      <th>SlogP_VSA3</th>\n",
       "      <th>SlogP_VSA4</th>\n",
       "      <th>SlogP_VSA5</th>\n",
       "      <th>SlogP_VSA6</th>\n",
       "      <th>SlogP_VSA7</th>\n",
       "      <th>SlogP_VSA8</th>\n",
       "      <th>SlogP_VSA9</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>EState_VSA1</th>\n",
       "      <th>EState_VSA10</th>\n",
       "      <th>EState_VSA11</th>\n",
       "      <th>EState_VSA2</th>\n",
       "      <th>EState_VSA3</th>\n",
       "      <th>EState_VSA4</th>\n",
       "      <th>EState_VSA5</th>\n",
       "      <th>EState_VSA6</th>\n",
       "      <th>EState_VSA7</th>\n",
       "      <th>EState_VSA8</th>\n",
       "      <th>EState_VSA9</th>\n",
       "      <th>VSA_EState1</th>\n",
       "      <th>VSA_EState10</th>\n",
       "      <th>VSA_EState2</th>\n",
       "      <th>VSA_EState3</th>\n",
       "      <th>VSA_EState4</th>\n",
       "      <th>VSA_EState5</th>\n",
       "      <th>VSA_EState6</th>\n",
       "      <th>VSA_EState7</th>\n",
       "      <th>VSA_EState8</th>\n",
       "      <th>VSA_EState9</th>\n",
       "      <th>FractionCSP3</th>\n",
       "      <th>HeavyAtomCount</th>\n",
       "      <th>NHOHCount</th>\n",
       "      <th>NOCount</th>\n",
       "      <th>NumAliphaticCarbocycles</th>\n",
       "      <th>NumAliphaticHeterocycles</th>\n",
       "      <th>NumAliphaticRings</th>\n",
       "      <th>NumAromaticCarbocycles</th>\n",
       "      <th>NumAromaticHeterocycles</th>\n",
       "      <th>NumAromaticRings</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHeteroatoms</th>\n",
       "      <th>NumRotatableBonds</th>\n",
       "      <th>NumSaturatedCarbocycles</th>\n",
       "      <th>NumSaturatedHeterocycles</th>\n",
       "      <th>NumSaturatedRings</th>\n",
       "      <th>RingCount</th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>MolMR</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>fr_Ar_OH</th>\n",
       "      <th>fr_COO</th>\n",
       "      <th>fr_COO2</th>\n",
       "      <th>fr_C_O</th>\n",
       "      <th>fr_C_O_noCOO</th>\n",
       "      <th>fr_C_S</th>\n",
       "      <th>fr_HOCCN</th>\n",
       "      <th>fr_Imine</th>\n",
       "      <th>fr_NH0</th>\n",
       "      <th>fr_NH1</th>\n",
       "      <th>fr_NH2</th>\n",
       "      <th>fr_N_O</th>\n",
       "      <th>fr_Ndealkylation1</th>\n",
       "      <th>fr_Ndealkylation2</th>\n",
       "      <th>fr_Nhpyrrole</th>\n",
       "      <th>fr_SH</th>\n",
       "      <th>fr_aldehyde</th>\n",
       "      <th>fr_alkyl_carbamate</th>\n",
       "      <th>fr_alkyl_halide</th>\n",
       "      <th>fr_allylic_oxid</th>\n",
       "      <th>fr_amide</th>\n",
       "      <th>fr_amidine</th>\n",
       "      <th>fr_aniline</th>\n",
       "      <th>fr_aryl_methyl</th>\n",
       "      <th>fr_azide</th>\n",
       "      <th>fr_azo</th>\n",
       "      <th>fr_barbitur</th>\n",
       "      <th>fr_benzene</th>\n",
       "      <th>fr_benzodiazepine</th>\n",
       "      <th>fr_bicyclic</th>\n",
       "      <th>fr_diazo</th>\n",
       "      <th>fr_dihydropyridine</th>\n",
       "      <th>fr_epoxide</th>\n",
       "      <th>fr_ester</th>\n",
       "      <th>fr_ether</th>\n",
       "      <th>fr_furan</th>\n",
       "      <th>fr_guanido</th>\n",
       "      <th>fr_halogen</th>\n",
       "      <th>fr_hdrzine</th>\n",
       "      <th>fr_hdrzone</th>\n",
       "      <th>fr_imidazole</th>\n",
       "      <th>fr_imide</th>\n",
       "      <th>fr_isocyan</th>\n",
       "      <th>fr_isothiocyan</th>\n",
       "      <th>fr_ketone</th>\n",
       "      <th>fr_ketone_Topliss</th>\n",
       "      <th>fr_lactam</th>\n",
       "      <th>fr_lactone</th>\n",
       "      <th>fr_methoxy</th>\n",
       "      <th>fr_morpholine</th>\n",
       "      <th>fr_nitrile</th>\n",
       "      <th>fr_nitro</th>\n",
       "      <th>fr_nitro_arom</th>\n",
       "      <th>fr_nitro_arom_nonortho</th>\n",
       "      <th>fr_nitroso</th>\n",
       "      <th>fr_oxazole</th>\n",
       "      <th>fr_oxime</th>\n",
       "      <th>fr_para_hydroxylation</th>\n",
       "      <th>fr_phenol</th>\n",
       "      <th>fr_phenol_noOrthoHbond</th>\n",
       "      <th>fr_phos_acid</th>\n",
       "      <th>fr_phos_ester</th>\n",
       "      <th>fr_piperdine</th>\n",
       "      <th>fr_piperzine</th>\n",
       "      <th>fr_priamide</th>\n",
       "      <th>fr_prisulfonamd</th>\n",
       "      <th>fr_pyridine</th>\n",
       "      <th>fr_quatN</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.239374</td>\n",
       "      <td>175.482382</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>5.094096</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.387225</td>\n",
       "      <td>0.417362</td>\n",
       "      <td>42.928571</td>\n",
       "      <td>384.652</td>\n",
       "      <td>340.300</td>\n",
       "      <td>384.350449</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>1.321429</td>\n",
       "      <td>14.822266</td>\n",
       "      <td>9.700470</td>\n",
       "      <td>2.600532</td>\n",
       "      <td>-2.343082</td>\n",
       "      <td>2.644698</td>\n",
       "      <td>-2.322229</td>\n",
       "      <td>5.944519</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.164038</td>\n",
       "      <td>611.920301</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>19.534409</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.204226</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>12.058078</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>10.695991</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>7.340247</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>20.606247</td>\n",
       "      <td>6.947534</td>\n",
       "      <td>2.868737</td>\n",
       "      <td>173.630124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>64.208216</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.188192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.807589</td>\n",
       "      <td>1.764908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.258223</td>\n",
       "      <td>16.981087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1212</td>\n",
       "      <td>121.5300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.771831</td>\n",
       "      <td>5.402819</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>3.961417</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.533868</td>\n",
       "      <td>0.462473</td>\n",
       "      <td>45.214286</td>\n",
       "      <td>388.684</td>\n",
       "      <td>340.300</td>\n",
       "      <td>388.381750</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>-0.313407</td>\n",
       "      <td>0.313407</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>14.975110</td>\n",
       "      <td>9.689226</td>\n",
       "      <td>2.614066</td>\n",
       "      <td>-2.394690</td>\n",
       "      <td>2.658342</td>\n",
       "      <td>-2.444817</td>\n",
       "      <td>5.134527</td>\n",
       "      <td>0.120322</td>\n",
       "      <td>3.150503</td>\n",
       "      <td>1.080362</td>\n",
       "      <td>516.780124</td>\n",
       "      <td>20.208896</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>19.794682</td>\n",
       "      <td>13.127794</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.595754</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>12.648545</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>11.473090</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>8.180905</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2.187750e+06</td>\n",
       "      <td>21.163454</td>\n",
       "      <td>7.257648</td>\n",
       "      <td>3.027177</td>\n",
       "      <td>174.939204</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.384066</td>\n",
       "      <td>97.951860</td>\n",
       "      <td>12.083682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.633577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.173194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>77.297729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.176000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.922833</td>\n",
       "      <td>2.153503</td>\n",
       "      <td>1.914377</td>\n",
       "      <td>1.536674</td>\n",
       "      <td>14.135381</td>\n",
       "      <td>17.670565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6.1556</td>\n",
       "      <td>120.5074</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>223.808778</td>\n",
       "      <td>161.142320</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>2.627117</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.543231</td>\n",
       "      <td>0.260923</td>\n",
       "      <td>42.187500</td>\n",
       "      <td>446.808</td>\n",
       "      <td>388.344</td>\n",
       "      <td>446.458903</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>-0.325573</td>\n",
       "      <td>0.325573</td>\n",
       "      <td>0.094802</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.156250</td>\n",
       "      <td>15.353938</td>\n",
       "      <td>9.681293</td>\n",
       "      <td>2.665274</td>\n",
       "      <td>-2.477203</td>\n",
       "      <td>2.679014</td>\n",
       "      <td>-2.565224</td>\n",
       "      <td>5.117187</td>\n",
       "      <td>-0.922902</td>\n",
       "      <td>3.214947</td>\n",
       "      <td>1.219066</td>\n",
       "      <td>643.620154</td>\n",
       "      <td>23.794682</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>23.689110</td>\n",
       "      <td>14.595754</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>14.249005</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>15.671216</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>13.402236</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>10.140303</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>8.610751e+06</td>\n",
       "      <td>25.026112</td>\n",
       "      <td>7.709373</td>\n",
       "      <td>3.470070</td>\n",
       "      <td>201.238858</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>23.671624</td>\n",
       "      <td>53.363882</td>\n",
       "      <td>8.966062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>117.834321</td>\n",
       "      <td>41.280201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.329944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>105.750639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>23.919494</td>\n",
       "      <td>86.263791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.733111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.517630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.184127</td>\n",
       "      <td>1.930720</td>\n",
       "      <td>1.738402</td>\n",
       "      <td>14.491619</td>\n",
       "      <td>18.287216</td>\n",
       "      <td>10.183618</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.1292</td>\n",
       "      <td>138.4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.705624</td>\n",
       "      <td>107.855654</td>\n",
       "      <td>63.235294</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>5.097360</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>0.377846</td>\n",
       "      <td>41.862069</td>\n",
       "      <td>398.679</td>\n",
       "      <td>352.311</td>\n",
       "      <td>398.366099</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>-0.293526</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.038844</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>14.821216</td>\n",
       "      <td>9.700497</td>\n",
       "      <td>2.600529</td>\n",
       "      <td>-2.342885</td>\n",
       "      <td>2.644709</td>\n",
       "      <td>-2.322030</td>\n",
       "      <td>5.944502</td>\n",
       "      <td>0.193510</td>\n",
       "      <td>3.179270</td>\n",
       "      <td>1.120513</td>\n",
       "      <td>626.651366</td>\n",
       "      <td>20.916003</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>20.241516</td>\n",
       "      <td>13.627794</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.704226</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>12.411631</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>10.945991</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>7.517023</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>3.572142e+06</td>\n",
       "      <td>21.567454</td>\n",
       "      <td>7.485204</td>\n",
       "      <td>3.263848</td>\n",
       "      <td>179.995066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.804888</td>\n",
       "      <td>74.032366</td>\n",
       "      <td>35.342864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>13.089513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.512883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>112.171461</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.659962</td>\n",
       "      <td>24.925325</td>\n",
       "      <td>70.629038</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.542423</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.194720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.827852</td>\n",
       "      <td>1.769975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.695439</td>\n",
       "      <td>17.012013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.5113</td>\n",
       "      <td>126.1470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>107.131532</td>\n",
       "      <td>139.270991</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>5.150510</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.270476</td>\n",
       "      <td>0.429038</td>\n",
       "      <td>36.514286</td>\n",
       "      <td>466.713</td>\n",
       "      <td>424.377</td>\n",
       "      <td>466.334799</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>-0.257239</td>\n",
       "      <td>0.257239</td>\n",
       "      <td>0.062897</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>1.257143</td>\n",
       "      <td>14.831112</td>\n",
       "      <td>9.700386</td>\n",
       "      <td>2.602486</td>\n",
       "      <td>-2.342009</td>\n",
       "      <td>2.648473</td>\n",
       "      <td>-2.318893</td>\n",
       "      <td>5.963448</td>\n",
       "      <td>0.193687</td>\n",
       "      <td>3.337074</td>\n",
       "      <td>1.136678</td>\n",
       "      <td>1101.164252</td>\n",
       "      <td>24.639617</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>22.617677</td>\n",
       "      <td>16.526773</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.868825</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>13.613700</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>11.833480</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>8.119076</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>1.053758e+08</td>\n",
       "      <td>23.194917</td>\n",
       "      <td>7.639211</td>\n",
       "      <td>3.345855</td>\n",
       "      <td>211.919602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.807891</td>\n",
       "      <td>103.003916</td>\n",
       "      <td>22.253351</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.798143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.480583</td>\n",
       "      <td>86.488175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.657840</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.374773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.423370</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>33.495774</td>\n",
       "      <td>91.194256</td>\n",
       "      <td>58.515746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.72</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>29.631406</td>\n",
       "      <td>61.075203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.073360</td>\n",
       "      <td>9.984809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.301020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.071783</td>\n",
       "      <td>1.605178</td>\n",
       "      <td>17.869058</td>\n",
       "      <td>8.627311</td>\n",
       "      <td>14.692318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9.1148</td>\n",
       "      <td>148.3380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>996</td>\n",
       "      <td>31.000104</td>\n",
       "      <td>34.999650</td>\n",
       "      <td>1.129017</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>12.934891</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>-0.476142</td>\n",
       "      <td>0.382752</td>\n",
       "      <td>49.133333</td>\n",
       "      <td>414.542</td>\n",
       "      <td>380.270</td>\n",
       "      <td>414.240624</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.317890</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>1.866667</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>16.586886</td>\n",
       "      <td>9.344314</td>\n",
       "      <td>2.726237</td>\n",
       "      <td>-2.677345</td>\n",
       "      <td>2.739076</td>\n",
       "      <td>-2.646743</td>\n",
       "      <td>5.980114</td>\n",
       "      <td>-0.196385</td>\n",
       "      <td>3.023764</td>\n",
       "      <td>1.646946</td>\n",
       "      <td>857.600295</td>\n",
       "      <td>21.637464</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>18.825334</td>\n",
       "      <td>14.097861</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.665192</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>11.409461</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>10.058026</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>8.981266</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>6.242348e+06</td>\n",
       "      <td>20.263719</td>\n",
       "      <td>6.198453</td>\n",
       "      <td>2.219273</td>\n",
       "      <td>178.490760</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>24.360600</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.017713</td>\n",
       "      <td>23.857337</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.67</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.409521</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.503509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.515343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498752</td>\n",
       "      <td>-0.405436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.985276</td>\n",
       "      <td>8.824371</td>\n",
       "      <td>1.494852</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.3002</td>\n",
       "      <td>109.8350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>997</td>\n",
       "      <td>31.999934</td>\n",
       "      <td>33.999415</td>\n",
       "      <td>1.062484</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>13.635345</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>0.369425</td>\n",
       "      <td>44.542857</td>\n",
       "      <td>485.621</td>\n",
       "      <td>446.309</td>\n",
       "      <td>485.277738</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>-0.467493</td>\n",
       "      <td>0.467493</td>\n",
       "      <td>0.327562</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.457143</td>\n",
       "      <td>16.586914</td>\n",
       "      <td>9.343622</td>\n",
       "      <td>2.725543</td>\n",
       "      <td>-2.679467</td>\n",
       "      <td>2.738755</td>\n",
       "      <td>-2.655659</td>\n",
       "      <td>5.980828</td>\n",
       "      <td>-0.187625</td>\n",
       "      <td>3.130958</td>\n",
       "      <td>1.535171</td>\n",
       "      <td>1016.917688</td>\n",
       "      <td>25.499271</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>21.810933</td>\n",
       "      <td>16.402391</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>13.274017</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>12.636189</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>10.827369</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>9.372775</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>5.897229e+07</td>\n",
       "      <td>24.511583</td>\n",
       "      <td>7.908743</td>\n",
       "      <td>3.147136</td>\n",
       "      <td>207.296970</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>23.815096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.185457</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.966734</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>73.143616</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>23.344043</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.347395</td>\n",
       "      <td>5.917906</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.086396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.516353</td>\n",
       "      <td>2.923049</td>\n",
       "      <td>0.162524</td>\n",
       "      <td>-1.300354</td>\n",
       "      <td>-0.699355</td>\n",
       "      <td>7.521836</td>\n",
       "      <td>10.378794</td>\n",
       "      <td>1.327425</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3.8049</td>\n",
       "      <td>127.4397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>998</td>\n",
       "      <td>30.999883</td>\n",
       "      <td>33.999458</td>\n",
       "      <td>1.096761</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>13.991690</td>\n",
       "      <td>0.026535</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>0.284923</td>\n",
       "      <td>41.973684</td>\n",
       "      <td>545.742</td>\n",
       "      <td>502.398</td>\n",
       "      <td>545.281109</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>-0.467485</td>\n",
       "      <td>0.467485</td>\n",
       "      <td>0.327887</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>1.894737</td>\n",
       "      <td>2.552632</td>\n",
       "      <td>32.166365</td>\n",
       "      <td>9.343613</td>\n",
       "      <td>2.725818</td>\n",
       "      <td>-2.679527</td>\n",
       "      <td>2.738943</td>\n",
       "      <td>-2.656447</td>\n",
       "      <td>7.980998</td>\n",
       "      <td>-0.187687</td>\n",
       "      <td>3.204255</td>\n",
       "      <td>1.493776</td>\n",
       "      <td>1070.961298</td>\n",
       "      <td>27.620591</td>\n",
       "      <td>23.633394</td>\n",
       "      <td>24.449891</td>\n",
       "      <td>17.940396</td>\n",
       "      <td>14.301838</td>\n",
       "      <td>15.695685</td>\n",
       "      <td>13.248561</td>\n",
       "      <td>14.234160</td>\n",
       "      <td>11.326709</td>\n",
       "      <td>11.970659</td>\n",
       "      <td>9.725583</td>\n",
       "      <td>10.196987</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>2.627956e+08</td>\n",
       "      <td>27.726151</td>\n",
       "      <td>9.668673</td>\n",
       "      <td>3.822745</td>\n",
       "      <td>230.149965</td>\n",
       "      <td>14.790515</td>\n",
       "      <td>6.041841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.90718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.907916</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.764895</td>\n",
       "      <td>79.620167</td>\n",
       "      <td>10.829981</td>\n",
       "      <td>18.945610</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>35.576981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>78.682541</td>\n",
       "      <td>19.118420</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.316789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>48.975357</td>\n",
       "      <td>28.651875</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>72.640700</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98.77</td>\n",
       "      <td>28.759033</td>\n",
       "      <td>19.178149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.932405</td>\n",
       "      <td>12.338728</td>\n",
       "      <td>44.277783</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>11.761885</td>\n",
       "      <td>6.255769</td>\n",
       "      <td>39.087758</td>\n",
       "      <td>9.473726</td>\n",
       "      <td>10.305031</td>\n",
       "      <td>1.639399</td>\n",
       "      <td>52.527620</td>\n",
       "      <td>3.081660</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>-0.487671</td>\n",
       "      <td>-0.650790</td>\n",
       "      <td>10.055493</td>\n",
       "      <td>8.774745</td>\n",
       "      <td>1.364715</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.5381</td>\n",
       "      <td>144.7647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>999</td>\n",
       "      <td>31.998959</td>\n",
       "      <td>32.999644</td>\n",
       "      <td>1.031272</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>13.830180</td>\n",
       "      <td>0.146522</td>\n",
       "      <td>-1.408652</td>\n",
       "      <td>0.381559</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>522.635</td>\n",
       "      <td>480.299</td>\n",
       "      <td>522.282883</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>-0.468755</td>\n",
       "      <td>0.468755</td>\n",
       "      <td>0.312509</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.351351</td>\n",
       "      <td>1.864865</td>\n",
       "      <td>16.540061</td>\n",
       "      <td>9.364015</td>\n",
       "      <td>2.730109</td>\n",
       "      <td>-2.652209</td>\n",
       "      <td>2.704027</td>\n",
       "      <td>-2.678553</td>\n",
       "      <td>5.950258</td>\n",
       "      <td>-0.225309</td>\n",
       "      <td>2.887043</td>\n",
       "      <td>2.325807</td>\n",
       "      <td>957.299494</td>\n",
       "      <td>27.921921</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>23.380977</td>\n",
       "      <td>17.309003</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>13.174959</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>11.893254</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>10.158570</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>8.609327</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>7.702780e+07</td>\n",
       "      <td>29.111081</td>\n",
       "      <td>10.369092</td>\n",
       "      <td>4.164473</td>\n",
       "      <td>218.836986</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>5.783245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.877221</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.192033</td>\n",
       "      <td>56.278648</td>\n",
       "      <td>11.835812</td>\n",
       "      <td>51.104983</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>29.660466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.099656</td>\n",
       "      <td>42.920138</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>66.219879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>122.27</td>\n",
       "      <td>63.742418</td>\n",
       "      <td>23.972686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.512100</td>\n",
       "      <td>19.262465</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>28.439190</td>\n",
       "      <td>13.847474</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>6.923737</td>\n",
       "      <td>18.947452</td>\n",
       "      <td>20.885832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.303382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.785593</td>\n",
       "      <td>-6.848660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.955837</td>\n",
       "      <td>7.488627</td>\n",
       "      <td>5.083909</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.3649</td>\n",
       "      <td>131.7080</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1000</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>99.999531</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>13.380863</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>-0.447978</td>\n",
       "      <td>0.452565</td>\n",
       "      <td>48.580645</td>\n",
       "      <td>426.597</td>\n",
       "      <td>388.293</td>\n",
       "      <td>426.277010</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>-0.468587</td>\n",
       "      <td>0.468587</td>\n",
       "      <td>0.311311</td>\n",
       "      <td>1.064516</td>\n",
       "      <td>1.774194</td>\n",
       "      <td>2.451613</td>\n",
       "      <td>16.525216</td>\n",
       "      <td>9.330327</td>\n",
       "      <td>2.704274</td>\n",
       "      <td>-2.696212</td>\n",
       "      <td>2.737481</td>\n",
       "      <td>-2.668850</td>\n",
       "      <td>5.978085</td>\n",
       "      <td>-0.201381</td>\n",
       "      <td>2.740713</td>\n",
       "      <td>1.651446</td>\n",
       "      <td>870.462214</td>\n",
       "      <td>22.344571</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>19.831299</td>\n",
       "      <td>14.597861</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.464050</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>12.101953</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>10.667206</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>9.563076</td>\n",
       "      <td>-1.45</td>\n",
       "      <td>9.086032e+06</td>\n",
       "      <td>21.398566</td>\n",
       "      <td>6.776167</td>\n",
       "      <td>2.566599</td>\n",
       "      <td>186.107099</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>11.566490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.969305</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.841158</td>\n",
       "      <td>68.114460</td>\n",
       "      <td>30.092446</td>\n",
       "      <td>12.524788</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>17.535795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>7.109798</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.645593</td>\n",
       "      <td>19.120475</td>\n",
       "      <td>51.752408</td>\n",
       "      <td>79.061522</td>\n",
       "      <td>11.649125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.44</td>\n",
       "      <td>5.414990</td>\n",
       "      <td>14.383612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.470910</td>\n",
       "      <td>36.243945</td>\n",
       "      <td>38.524930</td>\n",
       "      <td>12.682902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.770969</td>\n",
       "      <td>4.736863</td>\n",
       "      <td>5.298868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.472742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776999</td>\n",
       "      <td>1.605640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.247616</td>\n",
       "      <td>8.999949</td>\n",
       "      <td>1.514853</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.1488</td>\n",
       "      <td>117.9840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    IC50, mM    CC50, mM         SI  MaxAbsEStateIndex  \\\n",
       "0             0    6.239374  175.482382  28.125000           5.094096   \n",
       "1             1    0.771831    5.402819   7.000000           3.961417   \n",
       "2             2  223.808778  161.142320   0.720000           2.627117   \n",
       "3             3    1.705624  107.855654  63.235294           5.097360   \n",
       "4             4  107.131532  139.270991   1.300000           5.150510   \n",
       "..          ...         ...         ...        ...                ...   \n",
       "993         996   31.000104   34.999650   1.129017          12.934891   \n",
       "994         997   31.999934   33.999415   1.062484          13.635345   \n",
       "995         998   30.999883   33.999458   1.096761          13.991690   \n",
       "996         999   31.998959   32.999644   1.031272          13.830180   \n",
       "997        1000   99.999531   99.999531   1.000000          13.380863   \n",
       "\n",
       "     MaxEStateIndex  MinAbsEStateIndex  MinEStateIndex       qed        SPS  \\\n",
       "0          5.094096           0.387225        0.387225  0.417362  42.928571   \n",
       "1          3.961417           0.533868        0.533868  0.462473  45.214286   \n",
       "2          2.627117           0.543231        0.543231  0.260923  42.187500   \n",
       "3          5.097360           0.390603        0.390603  0.377846  41.862069   \n",
       "4          5.150510           0.270476        0.270476  0.429038  36.514286   \n",
       "..              ...                ...             ...       ...        ...   \n",
       "993       12.934891           0.048029       -0.476142  0.382752  49.133333   \n",
       "994       13.635345           0.030329       -0.699355  0.369425  44.542857   \n",
       "995       13.991690           0.026535       -0.650790  0.284923  41.973684   \n",
       "996       13.830180           0.146522       -1.408652  0.381559  39.000000   \n",
       "997       13.380863           0.002425       -0.447978  0.452565  48.580645   \n",
       "\n",
       "       MolWt  HeavyAtomMolWt  ExactMolWt  NumValenceElectrons  \\\n",
       "0    384.652         340.300  384.350449                  158   \n",
       "1    388.684         340.300  388.381750                  162   \n",
       "2    446.808         388.344  446.458903                  186   \n",
       "3    398.679         352.311  398.366099                  164   \n",
       "4    466.713         424.377  466.334799                  184   \n",
       "..       ...             ...         ...                  ...   \n",
       "993  414.542         380.270  414.240624                  164   \n",
       "994  485.621         446.309  485.277738                  192   \n",
       "995  545.742         502.398  545.281109                  210   \n",
       "996  522.635         480.299  522.282883                  208   \n",
       "997  426.597         388.293  426.277010                  170   \n",
       "\n",
       "     NumRadicalElectrons  MaxPartialCharge  MinPartialCharge  \\\n",
       "0                      0          0.038844         -0.293526   \n",
       "1                      0          0.012887         -0.313407   \n",
       "2                      0          0.094802         -0.325573   \n",
       "3                      0          0.038844         -0.293526   \n",
       "4                      0          0.062897         -0.257239   \n",
       "..                   ...               ...               ...   \n",
       "993                    0          0.317890         -0.468587   \n",
       "994                    0          0.327562         -0.467493   \n",
       "995                    0          0.327887         -0.467485   \n",
       "996                    0          0.312509         -0.468755   \n",
       "997                    0          0.311311         -0.468587   \n",
       "\n",
       "     MaxAbsPartialCharge  MinAbsPartialCharge  FpDensityMorgan1  \\\n",
       "0               0.293526             0.038844          0.642857   \n",
       "1               0.313407             0.012887          0.607143   \n",
       "2               0.325573             0.094802          0.562500   \n",
       "3               0.293526             0.038844          0.620690   \n",
       "4               0.257239             0.062897          0.600000   \n",
       "..                   ...                  ...               ...   \n",
       "993             0.468587             0.317890          1.133333   \n",
       "994             0.467493             0.327562          1.085714   \n",
       "995             0.467485             0.327887          1.157895   \n",
       "996             0.468755             0.312509          0.756757   \n",
       "997             0.468587             0.311311          1.064516   \n",
       "\n",
       "     FpDensityMorgan2  FpDensityMorgan3  BCUT2D_MWHI  BCUT2D_MWLOW  \\\n",
       "0            1.035714          1.321429    14.822266      9.700470   \n",
       "1            1.000000          1.285714    14.975110      9.689226   \n",
       "2            0.906250          1.156250    15.353938      9.681293   \n",
       "3            1.000000          1.310345    14.821216      9.700497   \n",
       "4            0.971429          1.257143    14.831112      9.700386   \n",
       "..                ...               ...          ...           ...   \n",
       "993          1.866667          2.533333    16.586886      9.344314   \n",
       "994          1.800000          2.457143    16.586914      9.343622   \n",
       "995          1.894737          2.552632    32.166365      9.343613   \n",
       "996          1.351351          1.864865    16.540061      9.364015   \n",
       "997          1.774194          2.451613    16.525216      9.330327   \n",
       "\n",
       "     BCUT2D_CHGHI  BCUT2D_CHGLO  BCUT2D_LOGPHI  BCUT2D_LOGPLOW  BCUT2D_MRHI  \\\n",
       "0        2.600532     -2.343082       2.644698       -2.322229     5.944519   \n",
       "1        2.614066     -2.394690       2.658342       -2.444817     5.134527   \n",
       "2        2.665274     -2.477203       2.679014       -2.565224     5.117187   \n",
       "3        2.600529     -2.342885       2.644709       -2.322030     5.944502   \n",
       "4        2.602486     -2.342009       2.648473       -2.318893     5.963448   \n",
       "..            ...           ...            ...             ...          ...   \n",
       "993      2.726237     -2.677345       2.739076       -2.646743     5.980114   \n",
       "994      2.725543     -2.679467       2.738755       -2.655659     5.980828   \n",
       "995      2.725818     -2.679527       2.738943       -2.656447     7.980998   \n",
       "996      2.730109     -2.652209       2.704027       -2.678553     5.950258   \n",
       "997      2.704274     -2.696212       2.737481       -2.668850     5.978085   \n",
       "\n",
       "     BCUT2D_MRLOW    AvgIpc  BalabanJ      BertzCT       Chi0      Chi0n  \\\n",
       "0        0.193481  3.150503  1.164038   611.920301  20.208896  19.534409   \n",
       "1        0.120322  3.150503  1.080362   516.780124  20.208896  19.794682   \n",
       "2       -0.922902  3.214947  1.219066   643.620154  23.794682  23.689110   \n",
       "3        0.193510  3.179270  1.120513   626.651366  20.916003  20.241516   \n",
       "4        0.193687  3.337074  1.136678  1101.164252  24.639617  22.617677   \n",
       "..            ...       ...       ...          ...        ...        ...   \n",
       "993     -0.196385  3.023764  1.646946   857.600295  21.637464  18.825334   \n",
       "994     -0.187625  3.130958  1.535171  1016.917688  25.499271  21.810933   \n",
       "995     -0.187687  3.204255  1.493776  1070.961298  27.620591  23.633394   \n",
       "996     -0.225309  2.887043  2.325807   957.299494  27.921921  23.380977   \n",
       "997     -0.201381  2.740713  1.651446   870.462214  22.344571  19.831299   \n",
       "\n",
       "         Chi0v       Chi1      Chi1n      Chi1v      Chi2n      Chi2v  \\\n",
       "0    19.534409  13.127794  12.204226  12.204226  12.058078  12.058078   \n",
       "1    19.794682  13.127794  12.595754  12.595754  12.648545  12.648545   \n",
       "2    23.689110  14.595754  14.249005  14.249005  15.671216  15.671216   \n",
       "3    20.241516  13.627794  12.704226  12.704226  12.411631  12.411631   \n",
       "4    22.617677  16.526773  13.868825  13.868825  13.613700  13.613700   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "993  18.825334  14.097861  11.665192  11.665192  11.409461  11.409461   \n",
       "994  21.810933  16.402391  13.274017  13.274017  12.636189  12.636189   \n",
       "995  24.449891  17.940396  14.301838  15.695685  13.248561  14.234160   \n",
       "996  23.380977  17.309003  13.174959  13.174959  11.893254  11.893254   \n",
       "997  19.831299  14.597861  12.464050  12.464050  12.101953  12.101953   \n",
       "\n",
       "         Chi3n      Chi3v      Chi4n      Chi4v  HallKierAlpha           Ipc  \\\n",
       "0    10.695991  10.695991   7.340247   7.340247          -0.66  2.187750e+06   \n",
       "1    11.473090  11.473090   8.180905   8.180905          -0.08  2.187750e+06   \n",
       "2    13.402236  13.402236  10.140303  10.140303          -0.08  8.610751e+06   \n",
       "3    10.945991  10.945991   7.517023   7.517023          -0.66  3.572142e+06   \n",
       "4    11.833480  11.833480   8.119076   8.119076          -2.22  1.053758e+08   \n",
       "..         ...        ...        ...        ...            ...           ...   \n",
       "993  10.058026  10.058026   8.981266   8.981266          -1.65  6.242348e+06   \n",
       "994  10.827369  10.827369   9.372775   9.372775          -2.18  5.897229e+07   \n",
       "995  11.326709  11.970659   9.725583  10.196987          -1.83  2.627956e+08   \n",
       "996  10.158570  10.158570   8.609327   8.609327          -2.45  7.702780e+07   \n",
       "997  10.667206  10.667206   9.563076   9.563076          -1.45  9.086032e+06   \n",
       "\n",
       "        Kappa1     Kappa2    Kappa3   LabuteASA  PEOE_VSA1  PEOE_VSA10  \\\n",
       "0    20.606247   6.947534  2.868737  173.630124   0.000000    0.000000   \n",
       "1    21.163454   7.257648  3.027177  174.939204  10.633577    0.000000   \n",
       "2    25.026112   7.709373  3.470070  201.238858   8.966062    0.000000   \n",
       "3    21.567454   7.485204  3.263848  179.995066   0.000000    0.000000   \n",
       "4    23.194917   7.639211  3.345855  211.919602   0.000000    0.000000   \n",
       "..         ...        ...       ...         ...        ...         ...   \n",
       "993  20.263719   6.198453  2.219273  178.490760   9.473726    0.000000   \n",
       "994  24.511583   7.908743  3.147136  207.296970  14.790515    6.041841   \n",
       "995  27.726151   9.668673  3.822745  230.149965  14.790515    6.041841   \n",
       "996  29.111081  10.369092  4.164473  218.836986  18.947452    5.783245   \n",
       "997  21.398566   6.776167  2.566599  186.107099   4.736863   11.566490   \n",
       "\n",
       "     PEOE_VSA11  PEOE_VSA12  PEOE_VSA13  PEOE_VSA14  PEOE_VSA2  PEOE_VSA3  \\\n",
       "0           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "1           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "2           0.0     0.00000         0.0    0.000000   0.000000   0.000000   \n",
       "3           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "4           0.0     0.00000         0.0    0.000000   9.984809   0.000000   \n",
       "..          ...         ...         ...         ...        ...        ...   \n",
       "993         0.0     0.00000         0.0   17.907916  14.383612   0.000000   \n",
       "994         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "995         0.0     5.90718         0.0   17.907916  14.383612   4.794537   \n",
       "996         0.0     0.00000         0.0   23.877221  23.972686   0.000000   \n",
       "997         0.0     0.00000         0.0    5.969305  14.383612   0.000000   \n",
       "\n",
       "     PEOE_VSA4  PEOE_VSA5  PEOE_VSA6   PEOE_VSA7  PEOE_VSA8  PEOE_VSA9  \\\n",
       "0     0.000000        0.0  54.384066   74.032366  35.342864   0.000000   \n",
       "1     0.000000        0.0  54.384066   97.951860  12.083682   0.000000   \n",
       "2     0.000000        0.0  41.542423   74.032366  23.671624  53.363882   \n",
       "3     0.000000        0.0  60.804888   74.032366  35.342864   0.000000   \n",
       "4     0.000000        0.0  65.807891  103.003916  22.253351  11.374773   \n",
       "..         ...        ...        ...         ...        ...        ...   \n",
       "993   0.000000        0.0  38.841158   68.114460   5.414990  24.360600   \n",
       "994   0.000000        0.0  45.764895   68.114460  10.829981  18.945610   \n",
       "995  11.761885        0.0  45.764895   79.620167  10.829981  18.945610   \n",
       "996   0.000000        0.0  27.192033   56.278648  11.835812  51.104983   \n",
       "997   0.000000        0.0  38.841158   68.114460  30.092446  12.524788   \n",
       "\n",
       "      SMR_VSA1  SMR_VSA10  SMR_VSA2   SMR_VSA3   SMR_VSA4    SMR_VSA5  \\\n",
       "0     0.000000  11.423370       0.0   0.000000  43.480583  105.750639   \n",
       "1     0.000000   0.000000       0.0  10.633577  33.495774  117.834321   \n",
       "2     8.966062   0.000000       0.0   0.000000  33.495774  117.834321   \n",
       "3     0.000000  11.423370       0.0   0.000000  43.480583  112.171461   \n",
       "4     0.000000  22.798143       0.0   0.000000  43.480583   86.488175   \n",
       "..         ...        ...       ...        ...        ...         ...   \n",
       "993  23.857337  17.907916       0.0   0.000000  51.752408   66.219879   \n",
       "994  28.651875  23.815096       0.0   5.316789  51.752408   79.185457   \n",
       "995  28.651875  35.576981       0.0   5.316789  51.752408   78.682541   \n",
       "996  42.920138  29.660466       0.0   0.000000  51.752408   66.219879   \n",
       "997  19.120475  17.535795       0.0   0.000000  51.752408   79.061522   \n",
       "\n",
       "      SMR_VSA6   SMR_VSA7  SMR_VSA8  SMR_VSA9  SlogP_VSA1  SlogP_VSA10  \\\n",
       "0    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "1    13.089513   0.000000         0       0.0   10.633577     0.000000   \n",
       "2    41.280201   0.000000         0       0.0    0.000000     0.000000   \n",
       "3    13.089513   0.000000         0       0.0    0.000000     0.000000   \n",
       "4     0.000000  59.657840         0       0.0    0.000000    11.374773   \n",
       "..         ...        ...       ...       ...         ...          ...   \n",
       "993   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "994   7.109798  11.649125         0       0.0    5.316789     0.000000   \n",
       "995  19.118420  11.649125         0       0.0    5.316789     0.000000   \n",
       "996  28.439190   0.000000         0       0.0    0.000000     0.000000   \n",
       "997   7.109798  11.649125         0       0.0    0.000000     0.000000   \n",
       "\n",
       "     SlogP_VSA11  SlogP_VSA12  SlogP_VSA2  SlogP_VSA3  SlogP_VSA4  SlogP_VSA5  \\\n",
       "0            0.0     0.000000   24.512883    0.000000   33.495774  105.750639   \n",
       "1            0.0     0.000000   25.173194    0.000000   33.495774  105.750639   \n",
       "2            0.0     0.000000   62.329944    0.000000   33.495774  105.750639   \n",
       "3            0.0     0.000000   24.512883    0.000000   33.495774  112.171461   \n",
       "4            0.0     0.000000   11.423370    6.420822   33.495774   91.194256   \n",
       "..           ...          ...         ...         ...         ...         ...   \n",
       "993          0.0     0.000000   25.017713   23.857337   51.752408   66.219879   \n",
       "994          0.0     0.000000   36.966734   28.651875   51.752408   73.143616   \n",
       "995          0.0    11.761885   48.975357   28.651875   51.752408   72.640700   \n",
       "996          0.0     0.000000   58.099656   42.920138   51.752408   66.219879   \n",
       "997          0.0     0.000000   24.645593   19.120475   51.752408   79.061522   \n",
       "\n",
       "     SlogP_VSA6  SlogP_VSA7  SlogP_VSA8  SlogP_VSA9    TPSA  EState_VSA1  \\\n",
       "0      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "1      0.000000         0.0         0.0           0   24.06     0.000000   \n",
       "2      0.000000         0.0         0.0           0    0.00     0.000000   \n",
       "3      9.984809         0.0         0.0           0   24.72     0.000000   \n",
       "4     58.515746         0.0         0.0           0   24.72     0.000000   \n",
       "..          ...         ...         ...         ...     ...          ...   \n",
       "993   11.649125         0.0         0.0           0   69.67     5.414990   \n",
       "994   11.649125         0.0         0.0           0   98.77    23.344043   \n",
       "995   11.649125         0.0         0.0           0   98.77    28.759033   \n",
       "996    0.000000         0.0         0.0           0  122.27    63.742418   \n",
       "997   11.649125         0.0         0.0           0   60.44     5.414990   \n",
       "\n",
       "     EState_VSA10  EState_VSA11  EState_VSA2  EState_VSA3  EState_VSA4  \\\n",
       "0        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "1        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "2        0.000000           0.0     0.000000    21.659962    23.919494   \n",
       "3        0.000000           0.0     0.000000    21.659962    24.925325   \n",
       "4        0.000000           0.0    10.829981    10.829981    29.631406   \n",
       "..            ...           ...          ...          ...          ...   \n",
       "993     14.383612           0.0    52.409521    11.835812    38.524930   \n",
       "994     19.178149           0.0    52.347395     5.917906    38.524930   \n",
       "995     19.178149           0.0    46.932405    12.338728    44.277783   \n",
       "996     23.972686           0.0    30.512100    19.262465     6.420822   \n",
       "997     14.383612           0.0    40.470910    36.243945    38.524930   \n",
       "\n",
       "     EState_VSA5  EState_VSA6  EState_VSA7  EState_VSA8  EState_VSA9  \\\n",
       "0      64.208216    11.423370     0.000000    41.542423     9.984809   \n",
       "1      77.297729     0.000000     0.000000    52.176000     0.000000   \n",
       "2      86.263791     0.000000     0.000000    69.733111     0.000000   \n",
       "3      70.629038    11.423370     0.000000    41.542423     9.984809   \n",
       "4      61.075203     0.000000     0.000000    90.073360     9.984809   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    12.682902     0.000000     0.000000    33.770969     9.473726   \n",
       "994    12.682902     6.923737     0.000000    39.087758     9.473726   \n",
       "995    12.682902    11.761885     6.255769    39.087758     9.473726   \n",
       "996    28.439190    13.847474     6.923737     6.923737    18.947452   \n",
       "997    12.682902     0.000000     0.000000    33.770969     4.736863   \n",
       "\n",
       "     VSA_EState1  VSA_EState10  VSA_EState2  VSA_EState3  VSA_EState4  \\\n",
       "0       0.000000      0.000000    10.188192     0.000000     4.807589   \n",
       "1       0.000000      0.000000     0.000000     7.922833     2.153503   \n",
       "2       2.517630      0.000000     0.000000     0.000000     2.184127   \n",
       "3       0.000000      0.000000    10.194720     0.000000     4.827852   \n",
       "4       0.000000      0.000000    10.301020     0.000000     9.071783   \n",
       "..           ...           ...          ...          ...          ...   \n",
       "993    10.503509      0.000000    38.515343     0.000000     0.498752   \n",
       "994    10.086396      0.000000    51.516353     2.923049     0.162524   \n",
       "995    10.305031      1.639399    52.527620     3.081660     0.139799   \n",
       "996    20.885832      0.000000    67.303382     0.000000    -2.785593   \n",
       "997     5.298868      0.000000    39.472742     0.000000     0.776999   \n",
       "\n",
       "     VSA_EState5  VSA_EState6  VSA_EState7  VSA_EState8  VSA_EState9  \\\n",
       "0       1.764908     0.000000    13.258223    16.981087     0.000000   \n",
       "1       1.914377     1.536674    14.135381    17.670565     0.000000   \n",
       "2       1.930720     1.738402    14.491619    18.287216    10.183618   \n",
       "3       1.769975     0.000000    14.695439    17.012013     0.000000   \n",
       "4       1.605178    17.869058     8.627311    14.692318     0.000000   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "993    -0.405436     0.000000     7.985276     8.824371     1.494852   \n",
       "994    -1.300354    -0.699355     7.521836    10.378794     1.327425   \n",
       "995    -0.487671    -0.650790    10.055493     8.774745     1.364715   \n",
       "996    -6.848660     0.000000     2.955837     7.488627     5.083909   \n",
       "997     1.605640     0.000000     9.247616     8.999949     1.514853   \n",
       "\n",
       "     FractionCSP3  HeavyAtomCount  NHOHCount  NOCount  \\\n",
       "0        0.923077              28          0        2   \n",
       "1        1.000000              28          2        2   \n",
       "2        1.000000              32          0        2   \n",
       "3        0.925926              29          0        2   \n",
       "4        0.575758              35          0        2   \n",
       "..            ...             ...        ...      ...   \n",
       "993      0.800000              30          0        5   \n",
       "994      0.785714              35          1        7   \n",
       "995      0.800000              38          1        7   \n",
       "996      0.821429              37          0        9   \n",
       "997      0.814815              31          0        4   \n",
       "\n",
       "     NumAliphaticCarbocycles  NumAliphaticHeterocycles  NumAliphaticRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        5                         1                  6   \n",
       "994                        5                         1                  6   \n",
       "995                        5                         1                  6   \n",
       "996                        3                         0                  3   \n",
       "997                        6                         0                  6   \n",
       "\n",
       "     NumAromaticCarbocycles  NumAromaticHeterocycles  NumAromaticRings  \\\n",
       "0                         0                        0                 0   \n",
       "1                         0                        0                 0   \n",
       "2                         0                        0                 0   \n",
       "3                         0                        0                 0   \n",
       "4                         2                        0                 2   \n",
       "..                      ...                      ...               ...   \n",
       "993                       0                        0                 0   \n",
       "994                       0                        0                 0   \n",
       "995                       0                        0                 0   \n",
       "996                       0                        0                 0   \n",
       "997                       0                        0                 0   \n",
       "\n",
       "     NumHAcceptors  NumHDonors  NumHeteroatoms  NumRotatableBonds  \\\n",
       "0                2           0               2                  7   \n",
       "1                2           2               2                  9   \n",
       "2                0           0               2                  9   \n",
       "3                2           0               2                  8   \n",
       "4                2           0               2                  4   \n",
       "..             ...         ...             ...                ...   \n",
       "993              5           0               5                  2   \n",
       "994              6           1               7                  4   \n",
       "995              7           1               8                  7   \n",
       "996              9           0               9                  6   \n",
       "997              4           0               4                  2   \n",
       "\n",
       "     NumSaturatedCarbocycles  NumSaturatedHeterocycles  NumSaturatedRings  \\\n",
       "0                          4                         0                  4   \n",
       "1                          4                         0                  4   \n",
       "2                          4                         0                  4   \n",
       "3                          4                         0                  4   \n",
       "4                          4                         0                  4   \n",
       "..                       ...                       ...                ...   \n",
       "993                        3                         1                  4   \n",
       "994                        3                         1                  4   \n",
       "995                        3                         1                  4   \n",
       "996                        3                         0                  3   \n",
       "997                        4                         0                  4   \n",
       "\n",
       "     RingCount  MolLogP     MolMR  fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  \\\n",
       "0            4   7.1212  121.5300          0         0                0   \n",
       "1            4   6.1556  120.5074          0         0                0   \n",
       "2            4   7.1292  138.4528          0         0                0   \n",
       "3            4   7.5113  126.1470          0         0                0   \n",
       "4            6   9.1148  148.3380          0         0                0   \n",
       "..         ...      ...       ...        ...       ...              ...   \n",
       "993          6   4.3002  109.8350          0         0                0   \n",
       "994          6   3.8049  127.4397          0         0                0   \n",
       "995          6   4.5381  144.7647          0         0                0   \n",
       "996          3   3.3649  131.7080          0         0                0   \n",
       "997          6   5.1488  117.9840          0         0                0   \n",
       "\n",
       "     fr_ArN  fr_Ar_COO  fr_Ar_N  fr_Ar_NH  fr_Ar_OH  fr_COO  fr_COO2  fr_C_O  \\\n",
       "0         0          0        0         0         0       0        0       0   \n",
       "1         0          0        0         0         0       0        0       0   \n",
       "2         0          0        0         0         0       0        0       0   \n",
       "3         0          0        0         0         0       0        0       0   \n",
       "4         0          0        0         0         0       0        0       0   \n",
       "..      ...        ...      ...       ...       ...     ...      ...     ...   \n",
       "993       0          0        0         0         0       0        0       3   \n",
       "994       0          0        0         0         0       0        0       4   \n",
       "995       0          0        0         0         0       0        0       4   \n",
       "996       0          0        0         0         0       0        0       5   \n",
       "997       0          0        0         0         0       0        0       3   \n",
       "\n",
       "     fr_C_O_noCOO  fr_C_S  fr_HOCCN  fr_Imine  fr_NH0  fr_NH1  fr_NH2  fr_N_O  \\\n",
       "0               0       0         0         2       2       0       0       0   \n",
       "1               0       0         0         0       0       2       0       0   \n",
       "2               0       0         0         0       2       0       0       0   \n",
       "3               0       0         0         2       2       0       0       0   \n",
       "4               0       0         0         2       2       0       0       0   \n",
       "..            ...     ...       ...       ...     ...     ...     ...     ...   \n",
       "993             3       0         0         0       0       0       0       0   \n",
       "994             4       0         0         0       0       1       0       0   \n",
       "995             4       0         0         0       0       1       0       0   \n",
       "996             5       0         0         0       0       0       0       0   \n",
       "997             3       0         0         0       0       0       0       0   \n",
       "\n",
       "     fr_Ndealkylation1  fr_Ndealkylation2  fr_Nhpyrrole  fr_SH  fr_aldehyde  \\\n",
       "0                    0                  0             0      0            0   \n",
       "1                    0                  0             0      0            0   \n",
       "2                    0                  0             0      0            0   \n",
       "3                    0                  0             0      0            0   \n",
       "4                    0                  0             0      0            0   \n",
       "..                 ...                ...           ...    ...          ...   \n",
       "993                  0                  0             0      0            0   \n",
       "994                  0                  0             0      0            0   \n",
       "995                  0                  0             0      0            0   \n",
       "996                  0                  0             0      0            0   \n",
       "997                  0                  0             0      0            0   \n",
       "\n",
       "     fr_alkyl_carbamate  fr_alkyl_halide  fr_allylic_oxid  fr_amide  \\\n",
       "0                     0                0                0         0   \n",
       "1                     0                0                0         0   \n",
       "2                     0                0                0         0   \n",
       "3                     0                0                0         0   \n",
       "4                     0                0                0         0   \n",
       "..                  ...              ...              ...       ...   \n",
       "993                   0                0                2         0   \n",
       "994                   0                0                2         1   \n",
       "995                   0                0                2         1   \n",
       "996                   0                0                0         0   \n",
       "997                   0                0                2         0   \n",
       "\n",
       "     fr_amidine  fr_aniline  fr_aryl_methyl  fr_azide  fr_azo  fr_barbitur  \\\n",
       "0             0           0               0         0       0            0   \n",
       "1             0           0               0         0       0            0   \n",
       "2             0           0               0         0       0            0   \n",
       "3             0           0               0         0       0            0   \n",
       "4             0           0               0         0       0            0   \n",
       "..          ...         ...             ...       ...     ...          ...   \n",
       "993           0           0               0         0       0            0   \n",
       "994           0           0               0         0       0            0   \n",
       "995           0           0               0         0       0            0   \n",
       "996           0           0               0         0       0            0   \n",
       "997           0           0               0         0       0            0   \n",
       "\n",
       "     fr_benzene  fr_benzodiazepine  fr_bicyclic  fr_diazo  fr_dihydropyridine  \\\n",
       "0             0                  0            4         0                   0   \n",
       "1             0                  0            4         0                   0   \n",
       "2             0                  0            4         0                   0   \n",
       "3             0                  0            4         0                   0   \n",
       "4             2                  0            4         0                   0   \n",
       "..          ...                ...          ...       ...                 ...   \n",
       "993           0                  0            1         0                   0   \n",
       "994           0                  0            1         0                   0   \n",
       "995           0                  0            1         0                   0   \n",
       "996           0                  0            3         0                   0   \n",
       "997           0                  0            1         0                   0   \n",
       "\n",
       "     fr_epoxide  fr_ester  fr_ether  fr_furan  fr_guanido  fr_halogen  \\\n",
       "0             0         0         0         0           0           0   \n",
       "1             0         0         0         0           0           0   \n",
       "2             0         0         0         0           0           0   \n",
       "3             0         0         0         0           0           0   \n",
       "4             0         0         0         0           0           0   \n",
       "..          ...       ...       ...       ...         ...         ...   \n",
       "993           0         3         2         0           0           0   \n",
       "994           0         3         2         0           0           0   \n",
       "995           0         3         2         0           0           0   \n",
       "996           0         4         4         0           0           0   \n",
       "997           0         1         1         0           0           0   \n",
       "\n",
       "     fr_hdrzine  fr_hdrzone  fr_imidazole  fr_imide  fr_isocyan  \\\n",
       "0             0           0             0         0           0   \n",
       "1             0           0             0         0           0   \n",
       "2             0           0             0         0           0   \n",
       "3             0           0             0         0           0   \n",
       "4             0           0             0         0           0   \n",
       "..          ...         ...           ...       ...         ...   \n",
       "993           0           0             0         0           0   \n",
       "994           0           0             0         0           0   \n",
       "995           0           0             0         0           0   \n",
       "996           0           0             0         0           0   \n",
       "997           0           0             0         0           0   \n",
       "\n",
       "     fr_isothiocyan  fr_ketone  fr_ketone_Topliss  fr_lactam  fr_lactone  \\\n",
       "0                 0          0                  0          0           0   \n",
       "1                 0          0                  0          0           0   \n",
       "2                 0          0                  0          0           0   \n",
       "3                 0          0                  0          0           0   \n",
       "4                 0          0                  0          0           0   \n",
       "..              ...        ...                ...        ...         ...   \n",
       "993               0          0                  0          0           2   \n",
       "994               0          0                  0          0           2   \n",
       "995               0          0                  0          0           2   \n",
       "996               0          1                  1          0           0   \n",
       "997               0          2                  2          0           0   \n",
       "\n",
       "     fr_methoxy  fr_morpholine  fr_nitrile  fr_nitro  fr_nitro_arom  \\\n",
       "0             0              0           0         0              0   \n",
       "1             0              0           0         0              0   \n",
       "2             0              0           0         0              0   \n",
       "3             0              0           0         0              0   \n",
       "4             0              0           0         0              0   \n",
       "..          ...            ...         ...       ...            ...   \n",
       "993           1              0           0         0              0   \n",
       "994           1              0           0         0              0   \n",
       "995           1              0           0         0              0   \n",
       "996           4              0           0         0              0   \n",
       "997           1              0           0         0              0   \n",
       "\n",
       "     fr_nitro_arom_nonortho  fr_nitroso  fr_oxazole  fr_oxime  \\\n",
       "0                         0           0           0         0   \n",
       "1                         0           0           0         0   \n",
       "2                         0           0           0         0   \n",
       "3                         0           0           0         0   \n",
       "4                         0           0           0         0   \n",
       "..                      ...         ...         ...       ...   \n",
       "993                       0           0           0         0   \n",
       "994                       0           0           0         0   \n",
       "995                       0           0           0         0   \n",
       "996                       0           0           0         0   \n",
       "997                       0           0           0         0   \n",
       "\n",
       "     fr_para_hydroxylation  fr_phenol  fr_phenol_noOrthoHbond  fr_phos_acid  \\\n",
       "0                        0          0                       0             0   \n",
       "1                        0          0                       0             0   \n",
       "2                        0          0                       0             0   \n",
       "3                        0          0                       0             0   \n",
       "4                        0          0                       0             0   \n",
       "..                     ...        ...                     ...           ...   \n",
       "993                      0          0                       0             0   \n",
       "994                      0          0                       0             0   \n",
       "995                      0          0                       0             0   \n",
       "996                      0          0                       0             0   \n",
       "997                      0          0                       0             0   \n",
       "\n",
       "     fr_phos_ester  fr_piperdine  fr_piperzine  fr_priamide  fr_prisulfonamd  \\\n",
       "0                0             0             0            0                0   \n",
       "1                0             0             0            0                0   \n",
       "2                0             0             0            0                0   \n",
       "3                0             0             0            0                0   \n",
       "4                0             0             0            0                0   \n",
       "..             ...           ...           ...          ...              ...   \n",
       "993              0             0             0            0                0   \n",
       "994              0             0             0            0                0   \n",
       "995              0             0             0            0                0   \n",
       "996              0             0             0            0                0   \n",
       "997              0             0             0            0                0   \n",
       "\n",
       "     fr_pyridine  fr_quatN  fr_sulfide  fr_sulfonamd  fr_sulfone  \\\n",
       "0              0         0           0             0           0   \n",
       "1              0         0           0             0           0   \n",
       "2              0         2           0             0           0   \n",
       "3              0         0           0             0           0   \n",
       "4              0         0           0             0           0   \n",
       "..           ...       ...         ...           ...         ...   \n",
       "993            0         0           0             0           0   \n",
       "994            0         0           0             0           0   \n",
       "995            0         0           1             0           0   \n",
       "996            0         0           0             0           0   \n",
       "997            0         0           0             0           0   \n",
       "\n",
       "     fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  \\\n",
       "0                    0             0            0            0             0   \n",
       "1                    0             0            0            0             0   \n",
       "2                    0             0            0            0             0   \n",
       "3                    0             0            0            0             0   \n",
       "4                    0             0            0            0             0   \n",
       "..                 ...           ...          ...          ...           ...   \n",
       "993                  0             0            0            0             0   \n",
       "994                  0             0            0            0             0   \n",
       "995                  0             0            0            0             0   \n",
       "996                  0             0            0            0             0   \n",
       "997                  0             0            0            0             0   \n",
       "\n",
       "     fr_unbrch_alkane  fr_urea  \n",
       "0                   3        0  \n",
       "1                   3        0  \n",
       "2                   3        0  \n",
       "3                   4        0  \n",
       "4                   0        0  \n",
       "..                ...      ...  \n",
       "993                 0        0  \n",
       "994                 0        0  \n",
       "995                 0        0  \n",
       "996                 0        0  \n",
       "997                 0        0  \n",
       "\n",
       "[998 rows x 214 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a946941faca937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:13.720652Z",
     "start_time": "2025-05-28T16:57:13.716638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем немнформативный признак\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22040ef150ffc8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.281372Z",
     "start_time": "2025-05-28T16:57:14.276214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем пропуски\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e0bb2972cbaa906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:14.915613Z",
     "start_time": "2025-05-28T16:57:14.885015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Удаляем дубликаты\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7089b7fa5f494e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T16:57:16.413719Z",
     "start_time": "2025-05-28T16:57:16.409041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(966, 213)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4529ddf3083cdcc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:00:51.840374Z",
     "start_time": "2025-05-28T17:00:51.836087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Вычисляем медиану столбца\n",
    "median_value_ic50 = df['IC50, mM'].median()\n",
    "median_value_cc50 = df['CC50, mM'].median()\n",
    "median_value_si = df['SI'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "400a82221f828fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:02:21.052804Z",
     "start_time": "2025-05-28T17:02:21.047421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем целевые признаки\n",
    "df['SI'] = (df['SI'] > median_value_si).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502ea21feffa4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:05:44.334975Z",
     "start_time": "2025-05-28T17:05:44.331153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Подготавливаем данные и целевую переменную\n",
    "X = df.drop(columns = ['IC50, mM', 'CC50, mM', 'SI'])\n",
    "y = df['SI']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae82771-5320-4ecd-a016-d937810c06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество выбросов по признакам:\n",
      "IC50, mM              0\n",
      "CC50, mM              0\n",
      "SI                    0\n",
      "MaxAbsEStateIndex    60\n",
      "MaxEStateIndex       60\n",
      "                     ..\n",
      "fr_thiazole          52\n",
      "fr_thiocyan           0\n",
      "fr_thiophene         68\n",
      "fr_unbrch_alkane     49\n",
      "fr_urea               7\n",
      "Length: 213, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers(df, alpha=0.05, method='iqr', normality_test='shapiro', add_sum_column=False):\n",
    "    \"\"\"\n",
    "    Обнаружение выбросов в DataFrame с использованием различных статистических методов.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Входной DataFrame с данными для анализа\n",
    "    alpha : float, по умолчанию 0.05\n",
    "        Уровень значимости для тестов на нормальность\n",
    "    method : str, по умолчанию 'iqr'\n",
    "        Метод обнаружения выбросов для ненормальных данных:\n",
    "        - 'iqr' - метод межквартильного размаха\n",
    "        - 'zscore' - модифицированный Z-score\n",
    "    normality_test : str, по умолчанию 'shapiro'\n",
    "        Тест на нормальность распределения:\n",
    "        - 'shapiro' - тест Шапиро-Уилка\n",
    "        - 'normaltest' - тест на нормальность D'Agostino-Pearson\n",
    "        - 'anderson' - тест Андерсона-Дарлинга\n",
    "    add_sum_column : bool, по умолчанию False\n",
    "        Если True, добавляет столбец с общим количеством выбросов для каждой строки\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        DataFrame с булевыми значениями, где True указывает на выброс\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем DataFrame для хранения результатов (по умолчанию все значения False)\n",
    "    outliers = pd.DataFrame(False, index=df.index, columns=df.columns)\n",
    "    \n",
    "    # Анализируем каждый столбец отдельно\n",
    "    for col in df.columns:\n",
    "        # Удаляем пропущенные значения для текущего столбца\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Если в столбце меньше 3 значений, пропускаем его\n",
    "        if len(data) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Проверяем нормальность распределения\n",
    "        normal = False  # Флаг нормальности распределения\n",
    "        \n",
    "        try:\n",
    "            # Выбираем тест на нормальность в зависимости от параметра normality_test\n",
    "            if normality_test == 'shapiro':\n",
    "                # Тест Шапиро-Уилка (подходит для небольших выборок < 5000)\n",
    "                _, p = stats.shapiro(data)\n",
    "                normal = p > alpha  # Если p-value > alpha, распределение считается нормальным\n",
    "                \n",
    "            elif normality_test == 'normaltest':\n",
    "                # Тест D'Agostino-Pearson (работает для выборок > 20)\n",
    "                _, p = stats.normaltest(data)\n",
    "                normal = p > alpha\n",
    "                \n",
    "            elif normality_test == 'anderson':\n",
    "                # Тест Андерсона-Дарлинга (более строгий)\n",
    "                result = stats.anderson(data)\n",
    "                # Сравниваем статистику с критическим значением для выбранного alpha\n",
    "                normal = result.statistic < result.critical_values[np.where(result.significance_level == int(alpha*100))[0][0]]\n",
    "        except:\n",
    "            # В случае ошибки в тесте считаем распределение ненормальным\n",
    "            pass\n",
    "        \n",
    "        # Если распределение нормальное, используем стандартный Z-score\n",
    "        if normal:\n",
    "            z = np.abs(stats.zscore(data))  # Вычисляем Z-оценки\n",
    "            outliers.loc[data.index, col] = z > 3  # Выбросы > 3 стандартных отклонений\n",
    "            \n",
    "        # Для ненормальных распределений используем выбранный метод\n",
    "        else:\n",
    "            if method == 'iqr':\n",
    "                # Метод межквартильного размаха (IQR)\n",
    "                q1 = data.quantile(0.25)  # Первый квартиль (25-й перцентиль)\n",
    "                q3 = data.quantile(0.75)  # Третий квартиль (75-й перцентиль)\n",
    "                iqr = q3 - q1  # Межквартильный размах\n",
    "                \n",
    "                # Границы для выбросов\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "                \n",
    "                # Отмечаем выбросы\n",
    "                outliers.loc[data.index, col] = (data < lower_bound) | (data > upper_bound)\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                # Модифицированный Z-score (более устойчивый к выбросам)\n",
    "                median = data.median()  # Медиана вместо среднего\n",
    "                mad = stats.median_abs_deviation(data, scale='normal')  # Медианное абсолютное отклонение\n",
    "                modified_z = np.abs(0.6745 * (data - median) / mad)  # Модифицированный Z-score\n",
    "                \n",
    "                # Выбросы при modified_z > 3.5\n",
    "                outliers.loc[data.index, col] = modified_z > 3.5\n",
    "    \n",
    "    # Добавляем столбец с суммой выбросов по строкам, если нужно\n",
    "    if add_sum_column:\n",
    "        outliers['outliers_sum'] = outliers.sum(axis=1)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Находим выбросы\n",
    "outliers = detect_outliers(df)\n",
    "\n",
    "# Выводим количество выбросов по каждому признаку\n",
    "print(\"\\nКоличество выбросов по признакам:\")\n",
    "print(outliers.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3a0ea30522688a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T17:06:25.366761Z",
     "start_time": "2025-05-28T17:06:15.792777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Обучение Logistic Regression...\n",
      " Logistic Regression\n",
      "Accuracy: 0.6701 | Precision: 0.6713 | Recall: 0.6701 | F1: 0.6695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.63      0.66        97\n",
      "           1       0.66      0.71      0.68        97\n",
      "\n",
      "    accuracy                           0.67       194\n",
      "   macro avg       0.67      0.67      0.67       194\n",
      "weighted avg       0.67      0.67      0.67       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение SVM...\n",
      " SVM\n",
      "Accuracy: 0.7216 | Precision: 0.7232 | Recall: 0.7216 | F1: 0.7212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73        97\n",
      "           1       0.74      0.68      0.71        97\n",
      "\n",
      "    accuracy                           0.72       194\n",
      "   macro avg       0.72      0.72      0.72       194\n",
      "weighted avg       0.72      0.72      0.72       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение KNN...\n",
      " KNN\n",
      "Accuracy: 0.6649 | Precision: 0.6664 | Recall: 0.6649 | F1: 0.6642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65        97\n",
      "           1       0.65      0.71      0.68        97\n",
      "\n",
      "    accuracy                           0.66       194\n",
      "   macro avg       0.67      0.66      0.66       194\n",
      "weighted avg       0.67      0.66      0.66       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение Random Forest...\n",
      " Random Forest\n",
      "Accuracy: 0.6443 | Precision: 0.6451 | Recall: 0.6443 | F1: 0.6439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.68      0.66        97\n",
      "           1       0.66      0.61      0.63        97\n",
      "\n",
      "    accuracy                           0.64       194\n",
      "   macro avg       0.65      0.64      0.64       194\n",
      "weighted avg       0.65      0.64      0.64       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение XGBoost...\n",
      " XGBoost\n",
      "Accuracy: 0.6031 | Precision: 0.6031 | Recall: 0.6031 | F1: 0.6031\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.61        97\n",
      "           1       0.60      0.60      0.60        97\n",
      "\n",
      "    accuracy                           0.60       194\n",
      "   macro avg       0.60      0.60      0.60       194\n",
      "weighted avg       0.60      0.60      0.60       194\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "🔍 Обучение CatBoost...\n",
      " CatBoost\n",
      "Accuracy: 0.6598 | Precision: 0.6609 | Recall: 0.6598 | F1: 0.6592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.70      0.67        97\n",
      "           1       0.67      0.62      0.65        97\n",
      "\n",
      "    accuracy                           0.66       194\n",
      "   macro avg       0.66      0.66      0.66       194\n",
      "weighted avg       0.66      0.66      0.66       194\n",
      "\n",
      "──────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def test_classification_models_with_scaling(X, y, test_size=0.2, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Тестирует классификационные модели со стандартизацией данных и возвращает результаты.\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame или np.array\n",
    "        Матрица признаков\n",
    "    y : pd.Series или np.array\n",
    "        Целевая переменная\n",
    "    test_size : float, optional\n",
    "        Размер тестовой выборки (по умолчанию 0.2)\n",
    "    random_state : int, optional\n",
    "        Seed для воспроизводимости (по умолчанию 42)\n",
    "    verbose : bool, optional\n",
    "        Выводить ли прогресс (по умолчанию True)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame\n",
    "        Таблица с результатами метрик и временем обучения для каждой модели\n",
    "    \"\"\"\n",
    "\n",
    "    # Разделение данных\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Инициализация моделей с пайплайнами (StandardScaler + модель)\n",
    "    models = {\n",
    "        'Logistic Regression': make_pipeline(StandardScaler(),\n",
    "                                           LogisticRegression(random_state=random_state, max_iter=1000)),\n",
    "        'SVM': make_pipeline(StandardScaler(),\n",
    "                            SVC(random_state=random_state)),\n",
    "        'KNN': make_pipeline(StandardScaler(),\n",
    "                            KNeighborsClassifier()),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "        'XGBoost': XGBClassifier(random_state=random_state, n_jobs=-1, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(random_state=random_state, verbose=False)\n",
    "    }\n",
    "\n",
    "    # Словарь для хранения результатов\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1 Score': [],\n",
    "        'Train Time (s)': [],\n",
    "        'Scaler Used': []\n",
    "    }\n",
    "\n",
    "    # Обучение и оценка моделей\n",
    "    for name, model in models.items():\n",
    "        if verbose:\n",
    "            print(f\"🔍 Обучение {name}...\")\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Обучение модели\n",
    "            model.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "\n",
    "            # Предсказание\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Расчет метрик\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            # Определение использования StandardScaler\n",
    "            scaler_used = 'StandardScaler' if 'standardscaler' in str(model).lower() else 'No'\n",
    "\n",
    "            # Сохранение результатов\n",
    "            results['Model'].append(name)\n",
    "            results['Accuracy'].append(accuracy)\n",
    "            results['Precision'].append(precision)\n",
    "            results['Recall'].append(recall)\n",
    "            results['F1 Score'].append(f1)\n",
    "            results['Train Time (s)'].append(train_time)\n",
    "            results['Scaler Used'].append(scaler_used)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\" {name}\\nAccuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "                print(\"─\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Ошибка в {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создание DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results).sort_values('F1 Score', ascending=False)\n",
    "\n",
    "    # Визуализация результатов\n",
    "    if verbose:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "        x = np.arange(len(results_df['Model']))\n",
    "        width = 0.2\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "        plt.title('Сравнение моделей классификации', pad=20)\n",
    "        plt.xlabel('Модели')\n",
    "        plt.ylabel('Оценка')\n",
    "        plt.xticks(x + width*1.5, results_df['Model'], rotation=45, ha='right')\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Запуск расчета\n",
    "results = test_classification_models_with_scaling(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "294a5ea4c764740",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-28T17:09:08.860255Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Initial F1: 0.6695, Accuracy: 0.6701 with 210 features\n",
      "Best F1: 0.7165, Accuracy: 0.7165 with 204 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== SVM ===\n",
      "Initial F1: 0.7212, Accuracy: 0.7216 with 210 features\n",
      "Best F1: 0.7418, Accuracy: 0.7423 with 207 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane']\n",
      "\n",
      "=== KNN ===\n",
      "Initial F1: 0.6642, Accuracy: 0.6649 with 210 features\n",
      "Best F1: 0.7108, Accuracy: 0.7113 with 207 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane']\n",
      "\n",
      "=== RandomForest ===\n",
      "Initial F1: 0.6439, Accuracy: 0.6443 with 210 features\n",
      "Best F1: 0.6907, Accuracy: 0.6907 with 207 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== XGBoost ===\n",
      "Initial F1: 0.6031, Accuracy: 0.6031 with 210 features\n",
      "Best F1: 0.6546, Accuracy: 0.6546 with 209 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n",
      "\n",
      "=== CatBoost ===\n",
      "Initial F1: 0.6592, Accuracy: 0.6598 with 210 features\n",
      "Best F1: 0.6796, Accuracy: 0.6804 with 208 features\n",
      "Optimal features: ['MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea']\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации\n",
    "\n",
    "def evaluate_model(X, y, model, test_size=0.2, random_state=42):\n",
    "    \"\"\"Функция оценки модели классификации с добавлением StandardScaler\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Для моделей, чувствительных к масштабу, используем Pipeline со StandardScaler\n",
    "    if isinstance(model, (LogisticRegression, SVC, KNeighborsClassifier)):\n",
    "        model = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "def find_best_feature_subset(X, y, model, model_name):\n",
    "    \"\"\"Модифицированная версия для классификации с указанием имени модели\"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    current_features = X.columns.tolist()\n",
    "    initial_scores = evaluate_model(X[current_features], y, model)\n",
    "    best_f1 = initial_scores['f1']\n",
    "    best_accuracy = initial_scores['accuracy']\n",
    "    best_features = current_features.copy()\n",
    "    history = []\n",
    "\n",
    "    history.append({\n",
    "        'features': current_features.copy(),\n",
    "        'f1': best_f1,\n",
    "        'accuracy': best_accuracy,\n",
    "        'action': 'initial'\n",
    "    })\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Initial F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(current_features)} features\")\n",
    "\n",
    "    improved = True\n",
    "    while improved and len(current_features) > 1:\n",
    "        improved = False\n",
    "        worst_feature = None\n",
    "\n",
    "        for feature in current_features:\n",
    "            trial_features = [f for f in current_features if f != feature]\n",
    "            current_scores = evaluate_model(X[trial_features], y, model)\n",
    "\n",
    "            history.append({\n",
    "                'features': trial_features.copy(),\n",
    "                'f1': current_scores['f1'],\n",
    "                'accuracy': current_scores['accuracy'],\n",
    "                'action': f'removed {feature}'\n",
    "            })\n",
    "\n",
    "            if current_scores['f1'] > best_f1:\n",
    "                best_f1 = current_scores['f1']\n",
    "                best_accuracy = current_scores['accuracy']\n",
    "                best_features = trial_features.copy()\n",
    "                worst_feature = feature\n",
    "                improved = True\n",
    "\n",
    "        if improved:\n",
    "            current_features.remove(worst_feature)\n",
    "\n",
    "    print(f\"Best F1: {best_f1:.4f}, Accuracy: {best_accuracy:.4f} with {len(best_features)} features\")\n",
    "    print(\"Optimal features:\", best_features)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_f1,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей по очереди\"\"\"\n",
    "    # Создаем модели с дефолтными параметрами\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = find_best_feature_subset(X, y, model, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Создаем DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53e9f1a111a073db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: {'accuracy': 0.7010309278350515, 'f1': 0.6989834135901551, 'roc_auc': 0.722818577957275}\n",
      "Selected features: 95 (Removed 115 outliers)\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: {'accuracy': 0.7216494845360825, 'f1': 0.7211753433407857, 'roc_auc': 0.7736741417791476}\n",
      "Selected features: 209 (Removed 1 outliers)\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: {'accuracy': 0.7216494845360825, 'f1': 0.7209079283887468, 'roc_auc': 0.7812732490168987}\n",
      "Selected features: 187 (Removed 23 outliers)\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: {'accuracy': 0.6855670103092784, 'f1': 0.6841487175381002, 'roc_auc': 0.7083111914124773}\n",
      "Selected features: 187 (Removed 23 outliers)\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: {'accuracy': 0.6701030927835051, 'f1': 0.6699627857522594, 'roc_auc': 0.6652141566585185}\n",
      "Selected features: 106 (Removed 104 outliers)\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: {'accuracy': 0.6958762886597938, 'f1': 0.6954798201505838, 'roc_auc': 0.7188861728132638}\n",
      "Selected features: 132 (Removed 78 outliers)\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков для повышения качества классификации, через последовательное удаление признаков, упорядоченных по количеству возможных выбросов\n",
    "\n",
    "outliers_count = pd.DataFrame({\n",
    "        'feature': df.columns,\n",
    "        'outliers': outliers.sum(axis=0)\n",
    "    }).sort_values('outliers', ascending=False)\n",
    "all_features_outliers = outliers_count['feature'][outliers_count['outliers']>0].tolist()\n",
    "\n",
    "\n",
    "def test_all_models(X, y):\n",
    "    \"\"\"Тестирование всех классификационных моделей\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = get_best_features(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_best_features(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с удалением выбросов для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "\n",
    "    # Перебираем количество удаляемых признаков с выбросами\n",
    "    for n in range(1, len(all_features_outliers)+1):\n",
    "        \n",
    "        current_features = X.drop(columns = all_features_outliers[:n]).columns.tolist()\n",
    "\n",
    "        metrics = evaluate_metrics(X[current_features], y, model_name)\n",
    "\n",
    "        if metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = metrics\n",
    "            best_features = current_features.copy()\n",
    "            removed = all_features_outliers[:n]\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: {best_metrics}\")\n",
    "    print(f\"Selected features: {len(best_features)} (Removed {len(removed)} outliers)\")\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'outliers'\n",
    "    }\n",
    "   \n",
    "\n",
    "def evaluate_metrics(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        metrics['roc_auc'] = None\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_2 = test_all_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8277aa7f99158c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.6649, F1=0.6649, ROC-AUC=0.6924752896163249\n",
      "Optimal features (13): ['BCUT2D_MRLOW', 'FractionCSP3', 'MinEStateIndex', 'SMR_VSA7', 'NumSaturatedCarbocycles']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.6753, F1=0.6752, ROC-AUC=0.6855138696992241\n",
      "Optimal features (17): ['BCUT2D_MRLOW', 'SMR_VSA7', 'FractionCSP3', 'VSA_EState7', 'NumSaturatedCarbocycles']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.6907, F1=0.6906, ROC-AUC=0.7146349240089277\n",
      "Optimal features (16): ['NumSaturatedCarbocycles', 'BertzCT', 'qed', 'PEOE_VSA9', 'NumAliphaticHeterocycles']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью SHAP\n",
    "\n",
    "def test_tree_models_sh(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с SHAP-анализом\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_shap_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_shap_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков с помощью SHAP для классификации\n",
    "\n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostRegressor(\n",
    "        iterations=100,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X)\n",
    "        importance = get_significant_shap_features(shap_values, feature_names)\n",
    "   \n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': importance['feature'],\n",
    "        'importance': importance['mean_abs_shap']\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'][feat_importance['importance']>0].tolist()\n",
    "    \n",
    "    \n",
    "    # Находим оптимальное количество признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "\n",
    "    for n in range(1, min(20, len(all_features)+1)):  # Ограничиваем до 20 признаков для скорости\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'shap'\n",
    "    }\n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_significant_shap_features(shap_values, feature_names, threshold=0):\n",
    "    \"\"\"\n",
    "    Возвращает отсортированный по убыванию список признаков с SHAP-значимостью\n",
    "\n",
    "    Параметры:\n",
    "    ----------\n",
    "    shap_values : shap.Explanation или np.ndarray\n",
    "        SHAP значения для всех наблюдений\n",
    "    feature_names : list или pd.Index\n",
    "        Список названий признаков\n",
    "    threshold : float, optional\n",
    "        Порог значимости (по умолчанию 0)\n",
    "\n",
    "    Возвращает:\n",
    "    -----------\n",
    "    pd.DataFrame: DataFrame с колонками 'feature' и 'mean_abs_shap',\n",
    "                  отсортированный по убыванию важности\n",
    "    \"\"\"\n",
    "    if isinstance(shap_values, shap.Explanation):\n",
    "        shap_array = shap_values.values\n",
    "    else:\n",
    "        shap_array = shap_values\n",
    "\n",
    "    # Для многоклассовой классификации берем среднее по всем классам\n",
    "    if len(shap_array.shape) == 3:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=(0, 1))\n",
    "    else:\n",
    "        mean_abs_shap = np.abs(shap_array).mean(axis=0)\n",
    "\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': mean_abs_shap\n",
    "    })\n",
    "\n",
    "    significant_features = shap_importance[shap_importance['mean_abs_shap'] > threshold] \\\n",
    "        .sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    return significant_features.reset_index(drop=True)\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_3 = test_tree_models_sh(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13843390031221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.6701, F1=0.6701, ROC-AUC=0.7035816771176533\n",
      "Optimal features (10): ['BCUT2D_MRLOW', 'VSA_EState4', 'MaxEStateIndex', 'BCUT2D_LOGPHI', 'BCUT2D_MWLOW']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.6753, F1=0.6748, ROC-AUC=0.6943883515782762\n",
      "Optimal features (11): ['NumSaturatedCarbocycles', 'fr_Ar_OH', 'NumSaturatedHeterocycles', 'FractionCSP3', 'fr_thiazole']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7010, F1=0.7002, ROC-AUC=0.72515676479966\n",
      "Optimal features (12): ['BCUT2D_MRLOW', 'Chi1v', 'FractionCSP3', 'VSA_EState2', 'NumAliphaticCarbocycles']...\n"
     ]
    }
   ],
   "source": [
    "# Ищем наилучший набор признаков с учетом значимости признаков определенных с помощью features importance\n",
    "\n",
    "def test_tree_models_fs(X, y):\n",
    "    \"\"\"Тестирование всех моделей классификации с отбором признаков\"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            result = get_feature_selection(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_feature_selection(X, y, name):\n",
    "    \"\"\"\n",
    "    Отбор признаков для классификации на основе важности признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    - all_features: все признаки отсортированные по важности\n",
    "    \"\"\"\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Инициализация и обучение модели\n",
    "    if name == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif name == 'XGBoost':\n",
    "        model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "    elif name == 'CatBoost':\n",
    "        model = CatBoostClassifier(iterations=100, random_seed=42, verbose=False)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Получение важности признаков\n",
    "    if name == 'XGBoost':\n",
    "        importance = model.feature_importances_\n",
    "    else:\n",
    "        importance = model.feature_importances_\n",
    "    \n",
    "    # Сортировка признаков по важности\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    all_features = feat_importance['feature'].tolist()\n",
    "    \n",
    "    # Поиск оптимального набора признаков\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Ограничиваем количество проверяемых комбинаций для скорости\n",
    "    max_features_to_test = min(20, len(all_features))\n",
    "    \n",
    "    for n in range(1, max_features_to_test + 1):\n",
    "        current_features = all_features[:n]\n",
    "        current_metrics = evaluate_classification_metrics(X[current_features], y, name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")  # Показываем первые 5 признаков\n",
    "\n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features),\n",
    "        'selector': 'feature_importance'\n",
    "    }\n",
    "    \n",
    "\n",
    "def evaluate_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"CatBoost\": CatBoostClassifier(silent=True, random_state=42),\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_col_combination_4  = test_tree_models_fs(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5860f10-8bb9-4002-8bf4-4c0889b7f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем все лучшие результаты\n",
    "results_df = pd.concat([results_col_combination,results_col_combination_2, results_col_combination_3, results_col_combination_4], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943b2ea9-ff60-444e-be72-a4bd2ff042ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.716487</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>204</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.741829</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>207</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.710849</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>207</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.690689</td>\n",
       "      <td>0.690722</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>207</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.654630</td>\n",
       "      <td>0.654639</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>209</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.679561</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>208</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.685358</td>\n",
       "      <td>0.685567</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>200</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.721175</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>210</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.705552</td>\n",
       "      <td>0.706186</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>191</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.675249</td>\n",
       "      <td>0.675258</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>191</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.623701</td>\n",
       "      <td>0.623711</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>204</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.659214</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>[MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...</td>\n",
       "      <td>210</td>\n",
       "      <td>outliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.664940</td>\n",
       "      <td>0.664948</td>\n",
       "      <td>[BCUT2D_MRLOW, FractionCSP3, MinEStateIndex, S...</td>\n",
       "      <td>13</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.675249</td>\n",
       "      <td>0.675258</td>\n",
       "      <td>[BCUT2D_MRLOW, SMR_VSA7, FractionCSP3, VSA_ESt...</td>\n",
       "      <td>17</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.690590</td>\n",
       "      <td>0.690722</td>\n",
       "      <td>[NumSaturatedCarbocycles, BertzCT, qed, PEOE_V...</td>\n",
       "      <td>16</td>\n",
       "      <td>shap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.670068</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>[BCUT2D_MRLOW, VSA_EState4, MaxEStateIndex, BC...</td>\n",
       "      <td>10</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.674834</td>\n",
       "      <td>0.675258</td>\n",
       "      <td>[NumSaturatedCarbocycles, fr_Ar_OH, NumSaturat...</td>\n",
       "      <td>11</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.700234</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>[BCUT2D_MRLOW, Chi1v, FractionCSP3, VSA_EState...</td>\n",
       "      <td>12</td>\n",
       "      <td>feature_importance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model   best_f1  best_accuracy  \\\n",
       "0   LogisticRegression  0.716487       0.716495   \n",
       "1                  SVM  0.741829       0.742268   \n",
       "2                  KNN  0.710849       0.711340   \n",
       "3         RandomForest  0.690689       0.690722   \n",
       "4              XGBoost  0.654630       0.654639   \n",
       "5             CatBoost  0.679561       0.680412   \n",
       "6   LogisticRegression  0.685358       0.685567   \n",
       "7                  SVM  0.721175       0.721649   \n",
       "8                  KNN  0.705552       0.706186   \n",
       "9         RandomForest  0.675249       0.675258   \n",
       "10             XGBoost  0.623701       0.623711   \n",
       "11            CatBoost  0.659214       0.659794   \n",
       "12        RandomForest  0.664940       0.664948   \n",
       "13             XGBoost  0.675249       0.675258   \n",
       "14            CatBoost  0.690590       0.690722   \n",
       "15        RandomForest  0.670068       0.670103   \n",
       "16             XGBoost  0.674834       0.675258   \n",
       "17            CatBoost  0.700234       0.701031   \n",
       "\n",
       "                                        best_features  num_features  \\\n",
       "0   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           204   \n",
       "1   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           207   \n",
       "2   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           207   \n",
       "3   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           207   \n",
       "4   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           209   \n",
       "5   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           208   \n",
       "6   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           200   \n",
       "7   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           210   \n",
       "8   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           191   \n",
       "9   [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           191   \n",
       "10  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           204   \n",
       "11  [MaxAbsEStateIndex, MaxEStateIndex, MinAbsESta...           210   \n",
       "12  [BCUT2D_MRLOW, FractionCSP3, MinEStateIndex, S...            13   \n",
       "13  [BCUT2D_MRLOW, SMR_VSA7, FractionCSP3, VSA_ESt...            17   \n",
       "14  [NumSaturatedCarbocycles, BertzCT, qed, PEOE_V...            16   \n",
       "15  [BCUT2D_MRLOW, VSA_EState4, MaxEStateIndex, BC...            10   \n",
       "16  [NumSaturatedCarbocycles, fr_Ar_OH, NumSaturat...            11   \n",
       "17  [BCUT2D_MRLOW, Chi1v, FractionCSP3, VSA_EState...            12   \n",
       "\n",
       "              selector  \n",
       "0    without selection  \n",
       "1    without selection  \n",
       "2    without selection  \n",
       "3    without selection  \n",
       "4    without selection  \n",
       "5    without selection  \n",
       "6             outliers  \n",
       "7             outliers  \n",
       "8             outliers  \n",
       "9             outliers  \n",
       "10            outliers  \n",
       "11            outliers  \n",
       "12                shap  \n",
       "13                shap  \n",
       "14                shap  \n",
       "15  feature_importance  \n",
       "16  feature_importance  \n",
       "17  feature_importance  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e05de657-40c5-4224-8a4d-a4d6ee638132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.6804, F1=0.6801, ROC-AUC=0.7221808906366244\n",
      "Optimal features (204): ['fr_Ndealkylation1', 'fr_aryl_methyl', 'fr_bicyclic', 'SMR_VSA3', 'fr_Al_OH_noTert']...\n",
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7423, F1=0.7418, ROC-AUC=0.7819109363375492\n",
      "Optimal features (207): ['fr_Ndealkylation1', 'fr_aryl_methyl', 'fr_bicyclic', 'SMR_VSA3', 'fr_Al_OH_noTert']...\n",
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.7113, F1=0.7113, ROC-AUC=0.753905834838984\n",
      "Optimal features (12): ['BCUT2D_CHGLO', 'VSA_EState2', 'BCUT2D_MRLOW', 'VSA_EState5', 'SPS']...\n",
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.6649, F1=0.6649, ROC-AUC=0.7010840684451057\n",
      "Optimal features (207): ['fr_Ndealkylation1', 'fr_aryl_methyl', 'fr_bicyclic', 'SMR_VSA3', 'fr_Al_OH_noTert']...\n",
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.6649, F1=0.6649, ROC-AUC=0.6662769688596025\n",
      "Optimal features (16): ['NumAliphaticHeterocycles', 'VSA_EState4', 'FpDensityMorgan1', 'EState_VSA4', 'SMR_VSA4']...\n",
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7165, F1=0.7152, ROC-AUC=0.7584227866935912\n",
      "Optimal features (11): ['fr_Ar_OH', 'NumSaturatedHeterocycles', 'fr_piperdine', 'SMR_VSA7', 'BCUT2D_MRLOW']...\n"
     ]
    }
   ],
   "source": [
    "# Определяем налучший результат работы моделей\n",
    "def test_all_classifiers(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = evaluate_classifier(X, y, name)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_classifier(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Оценка классификатора с оптимальным набором признаков\n",
    "    \n",
    "    Возвращает:\n",
    "    - best_features: список лучших признаков\n",
    "    - best_metrics: лучшие метрики\n",
    "    \"\"\"\n",
    "    # Находим оптимальные метрики\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    best_features = []\n",
    "    \n",
    "    # Получаем все наборы признаков из предыдущих результатов\n",
    "    feature_sets = [set(features) for features in results_df['best_features']]\n",
    "    \n",
    "    # Добавляем все признаки для сравнения\n",
    "    feature_sets.append(set(X.columns))\n",
    "    \n",
    "    # Удаляем дубликаты\n",
    "    unique_feature_sets = []\n",
    "    seen = set()\n",
    "    for fs in feature_sets:\n",
    "        frozen = frozenset(fs)\n",
    "        if frozen not in seen:\n",
    "            seen.add(frozen)\n",
    "            unique_feature_sets.append(list(fs))\n",
    "    \n",
    "    for current_features in unique_feature_sets:\n",
    "        current_metrics = get_classification_metrics(X[current_features], y, model_name)\n",
    "        \n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            best_features = current_features.copy()\n",
    "    \n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Optimal features ({len(best_features)}): {best_features[:5]}...\")\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'best_features': best_features,\n",
    "        'num_features': len(best_features)\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "def get_classification_metrics(X, y, model_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Оценка метрик классификации\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    model = models[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y)) == 2:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Запуск расчета\n",
    "results_features_selection  = test_all_classifiers(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e76424a1-6c08-4999-b289-7a6dd18ad9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>best_features</th>\n",
       "      <th>num_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.680106</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>[fr_Ndealkylation1, fr_aryl_methyl, fr_bicycli...</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.741829</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[fr_Ndealkylation1, fr_aryl_methyl, fr_bicycli...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>[BCUT2D_CHGLO, VSA_EState2, BCUT2D_MRLOW, VSA_...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.664868</td>\n",
       "      <td>0.664948</td>\n",
       "      <td>[fr_Ndealkylation1, fr_aryl_methyl, fr_bicycli...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.664940</td>\n",
       "      <td>0.664948</td>\n",
       "      <td>[NumAliphaticHeterocycles, VSA_EState4, FpDens...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.715216</td>\n",
       "      <td>0.716495</td>\n",
       "      <td>[fr_Ar_OH, NumSaturatedHeterocycles, fr_piperd...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.680106       0.680412   \n",
       "1                 SVM  0.741829       0.742268   \n",
       "2                 KNN  0.711340       0.711340   \n",
       "3        RandomForest  0.664868       0.664948   \n",
       "4             XGBoost  0.664940       0.664948   \n",
       "5            CatBoost  0.715216       0.716495   \n",
       "\n",
       "                                       best_features  num_features  \n",
       "0  [fr_Ndealkylation1, fr_aryl_methyl, fr_bicycli...           204  \n",
       "1  [fr_Ndealkylation1, fr_aryl_methyl, fr_bicycli...           207  \n",
       "2  [BCUT2D_CHGLO, VSA_EState2, BCUT2D_MRLOW, VSA_...            12  \n",
       "3  [fr_Ndealkylation1, fr_aryl_methyl, fr_bicycli...           207  \n",
       "4  [NumAliphaticHeterocycles, VSA_EState4, FpDens...            16  \n",
       "5  [fr_Ar_OH, NumSaturatedHeterocycles, fr_piperd...            11  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_features_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58a92eb3-7eb3-464f-b484-e89cf9b9702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing LogisticRegression: 100%|██████████| 772/772 [00:25<00:00, 29.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression ===\n",
      "Best Metrics: Accuracy=0.7371, F1=0.7371, ROC-AUC=0.7247316399192263\n",
      "Удалено строк: 15 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing SVM: 100%|██████████| 772/772 [03:31<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM ===\n",
      "Best Metrics: Accuracy=0.7680, F1=0.7680, ROC-AUC=0.7897757466255713\n",
      "Удалено строк: 10 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing KNN: 100%|██████████| 772/772 [00:12<00:00, 63.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KNN ===\n",
      "Best Metrics: Accuracy=0.7371, F1=0.7371, ROC-AUC=0.7582633648634287\n",
      "Удалено строк: 7 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest: 100%|██████████| 772/772 [04:04<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RandomForest ===\n",
      "Best Metrics: Accuracy=0.6804, F1=0.6804, ROC-AUC=0.7059730045700925\n",
      "Удалено строк: 5 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost: 100%|██████████| 772/772 [00:44<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost ===\n",
      "Best Metrics: Accuracy=0.7062, F1=0.7060, ROC-AUC=0.6859389945796578\n",
      "Удалено строк: 9 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost: 100%|██████████| 772/772 [15:01<00:00,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost ===\n",
      "Best Metrics: Accuracy=0.7423, F1=0.7413, ROC-AUC=0.7567222871718566\n",
      "Удалено строк: 5 из 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Определяем лучшую комбинацию строк для повышения качества работы моделей на всех данных\n",
    "def test_all_classifiers_optimized(X, y):\n",
    "    \"\"\"Тестирование всех классификаторов с оптимизацией набора данных\"\"\"\n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('SVM', SVC(probability=True, random_state=42)),\n",
    "        ('KNN', KNeighborsClassifier()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('XGBoost', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "        ('CatBoost', CatBoostClassifier(silent=True, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        try:\n",
    "            result = optimize_classifier_data(X, y, name, model)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def optimize_classifier_data(X, y, name, model, verbose=True):\n",
    "    \"\"\"\n",
    "    Оптимизирует набор данных для классификатора путем последовательного удаления строк\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Признаки\n",
    "    y : pd.Series\n",
    "        Целевая переменная\n",
    "    name : str\n",
    "        Имя модели\n",
    "    model : sklearn classifier\n",
    "        Модель классификатора\n",
    "    verbose : bool\n",
    "        Выводить ли прогресс\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации\n",
    "    \"\"\"\n",
    "    # Используем оптимальные признаки для модели\n",
    "    models = {\n",
    "        'LogisticRegression': make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        'SVM': make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),\n",
    "        'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'CatBoost': CatBoostClassifier(silent=True, random_state=42)\n",
    "    }\n",
    "    if name == 'LogisticRegression':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][0]]\n",
    "        \n",
    "    elif name == 'SVM':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][1]]\n",
    "        \n",
    "    elif name == 'KNN':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][2]]\n",
    "    elif name == 'RandomForest':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][3]]\n",
    "    elif name == 'XGBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][4]]\n",
    "    elif name == 'CatBoost':\n",
    "        X = X[results_features_selection[results_features_selection['model'] == name]['best_features'][5]]\n",
    "    \n",
    "    \n",
    "    # Разделение данных со стратификацией\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Инициализация\n",
    "    X_opt = X_train.copy()\n",
    "    y_opt = y_train.copy()\n",
    "    best_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'f1': 0,\n",
    "        'roc_auc': 0\n",
    "    }\n",
    "    removed_indices = []\n",
    "    \n",
    "    # Прогресс-бар для наглядности\n",
    "    iterator = tqdm(X_train.index, desc=f\"Optimizing {name}\") if verbose else X_train.index\n",
    "    \n",
    "    for idx in iterator:\n",
    "        # Временно удаляем строку\n",
    "        X_temp = X_opt.drop(index=idx)\n",
    "        y_temp = y_opt.drop(index=idx)\n",
    "        \n",
    "        # Обучаем и оцениваем\n",
    "        model = models[name]\n",
    "        model.fit(X_temp, y_temp)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Рассчитываем метрики\n",
    "        current_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y)) == 2:\n",
    "            current_metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Решение о сохранении/удалении строки\n",
    "        if current_metrics['f1'] > best_metrics['f1']:\n",
    "            best_metrics = current_metrics\n",
    "            X_opt = X_temp\n",
    "            y_opt = y_temp\n",
    "            removed_indices.append(idx)\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Best Metrics: Accuracy={best_metrics['accuracy']:.4f}, F1={best_metrics['f1']:.4f}, ROC-AUC={best_metrics.get('roc_auc', 'N/A')}\")\n",
    "    print(f\"Удалено строк: {len(removed_indices)} из {len(X_train)}\")\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'model': name,\n",
    "        'best_f1': best_metrics['f1'],\n",
    "        'best_accuracy': best_metrics['accuracy'],\n",
    "        'removed_indices': removed_indices,\n",
    "        'num_removed': len(removed_indices),\n",
    "        'indices': X_opt.index.tolist(),\n",
    "        'selector': 'without selection'\n",
    "    }\n",
    "    \n",
    "# Запуск расчета\n",
    "results_indices_selection = test_all_classifiers_optimized(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2d9c0bc-20b0-429e-93ef-d9e9d1fe0044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>removed_indices</th>\n",
       "      <th>num_removed</th>\n",
       "      <th>indices</th>\n",
       "      <th>selector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.737106</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[380, 608, 401, 219, 383, 624, 546, 694, 346, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[284, 40, 747, 156, 144, 614, 474, 631, 751, 3...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.767986</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>[380, 473, 429, 458, 446, 421, 378, 428, 685, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>[284, 40, 747, 156, 144, 614, 474, 631, 751, 3...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.737106</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>[380, 144, 246, 109, 185, 610, 782]</td>\n",
       "      <td>7</td>\n",
       "      <td>[284, 40, 747, 156, 614, 474, 631, 751, 349, 8...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.680378</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>[380, 631, 339, 670, 198]</td>\n",
       "      <td>5</td>\n",
       "      <td>[284, 40, 747, 156, 144, 614, 474, 751, 349, 8...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.705990</td>\n",
       "      <td>0.706186</td>\n",
       "      <td>[380, 284, 40, 747, 144, 59, 570, 528, 185]</td>\n",
       "      <td>9</td>\n",
       "      <td>[156, 614, 474, 631, 751, 349, 881, 463, 951, ...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.741278</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>[380, 414, 410, 488, 428]</td>\n",
       "      <td>5</td>\n",
       "      <td>[284, 40, 747, 156, 144, 614, 474, 631, 751, 3...</td>\n",
       "      <td>without selection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  \\\n",
       "0  LogisticRegression  0.737106       0.737113   \n",
       "1                 SVM  0.767986       0.768041   \n",
       "2                 KNN  0.737106       0.737113   \n",
       "3        RandomForest  0.680378       0.680412   \n",
       "4             XGBoost  0.705990       0.706186   \n",
       "5            CatBoost  0.741278       0.742268   \n",
       "\n",
       "                                     removed_indices  num_removed  \\\n",
       "0  [380, 608, 401, 219, 383, 624, 546, 694, 346, ...           15   \n",
       "1  [380, 473, 429, 458, 446, 421, 378, 428, 685, ...           10   \n",
       "2                [380, 144, 246, 109, 185, 610, 782]            7   \n",
       "3                          [380, 631, 339, 670, 198]            5   \n",
       "4        [380, 284, 40, 747, 144, 59, 570, 528, 185]            9   \n",
       "5                          [380, 414, 410, 488, 428]            5   \n",
       "\n",
       "                                             indices           selector  \n",
       "0  [284, 40, 747, 156, 144, 614, 474, 631, 751, 3...  without selection  \n",
       "1  [284, 40, 747, 156, 144, 614, 474, 631, 751, 3...  without selection  \n",
       "2  [284, 40, 747, 156, 614, 474, 631, 751, 349, 8...  without selection  \n",
       "3  [284, 40, 747, 156, 144, 614, 474, 751, 349, 8...  without selection  \n",
       "4  [156, 614, 474, 631, 751, 349, 881, 463, 951, ...  without selection  \n",
       "5  [284, 40, 747, 156, 144, 614, 474, 631, 751, 3...  without selection  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_indices_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10259f61-a633-4394-88f7-24551659a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем налучшую комбинацию строк для повышения качества работы моделей\n",
    "\n",
    "def optimize_classifier(X_train, X_test, y_train, y_test, model_type='logistic', n_trials=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Оптимизирует гиперпараметры для классификаторов с улучшенной обработкой ошибок SVM\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    model_type : str\n",
    "        Тип модели: 'logistic', 'svm' или 'knn'\n",
    "    n_trials : int\n",
    "        Количество испытаний для Optuna\n",
    "    random_state : int\n",
    "        Seed для воспроизводимости\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    dict: Результаты оптимизации (лучшие параметры, метрики, study)\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "    # Проверяем, является ли это первым trial (нулевым по индексу)\n",
    "        \n",
    "            # Обычная логика подбора параметров\n",
    "        try:\n",
    "            if model_type == 'logistic':\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['l2', 'l1']),\n",
    "                    'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),LogisticRegression(**params, random_state=random_state))\n",
    "                \n",
    "            elif model_type == 'svm':\n",
    "                kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "                    'kernel': kernel,\n",
    "                    'gamma': trial.suggest_float('gamma', 1e-5, 10, log=True),\n",
    "                    'tol': trial.suggest_float('tol', 1e-5, 1e-1, log=True),\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "                }\n",
    "                if kernel == 'poly':\n",
    "                    params['degree'] = trial.suggest_int('degree', 2, 5)\n",
    "                \n",
    "                model = make_pipeline(StandardScaler(),SVC(**params, random_state=random_state, probability=True))\n",
    "                \n",
    "            elif model_type == 'knn':\n",
    "                params = {\n",
    "                    'n_neighbors': trial.suggest_int('n_neighbors', 1, 30),\n",
    "                    'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "                    'p': trial.suggest_int('p', 1, 3)\n",
    "                }\n",
    "                model = make_pipeline(StandardScaler(),KNeighborsClassifier(**params))\n",
    "        \n",
    "        except Exception as e:\n",
    "            if model_type == 'svm':\n",
    "                error_info = f\"SVM failed with params: {params}. Error: {str(e)}\"\n",
    "                trial.set_user_attr(\"svm_error\", error_info)\n",
    "            return float('-inf')\n",
    "        \n",
    "        # Общая часть для всех моделей (дефолтных и оптимизированных)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        if y_proba is not None and len(np.unique(y_train)) == 2:\n",
    "            trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "        \n",
    "        return f1\n",
    "        \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    # Анализ результатов\n",
    "    best_params = study.best_params\n",
    "    best_metrics = {\n",
    "        'f1': study.best_value,\n",
    "        'accuracy': study.best_trial.user_attrs['accuracy'],\n",
    "    }\n",
    "    \n",
    "    if 'roc_auc' in study.best_trial.user_attrs:\n",
    "        best_metrics['roc_auc'] = study.best_trial.user_attrs['roc_auc']\n",
    "    \n",
    "    # Добавляем информацию о неудачных trials для SVM\n",
    "    if model_type == 'svm':\n",
    "        failed_trials = [t for t in study.trials if 'svm_error' in t.user_attrs]\n",
    "        if failed_trials:\n",
    "            best_metrics['failed_trials_count'] = len(failed_trials)\n",
    "            best_metrics['last_error'] = failed_trials[-1].user_attrs['svm_error']\n",
    "    \n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'best_params': best_params,\n",
    "        'best_metrics': best_metrics,\n",
    "        'study': study\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44b5a03c-04b9-48de-81be-5b1fb9e7eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск подбора гиперпараметров\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train_logistic = X_train[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]].drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "y_train_logistic = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['removed_indices'][0])\n",
    "X_test_logistic = X_test[results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]]\n",
    "logistic_results = optimize_classifier(X_train_logistic, X_test_logistic, y_train_logistic, y_test, 'logistic', n_trials=200)\n",
    "\n",
    "X_train_svm = X_train[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]].drop(index=results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "y_train_svm = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'SVM']['removed_indices'][1])\n",
    "X_test_svm = X_test[results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]]\n",
    "svm_results = optimize_classifier(X_train_svm, X_test_svm, y_train_svm, y_test, 'svm', n_trials=200)\n",
    "X_train_knn = X_train[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]].drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "y_train_knn = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'KNN']['removed_indices'][2])\n",
    "X_test_knn = X_test[results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]]\n",
    "knn_results = optimize_classifier(X_train_knn, X_test_knn, y_train_knn, y_test, 'knn', n_trials=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e4f328-a7fb-40ed-b301-9daaee300865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best params: {'C': 1.7947067826898155, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 371}\n",
      "LogisticRegression best f1: 0.7423\n",
      "LogisticRegression best accuracy: 0.7423\n",
      "SVM best params: {'kernel': 'linear', 'C': 0.013150976445939896, 'gamma': 0.005579748423546171, 'tol': 0.0011767677954052738, 'max_iter': 572}\n",
      "SVM best f1: 0.7318\n",
      "SVM best accuracy: 0.7320\n",
      "KNN best params: {'n_neighbors': 15, 'weights': 'uniform', 'p': 3}\n",
      "KNN best f1: 0.7268\n",
      "KNN best accuracy: 0.7268\n"
     ]
    }
   ],
   "source": [
    "# Вывод результатов подбора гиперпараметров для линейных моделей\n",
    "print(f\"LogisticRegression best params: {logistic_results['best_params']}\")\n",
    "print(f\"LogisticRegression best f1: {logistic_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"LogisticRegression best accuracy: {logistic_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"SVM best params: {svm_results['best_params']}\")\n",
    "print(f\"SVM best f1: {svm_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"SVM best accuracy: {svm_results['best_metrics']['accuracy']:.4f}\")\n",
    "print(f\"KNN best params: {knn_results['best_params']}\")\n",
    "print(f\"KNN best f1: {knn_results['best_metrics']['f1']:.4f}\")\n",
    "print(f\"KNN best accuracy: {knn_results['best_metrics']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f55b546d-50e7-4fd4-9f1a-daf3824d6ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing CatBoost Classifier...\n",
      "\n",
      "Best trial:\n",
      "  F1-score: 0.750883\n",
      "  Accuracy: 0.752577\n",
      "  ROC-AUC: 0.770592\n",
      "\n",
      "Best params:\n",
      "  iterations: 307\n",
      "  depth: 9\n",
      "  learning_rate: 0.062288182204842384\n",
      "  l2_leaf_reg: 8.953076995787168\n",
      "  random_strength: 9.485244722710357\n",
      "  bagging_temperature: 0.3329201190078684\n",
      "  border_count: 185\n",
      "  min_data_in_leaf: 24\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для CatBoost\n",
    "# Подготовка данных для CatBoost\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train_cb = X_train[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]].drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "y_train_cb = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['removed_indices'][5])\n",
    "X_test_cb = X_test[results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]]\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров CatBoostClassifier\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42,\n",
    "        'thread_count': 4\n",
    "    }\n",
    "    \n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    # Обучение модели\n",
    "    model.fit(\n",
    "        X_train_cb, \n",
    "        y_train_cb,\n",
    "        eval_set=(X_test_cb, y_test),\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Предсказание и метрики\n",
    "    y_pred = model.predict(X_test_cb)\n",
    "    y_proba = model.predict_proba(X_test_cb)[:, 1] if len(y_train_cb.unique()) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing CatBoost Classifier...\")\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study.trials) > 0:\n",
    "    best_trial_cb = study.best_trial\n",
    "    \n",
    "    print(\"\\nBest trial:\")\n",
    "    print(f\"  F1-score: {best_trial_cb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_cb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_cb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_cb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_cb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_cb.params\n",
    "    best_params.update({\n",
    "        'loss_function': 'MultiClass' if len(y_train_cb.unique()) > 2 else 'Logloss',\n",
    "        'silent': True,\n",
    "        'random_seed': 42\n",
    "    })\n",
    "    \n",
    "    best_model = CatBoostClassifier(**best_params)\n",
    "    best_model.fit(\n",
    "        pd.concat([X_train_cb, X_test_cb]),\n",
    "        pd.concat([y_train_cb, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc0194b-d65f-4c27-98bf-4258b6634371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost Classifier...\n",
      "\n",
      "Best XGBoost Classifier:\n",
      "  F1-score: 0.751627\n",
      "  Accuracy: 0.752577\n",
      "  ROC-AUC: 0.771602\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 280\n",
      "  max_depth: 7\n",
      "  learning_rate: 0.29380045937640903\n",
      "  subsample: 0.9724067552199888\n",
      "  colsample_bytree: 0.8455483560173913\n",
      "  gamma: 0.7119285679047076\n",
      "  reg_alpha: 3.031212769039522\n",
      "  reg_lambda: 2.1572222911024275\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для XGBoost\n",
    "# Подготовка данных для XGBoost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]].drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['removed_indices'][4])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]]\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров XGBoost (классификация)\"\"\"\n",
    "    \n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'eval_metric': 'logloss',\n",
    "        'early_stopping_rounds': 20  # Перенесено сюда\n",
    "    }\n",
    "    \n",
    "    # Автоматическое определение типа задачи\n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        xgb_params['objective'] = 'multi:softmax'\n",
    "        xgb_params['num_class'] = len(np.unique(y_train))\n",
    "    else:\n",
    "        xgb_params['objective'] = 'binary:logistic'\n",
    "\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)],  # Оставлено здесь\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Остальной код без изменений\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_xgb = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing XGBoost Classifier...\")\n",
    "    study_xgb.optimize(\n",
    "        objective_xgb,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_xgb.trials) > 0:\n",
    "    best_trial_xgb = study_xgb.best_trial\n",
    "    \n",
    "    print(\"\\nBest XGBoost Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_xgb.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_xgb.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_xgb.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_xgb.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_xgb.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_xgb.params\n",
    "    best_params.update({\n",
    "        'objective': 'multi:softmax' if len(np.unique(y_train)) > 2 else 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        best_params['num_class'] = len(np.unique(y_train))\n",
    "    \n",
    "    best_xgb = XGBClassifier(**best_params)\n",
    "    best_xgb.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test]),\n",
    "        verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4b9f77b-8173-459f-bd02-b586ef3f9ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest Classifier...\n",
      "\n",
      "Best RandomForest Classifier:\n",
      "  F1-score: 0.715728\n",
      "  Accuracy: 0.721649\n",
      "  ROC-AUC: 0.760017\n",
      "\n",
      "Best params:\n",
      "  n_estimators: 71\n",
      "  max_depth: 3\n",
      "  min_samples_split: 12\n",
      "  min_samples_leaf: 4\n",
      "  max_features: 0.6123473364470902\n",
      "  bootstrap: False\n",
      "  class_weight: balanced\n"
     ]
    }
   ],
   "source": [
    "# Подбор гиперпараметров для RandomForest\n",
    "# Подготовка данных для RandomForest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "X_train = X_train[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]].drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "y_train = y_train.drop(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['removed_indices'][3])\n",
    "X_test = X_test[results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]]\n",
    "\n",
    "def objective_rf(trial):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров RandomForest (классификация)\"\"\"\n",
    "    \n",
    "    rf_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'criterion': 'gini'  # Можно добавить выбор между 'gini' и 'entropy'\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_train)) == 2 else None\n",
    "    \n",
    "    # Основная метрика - F1-score (взвешенный для многоклассовой)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Сохраняем дополнительные метрики\n",
    "    trial.set_user_attr(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    if y_proba is not None:\n",
    "        trial.set_user_attr(\"roc_auc\", roc_auc_score(y_test, y_proba))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def log_trial_progress(study, trial):\n",
    "    \"\"\"Callback для логирования прогресса\"\"\"\n",
    "    if trial.number == 0:\n",
    "        print(\"| Trial |   F1     | Accuracy | ROC-AUC  |\")\n",
    "        print(\"|-------|----------|----------|----------|\")\n",
    "    \n",
    "    f1 = trial.value if trial.value is not None else 0\n",
    "    acc = trial.user_attrs.get(\"accuracy\", 0)\n",
    "    roc_auc = trial.user_attrs.get(\"roc_auc\", \"N/A\")\n",
    "    \n",
    "    print(f\"| {trial.number:5} | {f1:.6f} | {acc:.6f} | {roc_auc if isinstance(roc_auc, str) else roc_auc:.6f} |\")\n",
    "\n",
    "# Настройка исследования Optuna\n",
    "study_rf = optuna.create_study(\n",
    "    direction='maximize',  # Максимизируем F1-score\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "\n",
    "# Запуск оптимизации\n",
    "try:\n",
    "    print(\"Optimizing RandomForest Classifier...\")\n",
    "    study_rf.optimize(\n",
    "        objective_rf,\n",
    "        n_trials=500,\n",
    "        #callbacks=[log_trial_progress],\n",
    "        gc_after_trial=True\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nOptimization stopped by user\")\n",
    "\n",
    "# Анализ результатов\n",
    "if len(study_rf.trials) > 0:\n",
    "    best_trial_rf = study_rf.best_trial\n",
    "    \n",
    "    print(\"\\nBest RandomForest Classifier:\")\n",
    "    print(f\"  F1-score: {best_trial_rf.value:.6f}\")\n",
    "    print(f\"  Accuracy: {best_trial_rf.user_attrs['accuracy']:.6f}\")\n",
    "    if 'roc_auc' in best_trial_rf.user_attrs:\n",
    "        print(f\"  ROC-AUC: {best_trial_rf.user_attrs['roc_auc']:.6f}\")\n",
    "    print(\"\\nBest params:\")\n",
    "    for key, value in best_trial_rf.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Лучшая модель\n",
    "    best_params = best_trial_rf.params\n",
    "    best_params.update({\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    })\n",
    "    \n",
    "    best_rf = RandomForestClassifier(**best_params)\n",
    "    best_rf.fit(\n",
    "        pd.concat([X_train, X_test]),\n",
    "        pd.concat([y_train, y_test])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b1c00b7-5c8d-4a50-983d-cda7ebcd19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем лучшие результаты\n",
    "best_results_total = []\n",
    "logistic = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['model'][0],\n",
    "        'best_f1':  logistic_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  logistic_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'LogisticRegression']['best_features'][0]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'LogisticRegression']['indices'][0])\n",
    "    }\n",
    "best_results_total.append(logistic)\n",
    "svm = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'SVM']['model'][1],\n",
    "        'best_f1':  svm_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  svm_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'SVM']['best_features'][1]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'SVM']['indices'][1])\n",
    "    }\n",
    "best_results_total.append(svm)\n",
    "knn = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'KNN']['model'][2],\n",
    "        'best_f1':  knn_results['best_metrics']['f1'],\n",
    "        'best_accuracy':  knn_results['best_metrics']['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'KNN']['best_features'][2]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'KNN']['indices'][2])\n",
    "    }\n",
    "best_results_total.append(knn)\n",
    "rf = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'RandomForest']['model'][3],\n",
    "        'best_f1':  best_trial_rf.value,\n",
    "        'best_accuracy':  best_trial_rf.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'RandomForest']['best_features'][3]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'RandomForest']['indices'][3])\n",
    "    }\n",
    "best_results_total.append(rf)\n",
    "\n",
    "xg = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'XGBoost']['model'][4],\n",
    "        'best_f1':  best_trial_xgb.value,\n",
    "        'best_accuracy':  best_trial_xgb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'XGBoost']['best_features'][4]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'XGBoost']['indices'][4])\n",
    "    }\n",
    "best_results_total.append(xg)\n",
    "cb = {\n",
    "        'model': results_indices_selection[results_indices_selection['model'] == 'CatBoost']['model'][5],\n",
    "        'best_f1':  best_trial_cb.value,\n",
    "        'best_accuracy':  best_trial_cb.user_attrs['accuracy'],\n",
    "        'count_features': len(results_features_selection[results_features_selection['model'] == 'CatBoost']['best_features'][5]),\n",
    "        'count_strings': len(results_indices_selection[results_indices_selection['model'] == 'CatBoost']['indices'][5])\n",
    "    }\n",
    "best_results_total.append(cb)\n",
    "\n",
    "best_results_total_df = pd.DataFrame(best_results_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b12ad85d-f5c6-490c-950b-c6e38a73a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_f1</th>\n",
       "      <th>best_accuracy</th>\n",
       "      <th>count_features</th>\n",
       "      <th>count_strings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>204</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.731845</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>207</td>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.726797</td>\n",
       "      <td>0.726804</td>\n",
       "      <td>12</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.715728</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>207</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.751627</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>16</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.750883</td>\n",
       "      <td>0.752577</td>\n",
       "      <td>11</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   best_f1  best_accuracy  count_features  count_strings\n",
       "0  LogisticRegression  0.742268       0.742268             204            757\n",
       "1                 SVM  0.731845       0.731959             207            762\n",
       "2                 KNN  0.726797       0.726804              12            765\n",
       "3        RandomForest  0.715728       0.721649             207            767\n",
       "4             XGBoost  0.751627       0.752577              16            763\n",
       "5            CatBoost  0.750883       0.752577              11            767"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лучшие результаты\n",
    "best_results_total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f059f47-0250-4ccd-9494-64142ba4112c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
